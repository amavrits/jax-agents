
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://amavrits.github.io/jax-agents/reference/ippobase/">
      
      
        <link rel="prev" href="../ppoagent/">
      
      
        <link rel="next" href="../ippo/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>IPPOBase - JAX Agents</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2afb09e1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#jaxagents.ippo.IPPOBase" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="JAX Agents" class="md-header__button md-logo" aria-label="JAX Agents" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            JAX Agents
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              IPPOBase
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/amavrits/jax-agents" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    amavrits/jax-agents
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../ppoagentbase/" class="md-tabs__link">
          
  
  
  Reference

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="JAX Agents" class="md-nav__button md-logo" aria-label="JAX Agents" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    JAX Agents
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/amavrits/jax-agents" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    amavrits/jax-agents
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ppoagentbase/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PPOAgentBase
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ppoagent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PPOAgent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    IPPOBase
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    IPPOBase
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase" class="md-nav__link">
    <span class="md-ellipsis">
      IPPOBase
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_minibatch_fn" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_minibatch_fn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_minibatch_fn" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_minibatch_fn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.actor_training" class="md-nav__link">
    <span class="md-ellipsis">
      actor_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.agent_trained" class="md-nav__link">
    <span class="md-ellipsis">
      agent_trained
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.config" class="md-nav__link">
    <span class="md-ellipsis">
      config
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.critic_training" class="md-nav__link">
    <span class="md-ellipsis">
      critic_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.eval_during_training" class="md-nav__link">
    <span class="md-ellipsis">
      eval_during_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.n_actors" class="md-nav__link">
    <span class="md-ellipsis">
      n_actors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.previous_training_max_step" class="md-nav__link">
    <span class="md-ellipsis">
      previous_training_max_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.training_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      training_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.training_runner" class="md-nav__link">
    <span class="md-ellipsis">
      training_runner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.__str__" class="md-nav__link">
    <span class="md-ellipsis">
      __str__
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_epoch" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_epoch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_loss" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_loss_input" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_loss_input
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_minibatch_update" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_minibatch_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_training_cond" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_training_cond
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_update" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._add_advantages" class="md-nav__link">
    <span class="md-ellipsis">
      _add_advantages
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._add_next_values" class="md-nav__link">
    <span class="md-ellipsis">
      _add_next_values
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._advantages" class="md-nav__link">
    <span class="md-ellipsis">
      _advantages
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._checkpoint" class="md-nav__link">
    <span class="md-ellipsis">
      _checkpoint
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._checkpoint_base" class="md-nav__link">
    <span class="md-ellipsis">
      _checkpoint_base
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._create_empty_trainstate" class="md-nav__link">
    <span class="md-ellipsis">
      _create_empty_trainstate
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._create_training" class="md-nav__link">
    <span class="md-ellipsis">
      _create_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._create_update_runner" class="md-nav__link">
    <span class="md-ellipsis">
      _create_update_runner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_epoch" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_epoch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_loss" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_loss_input" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_loss_input
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_minibatch_update" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_minibatch_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_update" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._entropy" class="md-nav__link">
    <span class="md-ellipsis">
      _entropy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_agent" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_body" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_body
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_cond" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_cond
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._generate_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      _generate_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_checkpointer" class="md-nav__link">
    <span class="md-ellipsis">
      _init_checkpointer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_env" class="md-nav__link">
    <span class="md-ellipsis">
      _init_env
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_network" class="md-nav__link">
    <span class="md-ellipsis">
      _init_network
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      _init_optimizer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      _log_prob
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._make_rollout_runners" class="md-nav__link">
    <span class="md-ellipsis">
      _make_rollout_runners
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._make_transition" class="md-nav__link">
    <span class="md-ellipsis">
      _make_transition
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._pp" class="md-nav__link">
    <span class="md-ellipsis">
      _pp
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._process_trajectory" class="md-nav__link">
    <span class="md-ellipsis">
      _process_trajectory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._returns" class="md-nav__link">
    <span class="md-ellipsis">
      _returns
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._rollout" class="md-nav__link">
    <span class="md-ellipsis">
      _rollout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._sample_actions" class="md-nav__link">
    <span class="md-ellipsis">
      _sample_actions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._training_step" class="md-nav__link">
    <span class="md-ellipsis">
      _training_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._trajectory_advantages" class="md-nav__link">
    <span class="md-ellipsis">
      _trajectory_advantages
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._trajectory_returns" class="md-nav__link">
    <span class="md-ellipsis">
      _trajectory_returns
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._update_step" class="md-nav__link">
    <span class="md-ellipsis">
      _update_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.collect_training" class="md-nav__link">
    <span class="md-ellipsis">
      collect_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.env_reset" class="md-nav__link">
    <span class="md-ellipsis">
      env_reset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.env_step" class="md-nav__link">
    <span class="md-ellipsis">
      env_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.eval" class="md-nav__link">
    <span class="md-ellipsis">
      eval
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.log_hyperparams" class="md-nav__link">
    <span class="md-ellipsis">
      log_hyperparams
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.policy" class="md-nav__link">
    <span class="md-ellipsis">
      policy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.restore" class="md-nav__link">
    <span class="md-ellipsis">
      restore
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.summarize" class="md-nav__link">
    <span class="md-ellipsis">
      summarize
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.train" class="md-nav__link">
    <span class="md-ellipsis">
      train
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ippo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IPPO
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase" class="md-nav__link">
    <span class="md-ellipsis">
      IPPOBase
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_minibatch_fn" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_minibatch_fn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_minibatch_fn" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_minibatch_fn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.actor_training" class="md-nav__link">
    <span class="md-ellipsis">
      actor_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.agent_trained" class="md-nav__link">
    <span class="md-ellipsis">
      agent_trained
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.config" class="md-nav__link">
    <span class="md-ellipsis">
      config
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.critic_training" class="md-nav__link">
    <span class="md-ellipsis">
      critic_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.eval_during_training" class="md-nav__link">
    <span class="md-ellipsis">
      eval_during_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.n_actors" class="md-nav__link">
    <span class="md-ellipsis">
      n_actors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.previous_training_max_step" class="md-nav__link">
    <span class="md-ellipsis">
      previous_training_max_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.training_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      training_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.training_runner" class="md-nav__link">
    <span class="md-ellipsis">
      training_runner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.__str__" class="md-nav__link">
    <span class="md-ellipsis">
      __str__
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_epoch" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_epoch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_loss" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_loss_input" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_loss_input
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_minibatch_update" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_minibatch_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_training_cond" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_training_cond
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._actor_update" class="md-nav__link">
    <span class="md-ellipsis">
      _actor_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._add_advantages" class="md-nav__link">
    <span class="md-ellipsis">
      _add_advantages
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._add_next_values" class="md-nav__link">
    <span class="md-ellipsis">
      _add_next_values
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._advantages" class="md-nav__link">
    <span class="md-ellipsis">
      _advantages
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._checkpoint" class="md-nav__link">
    <span class="md-ellipsis">
      _checkpoint
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._checkpoint_base" class="md-nav__link">
    <span class="md-ellipsis">
      _checkpoint_base
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._create_empty_trainstate" class="md-nav__link">
    <span class="md-ellipsis">
      _create_empty_trainstate
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._create_training" class="md-nav__link">
    <span class="md-ellipsis">
      _create_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._create_update_runner" class="md-nav__link">
    <span class="md-ellipsis">
      _create_update_runner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_epoch" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_epoch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_loss" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_loss_input" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_loss_input
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_minibatch_update" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_minibatch_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._critic_update" class="md-nav__link">
    <span class="md-ellipsis">
      _critic_update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._entropy" class="md-nav__link">
    <span class="md-ellipsis">
      _entropy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_agent" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_body" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_body
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_cond" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_cond
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._eval_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      _eval_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._generate_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      _generate_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_checkpointer" class="md-nav__link">
    <span class="md-ellipsis">
      _init_checkpointer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_env" class="md-nav__link">
    <span class="md-ellipsis">
      _init_env
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_network" class="md-nav__link">
    <span class="md-ellipsis">
      _init_network
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._init_optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      _init_optimizer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      _log_prob
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._make_rollout_runners" class="md-nav__link">
    <span class="md-ellipsis">
      _make_rollout_runners
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._make_transition" class="md-nav__link">
    <span class="md-ellipsis">
      _make_transition
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._pp" class="md-nav__link">
    <span class="md-ellipsis">
      _pp
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._process_trajectory" class="md-nav__link">
    <span class="md-ellipsis">
      _process_trajectory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._returns" class="md-nav__link">
    <span class="md-ellipsis">
      _returns
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._rollout" class="md-nav__link">
    <span class="md-ellipsis">
      _rollout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._sample_actions" class="md-nav__link">
    <span class="md-ellipsis">
      _sample_actions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._training_step" class="md-nav__link">
    <span class="md-ellipsis">
      _training_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._trajectory_advantages" class="md-nav__link">
    <span class="md-ellipsis">
      _trajectory_advantages
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._trajectory_returns" class="md-nav__link">
    <span class="md-ellipsis">
      _trajectory_returns
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase._update_step" class="md-nav__link">
    <span class="md-ellipsis">
      _update_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.collect_training" class="md-nav__link">
    <span class="md-ellipsis">
      collect_training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.env_reset" class="md-nav__link">
    <span class="md-ellipsis">
      env_reset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.env_step" class="md-nav__link">
    <span class="md-ellipsis">
      env_step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.eval" class="md-nav__link">
    <span class="md-ellipsis">
      eval
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.log_hyperparams" class="md-nav__link">
    <span class="md-ellipsis">
      log_hyperparams
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.policy" class="md-nav__link">
    <span class="md-ellipsis">
      policy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.restore" class="md-nav__link">
    <span class="md-ellipsis">
      restore
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.summarize" class="md-nav__link">
    <span class="md-ellipsis">
      summarize
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jaxagents.ippo.IPPOBase.train" class="md-nav__link">
    <span class="md-ellipsis">
      train
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>IPPOBase</h1>

<div class="doc doc-object doc-class">



<a id="jaxagents.ippo.IPPOBase"></a>
    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="abc.ABC">ABC</span></code></p>


        <p>Base for PPO agents.
Can be used for both discrete or continuous action environments, and its use depends on the provided actor network.
Follows the instructions of: https://spinningup.openai.com/en/latest/algorithms/ppo.html
Uses lax.scan for rollout, so trajectories may be truncated.</p>
<p>Training relies on jitting several methods by treating the 'self' arg as static. According to suggested practice,
this can prove dangerous (https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods -
How to use jit with methods?); if attrs of 'self' change during training, the changes will not be registered in
jit. In this case, neither agent training nor evaluation change any 'self' attrs, so using Strategy 2 of the
suggested practice is valid. Otherwise, strategy 3 should have been used.</p>







              <details class="quote">
                <summary>Source code in <code>jaxagents\ippo.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  88</span>
<span class="normal">  89</span>
<span class="normal">  90</span>
<span class="normal">  91</span>
<span class="normal">  92</span>
<span class="normal">  93</span>
<span class="normal">  94</span>
<span class="normal">  95</span>
<span class="normal">  96</span>
<span class="normal">  97</span>
<span class="normal">  98</span>
<span class="normal">  99</span>
<span class="normal"> 100</span>
<span class="normal"> 101</span>
<span class="normal"> 102</span>
<span class="normal"> 103</span>
<span class="normal"> 104</span>
<span class="normal"> 105</span>
<span class="normal"> 106</span>
<span class="normal"> 107</span>
<span class="normal"> 108</span>
<span class="normal"> 109</span>
<span class="normal"> 110</span>
<span class="normal"> 111</span>
<span class="normal"> 112</span>
<span class="normal"> 113</span>
<span class="normal"> 114</span>
<span class="normal"> 115</span>
<span class="normal"> 116</span>
<span class="normal"> 117</span>
<span class="normal"> 118</span>
<span class="normal"> 119</span>
<span class="normal"> 120</span>
<span class="normal"> 121</span>
<span class="normal"> 122</span>
<span class="normal"> 123</span>
<span class="normal"> 124</span>
<span class="normal"> 125</span>
<span class="normal"> 126</span>
<span class="normal"> 127</span>
<span class="normal"> 128</span>
<span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">IPPOBase</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Base for PPO agents.</span>
<span class="sd">        Can be used for both discrete or continuous action environments, and its use depends on the provided actor network.</span>
<span class="sd">        Follows the instructions of: https://spinningup.openai.com/en/latest/algorithms/ppo.html</span>
<span class="sd">        Uses lax.scan for rollout, so trajectories may be truncated.</span>

<span class="sd">        Training relies on jitting several methods by treating the &#39;self&#39; arg as static. According to suggested practice,</span>
<span class="sd">        this can prove dangerous (https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods -</span>
<span class="sd">        How to use jit with methods?); if attrs of &#39;self&#39; change during training, the changes will not be registered in</span>
<span class="sd">        jit. In this case, neither agent training nor evaluation change any &#39;self&#39; attrs, so using Strategy 2 of the</span>
<span class="sd">        suggested practice is valid. Otherwise, strategy 3 should have been used.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="c1"># Number of actors in environment</span>
    <span class="n">n_actors</span><span class="p">:</span> <span class="nb">int</span>
    <span class="c1"># Function for performing a minibatch update of the actor network.</span>
    <span class="n">_actor_minibatch_fn</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span>
        <span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span>
        <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span>
    <span class="p">]</span>
    <span class="c1"># Function for performing a minibatch update of the critic network.</span>
    <span class="n">_critic_minibatch_fn</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span>
        <span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]],</span>
        <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]]</span>
    <span class="p">]</span>
    <span class="n">agent_trained</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Whether the agent has been trained.</span>
    <span class="n">training_runner</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Runner</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Runner object after training.</span>
    <span class="n">actor_training</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">TrainState</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Actor training object.</span>
    <span class="n">critic_training</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">TrainState</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Critic training object.</span>
    <span class="n">training_metrics</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Metrics collected during training.</span>
    <span class="n">eval_during_training</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Whether the agent&#39;s performance is evaluated during training</span>
    <span class="c1"># The maximum step reached in precious training. Zero by default for starting a new training. Will be set by</span>
    <span class="c1"># restoring or passing a trained agent (from serial training or restoring)</span>
    <span class="n">previous_training_max_step</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">env</span><span class="p">:</span> <span class="n">Environment</span><span class="p">,</span>
            <span class="n">env_params</span><span class="p">:</span> <span class="n">EnvParams</span><span class="p">,</span>
            <span class="n">config</span><span class="p">:</span> <span class="n">AgentConfig</span><span class="p">,</span>
            <span class="n">eval_during_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param env: A gymnax or custom environment that inherits from the basic gymnax class.</span>
<span class="sd">        :param env_params: A dataclass named &quot;EnvParams&quot; containing the parametrization of the environment.</span>
<span class="sd">        :param config: The configuration of the agent as and AgentConfig object (from vpf_utils).</span>
<span class="sd">        :param eval_during_training: Whether evaluation should be performed during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">n_actors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span> <span class="o">=</span> <span class="n">eval_during_training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_checkpointer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_env</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_params</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a string containing only the non-default field values.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">field</span> <span class="o">+</span> <span class="s1">&#39;: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">field</span><span class="p">))</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_fields</span><span class="p">]</span>
        <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Agent configuration:&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_lst</span>

        <span class="k">return</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_lst</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; GENERAL METHODS&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_env</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">Environment</span><span class="p">,</span> <span class="n">env_params</span><span class="p">:</span> <span class="n">EnvParams</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Environment initialization.</span>
<span class="sd">        :param env: A gymnax or custom environment that inherits from the basic gymnax class.</span>
<span class="sd">        :param env_params: A dataclass containing the parametrization of the environment.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">env</span> <span class="o">=</span> <span class="n">TruncationWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_episode_steps</span><span class="p">)</span>
        <span class="c1"># env = FlattenObservationWrapper(env)</span>
        <span class="c1"># self.env = LogWrapper(env)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_params</span> <span class="o">=</span> <span class="n">env_params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_checkpointer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If</span>
<span class="sd">        so, sets the checkpoint manager using orbax.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">restore_agent</span><span class="p">:</span>

                <span class="n">dir_exists</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">dir_exists</span><span class="p">:</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">)</span>

                <span class="n">dir_files</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">file</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>
                <span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dir_files</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">dir_files</span><span class="p">:</span>
                        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
                        <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

                <span class="c1"># Log training configuration</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s1">&#39;training_configuration.txt&#39;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">())</span>

            <span class="n">orbax_checkpointer</span> <span class="o">=</span> <span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">Checkpointer</span><span class="p">(</span><span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">PyTreeCheckpointHandler</span><span class="p">())</span>

            <span class="n">options</span> <span class="o">=</span> <span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">CheckpointManagerOptions</span><span class="p">(</span>
                <span class="n">create</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">step_prefix</span><span class="o">=</span><span class="s1">&#39;trainingstep&#39;</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span>
                <span class="n">orbax_checkpointer</span><span class="p">,</span>
                <span class="n">options</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_empty_trainstate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainState</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates an empty TrainState object for restoring checkpoints.</span>
<span class="sd">        :param network: The actor or critic network.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Just a dummy PRNGKey for initializing the networks parameters.</span>
        <span class="n">network</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_network</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">network</span><span class="p">)</span>

        <span class="n">optimizer_params</span> <span class="o">=</span> <span class="n">OptimizerParams</span><span class="p">()</span>  <span class="c1"># Use the default values of the OptimizerParams object.</span>
        <span class="n">tx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_optimizer</span><span class="p">(</span><span class="n">optimizer_params</span><span class="p">)</span>

        <span class="n">empty_training</span> <span class="o">=</span> <span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">empty_training</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">restore</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;best&quot;</span><span class="p">,</span>
            <span class="n">best_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]],</span> <span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then,</span>
<span class="sd">        post-processes the restored checkpoint.</span>
<span class="sd">        :param mode: Determines whether the best performing or latest checkpoint should be restored.</span>
<span class="sd">        :param best_fn: The function that should be used in determining the best performing checkpoint.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">all_steps</span><span class="p">()</span>

        <span class="c1"># Log keys in checkpoints</span>
        <span class="n">ckpt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">ckpt_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">ckpt</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;runner&quot;</span><span class="p">]</span>

        <span class="c1"># Collect history of metrics in training. Useful for continuing training.</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ckpt_keys</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
            <span class="n">ckpt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ckpt_keys</span><span class="p">:</span>
                <span class="n">metrics</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">jnp</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;best&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">best_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">step</span> <span class="o">=</span> <span class="n">steps</span><span class="p">[</span><span class="n">best_fn</span><span class="p">(</span><span class="n">metrics</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Function for determining best checkpoint not provided&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;last&quot;</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">latest_step</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Unknown method for selecting a checkpoint.&quot;</span><span class="p">)</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create an empty target for restoring the checkpoint.</span>
<span class="sd">        Some of the arguments come from restoring one of the ckpts.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">empty_actor_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_empty_trainstate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">actor_network</span><span class="p">)</span>
        <span class="n">empty_critic_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_empty_trainstate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">critic_network</span><span class="p">)</span>

        <span class="c1"># Get some state and envstate for restoring the checkpoint.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">empty_runner</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span>
            <span class="n">actor_training</span><span class="o">=</span><span class="n">empty_actor_training</span><span class="p">,</span>
            <span class="n">critic_training</span><span class="o">=</span><span class="n">empty_critic_training</span><span class="p">,</span>
            <span class="n">envstate</span><span class="o">=</span><span class="n">envstate</span><span class="p">,</span>
            <span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="p">,</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">),</span>  <span class="c1"># Just a dummy PRNGKey for initializing the networks parameters.</span>
            <span class="c1"># Hyperparams can be loaded as a dict. If training continues, new hyperparams will be provided.</span>
            <span class="n">hyperparams</span><span class="o">=</span><span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;runner&quot;</span><span class="p">][</span><span class="s2">&quot;hyperparams&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">target_ckpt</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;runner&quot;</span><span class="p">:</span> <span class="n">empty_runner</span><span class="p">,</span>
            <span class="s2">&quot;terminated&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="s2">&quot;truncated&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="s2">&quot;final_rewards&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;final_rewards&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;returns&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
        <span class="p">}</span>

        <span class="n">ckpt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">items</span><span class="o">=</span><span class="n">target_ckpt</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">collect_training</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;runner&quot;</span><span class="p">],</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">previous_training_max_step</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">steps</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_params</span><span class="p">:</span> <span class="n">OptimizerParams</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">optax</span><span class="o">.</span><span class="n">chain</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to</span>
<span class="sd">        initialize the appropriate optimizer. In this way, the optimizer can be initialized within the &quot;train&quot; method,</span>
<span class="sd">        and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary.</span>
<span class="sd">        :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.</span>
<span class="sd">        :return: An optimizer in optax.chain.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">optimizer_params_dict</span> <span class="o">=</span> <span class="n">optimizer_params</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span>  <span class="c1"># Transform from NamedTuple to dict</span>
        <span class="n">optimizer_params_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;grad_clip&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Remove &#39;grad_clip&#39;, since it is not part of the optimizer args.</span>


<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get dictionary of optimizer parameters to pass in optimizer. The procedure preserves parameters that:</span>
<span class="sd">            - are given in the OptimizerParams NamedTuple and are requested as args by the optimizer</span>
<span class="sd">            - are requested as args by the optimizer and are given in the OptimizerParams NamedTuple</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">optimizer_arg_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="vm">__code__</span><span class="o">.</span><span class="n">co_varnames</span>  <span class="c1"># List names of args of optimizer.</span>

        <span class="c1"># Keep only the optimizer arg names that are also part of the OptimizerParams (dict from NamedTuple)</span>
        <span class="n">optimizer_arg_names</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">arg_name</span> <span class="k">for</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="n">optimizer_arg_names</span> <span class="k">if</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">optimizer_params_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer_arg_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;The defined optimizer parameters do not include relevant arguments for this optimizer.&quot;</span>
                <span class="s2">&quot;The optimizer has not been implemented yet. Define your own OptimizerParams object.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Keep only the optimizer params that are arg names for the specific optimizer</span>
        <span class="n">optimizer_params_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg_name</span><span class="p">:</span> <span class="n">optimizer_params_dict</span><span class="p">[</span><span class="n">arg_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="n">optimizer_arg_names</span><span class="p">}</span>

        <span class="c1"># No need to scale by -1.0. &#39;TrainState.apply_gradients&#39; is used for training, which subtracts the update.</span>
        <span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">optax</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">optimizer_params</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="o">**</span><span class="n">optimizer_params_dict</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">tx</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_network</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
            <span class="n">network</span><span class="p">:</span> <span class="n">flax</span><span class="o">.</span><span class="n">linen</span><span class="o">.</span><span class="n">Module</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">flax</span><span class="o">.</span><span class="n">linen</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">FrozenDict</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialization of the actor or critic network.</span>
<span class="sd">        :param rng: Random key for initialization.</span>
<span class="sd">        :param network: The actor or critic network.</span>
<span class="sd">        :return: A random key after splitting the input and the initial parameters of the policy network.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">network</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

        <span class="n">rng</span><span class="p">,</span> <span class="o">*</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">dummy_reset_rng</span><span class="p">,</span> <span class="n">network_init_rng</span> <span class="o">=</span> <span class="n">_rng</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">dummy_obs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">(</span><span class="n">dummy_reset_rng</span><span class="p">)</span>
        <span class="n">init_x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">dummy_obs</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>

        <span class="n">params</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">network_init_rng</span><span class="p">,</span> <span class="n">init_x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">network</span><span class="p">,</span> <span class="n">params</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">env_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">PRNGKeyArray</span><span class="p">,</span> <span class="n">ObsType</span><span class="p">,</span> <span class="n">LogEnvState</span> <span class="o">|</span> <span class="n">EnvState</span> <span class="o">|</span> <span class="n">TruncationEnvState</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Environment reset.</span>
<span class="sd">        :param rng: Random key for initialization.</span>
<span class="sd">        :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">reset_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">reset_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_params</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">rng</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">env_step</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
            <span class="n">envstate</span><span class="p">:</span> <span class="n">LogEnvState</span> <span class="o">|</span> <span class="n">EnvState</span> <span class="o">|</span> <span class="n">TruncationEnvState</span><span class="p">,</span>
            <span class="n">actions</span><span class="p">:</span> <span class="n">ActionType</span>
    <span class="p">)</span><span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
        <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">ObsType</span><span class="p">,</span>
        <span class="n">LogEnvState</span> <span class="o">|</span> <span class="n">EnvState</span> <span class="o">|</span> <span class="n">TruncationEnvState</span><span class="p">,</span>
        <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
        <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
        <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">bool</span><span class="p">]</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Environment step.</span>
<span class="sd">        :param rng: Random key for initialization.</span>
<span class="sd">        :param envstate: The environment state in LogEnvState format.</span>
<span class="sd">        :param actions: The actions selected per actor.</span>
<span class="sd">        :return: A tuple of: a random key after splitting the input, the next state in array and LogEnvState formats,</span>
<span class="sd">                 the collected reward after executing the action, episode termination and a dictionary of optional</span>
<span class="sd">                 additional information.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">step_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">step_rng</span><span class="p">,</span> <span class="n">envstate</span><span class="p">,</span> <span class="n">actions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_params</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">rng</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; METHODS FOR TRAINING &quot;&quot;&quot;</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_make_transition</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
            <span class="n">actions</span><span class="p">:</span> <span class="n">ActionType</span><span class="p">,</span>
            <span class="n">value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">],</span>
            <span class="n">log_prob</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">],</span>
            <span class="n">reward</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">],</span>
            <span class="n">next_obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
            <span class="n">terminated</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
            <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a transition object based on the input and output of an episode step.</span>
<span class="sd">        :param obs: The current state of the episode step in array format.</span>
<span class="sd">        :param actions: The action selected per actor.</span>
<span class="sd">        :param value: The value of the state per critic.</span>
<span class="sd">        :param log_prob: The actor log-probability of the selected action.</span>
<span class="sd">        :param reward: The collected reward after executing the action per actor.</span>
<span class="sd">        :param next_obs: The next state of the episode step in array format.</span>
<span class="sd">        :param terminated: Episode termination.</span>
<span class="sd">        :return: A transition object storing information about the state before and after executing the episode step,</span>
<span class="sd">                 the executed action, the collected reward, episode termination and optional additional information.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">transition</span> <span class="o">=</span> <span class="n">Transition</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">actions</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">terminated</span><span class="p">)</span>
        <span class="n">transition</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">transition</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">transition</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">update_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates metrics for on-policy learning. The agent performance during training is evaluated by running</span>
<span class="sd">        n_evals episodes (until termination). If the user selects not to generate metrics (leading to faster training),</span>
<span class="sd">        an empty dictionary is returned.</span>
<span class="sd">        :param runner: The update runner object, containing information about the current status of the actor&#39;s/critic&#39;s</span>
<span class="sd">        training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :param update_step: The number of the update step.</span>
<span class="sd">        :return: A dictionary of the sum of rewards collected over &#39;n_evals&#39; episodes, or empty dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">metric</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
            <span class="n">metric</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_agent</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_rng</span><span class="p">,</span>
                <span class="n">runner</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span>
                <span class="n">runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_evals</span>
            <span class="p">)</span>

        <span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
            <span class="s2">&quot;actor_loss&quot;</span><span class="p">:</span> <span class="n">runner</span><span class="o">.</span><span class="n">actor_loss</span><span class="p">,</span>
            <span class="s2">&quot;critic_loss&quot;</span><span class="p">:</span> <span class="n">runner</span><span class="o">.</span><span class="n">critic_loss</span>
        <span class="p">})</span>

        <span class="k">return</span> <span class="n">metric</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_training</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
            <span class="n">network</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">flax</span><span class="o">.</span><span class="n">linen</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
            <span class="n">optimizer_params</span><span class="p">:</span> <span class="n">OptimizerParams</span>
    <span class="p">)</span><span class="o">-&gt;</span> <span class="n">TrainState</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Creates a TrainState object for the actor or the critic.</span>
<span class="sd">        :param rng: Random key for initialization.</span>
<span class="sd">        :param network: The actor or critic network.</span>
<span class="sd">        :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.</span>
<span class="sd">        :return: A TrainState object to be used in training the actor and cirtic networks.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">network</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_network</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">network</span><span class="p">)</span>
        <span class="n">tx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_optimizer</span><span class="p">(</span><span class="n">optimizer_params</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span> <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_create_update_runner</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
            <span class="n">actor_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">critic_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runner</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the update runner as a Runner object. The runner contains batch_size initializations of the</span>
<span class="sd">        environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and</span>
<span class="sd">        one for the critic network, so that trajectory batches are used to train the same parameters.</span>
<span class="sd">        :param rng: Random key for initialization.</span>
<span class="sd">        :param actor_training: The actor TrainState objects used in training.</span>
<span class="sd">        :param critic_training: The critic TrainState objects used in training.</span>
<span class="sd">        :param hyperparams: An instance of HyperParameters for training.</span>
<span class="sd">        :return: An update runner object to be used in trajectory sampling and training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">reset_rng</span><span class="p">,</span> <span class="n">runner_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">reset_rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">reset_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">runner_rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">runner_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">)(</span><span class="n">reset_rngs</span><span class="p">)</span>

        <span class="n">update_runner</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span>
            <span class="n">actor_training</span><span class="o">=</span><span class="n">actor_training</span><span class="p">,</span>
            <span class="n">critic_training</span><span class="o">=</span><span class="n">critic_training</span><span class="p">,</span>
            <span class="n">envstate</span><span class="o">=</span><span class="n">envstate</span><span class="p">,</span>
            <span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="p">,</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">runner_rngs</span><span class="p">,</span>
            <span class="n">hyperparams</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">,</span>
            <span class="n">actor_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">critic_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">update_runner</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_add_next_values</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span>
            <span class="n">last_obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
            <span class="n">critic_training</span><span class="p">:</span> <span class="n">TrainState</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>

        <span class="n">last_state_value_vmap</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">critic_training</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">last_state_value</span> <span class="o">=</span> <span class="n">last_state_value_vmap</span><span class="p">(</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">critic_training</span><span class="o">.</span><span class="n">params</span><span class="p">),</span> <span class="n">last_obs</span><span class="p">)</span>
        <span class="n">last_state_value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">last_state_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;Remove first entry so that the next state values per step are in sync with the state rewards.&quot;&quot;&quot;</span>
        <span class="n">next_values_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
            <span class="n">traj_batch</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
            <span class="n">last_state_value</span>
        <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>

        <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">next_value</span><span class="o">=</span><span class="n">next_values_t</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">traj_batch</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_add_advantages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span> <span class="n">advantage</span><span class="p">:</span> <span class="n">ReturnsType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>

        <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">advantage</span><span class="o">=</span><span class="n">advantage</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">traj_batch</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_returns</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span>
            <span class="n">last_next_state_value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">],</span>
            <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
            <span class="n">gae_lambda</span><span class="p">:</span> <span class="nb">float</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ReturnsType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the</span>
<span class="sd">        trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with</span>
<span class="sd">        episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling</span>
<span class="sd">        step, trajectories do not start at the initial state.</span>
<span class="sd">        :param traj_batch: The batch of trajectories.</span>
<span class="sd">        :param last_next_state_value: The value of the last next state in each trajectory.</span>
<span class="sd">        :param gamma: Discount factor</span>
<span class="sd">        :param gae_lambda: The GAE  factor.</span>
<span class="sd">        :return: The returns over the episodes of the trajectory batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rewards_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">terminated_t</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">terminated</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">discounts_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">terminated_t</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;Remove first entry so that the next state values per step are in sync with the state rewards.&quot;&quot;&quot;</span>
        <span class="n">next_state_values_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span><span class="n">traj_batch</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">last_next_state_value</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]],</span>
            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">:]</span>

        <span class="n">rewards_t</span><span class="p">,</span> <span class="n">discounts_t</span><span class="p">,</span> <span class="n">next_state_values_t</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">discounts_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">gae_lambda</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">discounts_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">gae_lambda</span>

        <span class="n">traj_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">discounts_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">)</span>
        <span class="n">end_value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">next_state_values_t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Start from end of trajectory and work in reverse.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">returns</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trajectory_returns</span><span class="p">,</span> <span class="n">end_value</span><span class="p">,</span> <span class="n">traj_runner</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">returns</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">returns</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">returns</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_advantages</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span>
            <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
            <span class="n">gae_lambda</span><span class="p">:</span> <span class="nb">float</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ReturnsType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the</span>
<span class="sd">        trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with</span>
<span class="sd">        episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling</span>
<span class="sd">        step, trajectories do not start at the initial state.</span>
<span class="sd">        :param traj_batch: The batch of trajectories.</span>
<span class="sd">        :param last_next_state_value: The value of the last next state in each trajectory.</span>
<span class="sd">        :param gamma: Discount factor</span>
<span class="sd">        :param gae_lambda: The GAE  factor.</span>
<span class="sd">        :return: The advantages over the episodes of the trajectory batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rewards_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">values_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">terminated_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">traj_batch</span><span class="o">.</span><span class="n">terminated</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">next_state_values_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">next_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">gamma_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">terminated_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span>
        <span class="n">gae_lambda_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">terminated_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">gae_lambda</span>

        <span class="n">rewards_t</span><span class="p">,</span> <span class="n">values_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">terminated_t</span><span class="p">,</span> <span class="n">gamma_t</span><span class="p">,</span> <span class="n">gae_lambda_t</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">values_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">terminated_t</span><span class="p">,</span> <span class="n">gamma_t</span><span class="p">,</span> <span class="n">gae_lambda_t</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">traj_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">values_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">terminated_t</span><span class="p">,</span> <span class="n">gamma_t</span><span class="p">,</span> <span class="n">gae_lambda_t</span><span class="p">)</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TODO:</span>
<span class="sd">        Advantage of last step is taken from the critic, in contrast to traditional approaches, where the rollout </span>
<span class="sd">        ends with episode termination and the advantage is zero. Training is still successful and the influence of this</span>
<span class="sd">        implementation choice is negligible.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">end_advantage</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">))</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">advantages</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trajectory_advantages</span><span class="p">,</span> <span class="n">end_advantage</span><span class="p">,</span> <span class="n">traj_runner</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">advantages</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">advantages</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_make_rollout_runners</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">StepRunnerType</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner</span>
<span class="sd">        object and broadcasting the TrainState object for the critic and the network in the update_runner object to the</span>
<span class="sd">        same dimension.</span>
<span class="sd">        :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :return: Tuple with step runners to be used in rollout.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rollout_runner</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">envstate</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">rng</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">rollout_runners</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)(</span><span class="o">*</span><span class="n">rollout_runner</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rollout_runners</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_rollout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_runner</span><span class="p">:</span> <span class="n">StepRunnerType</span><span class="p">,</span> <span class="n">i_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">StepRunnerType</span><span class="p">,</span> <span class="n">Transition</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluation of trajectory rollout. In each step the agent:</span>
<span class="sd">        - evaluates policy and value</span>
<span class="sd">        - selects action</span>
<span class="sd">        - performs environment step</span>
<span class="sd">        - creates step transition</span>
<span class="sd">        :param step_runner: A tuple containing information on the environment state, the actor and critic training</span>
<span class="sd">        (parameters and networks) and a random key.</span>
<span class="sd">        :param i_step: Unused, required for lax.scan.</span>
<span class="sd">        :return: The updated step_runner tuple and the rollout step transition.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">envstate</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">step_runner</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">rng_action</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_actions</span><span class="p">(</span><span class="n">rng_action</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>

        <span class="n">values</span> <span class="o">=</span> <span class="n">critic_training</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">critic_training</span><span class="o">.</span><span class="n">params</span><span class="p">),</span> <span class="n">obs</span><span class="p">)</span>

        <span class="n">log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob</span><span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">actor_training</span><span class="o">.</span><span class="n">params</span><span class="p">),</span> <span class="n">obs</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_step</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">envstate</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

        <span class="n">step_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_envstate</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

        <span class="n">terminated</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">]</span>

        <span class="n">transition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_transition</span><span class="p">(</span>
            <span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="p">,</span>
            <span class="n">actions</span><span class="o">=</span><span class="n">actions</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">values</span><span class="p">,</span>
            <span class="n">log_prob</span><span class="o">=</span><span class="n">log_prob</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">next_obs</span><span class="o">=</span><span class="n">next_obs</span><span class="p">,</span>
            <span class="n">terminated</span><span class="o">=</span><span class="n">terminated</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">step_runner</span><span class="p">,</span> <span class="n">transition</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_process_trajectory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not</span>
<span class="sd">        guaranteed to end with termination, the value is estimated using the critic network. This assumption has been</span>
<span class="sd">        shown to have no influence by the end of training.</span>
<span class="sd">        :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :param traj_batch: The batch of trajectories, as collected by in rollout.</span>
<span class="sd">        :param last_state: The state at the end of every trajectory in the batch.</span>
<span class="sd">        :return: A batch of trajectories that includes an estimate of values and advantages.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">traj_batch</span><span class="p">)</span>
        <span class="n">traj_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_next_values</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">)</span>

        <span class="n">advantages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_advantages</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="o">.</span><span class="n">gae_lambda</span><span class="p">)</span>
        <span class="n">traj_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_advantages</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">advantages</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">traj_batch</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_actor_minibatch_update</span><span class="p">(</span>
            <span class="n">i_minibatch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">minibatch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
            <span class="n">grad_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">ActorLossInputType</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;n_minibatch&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be</span>
<span class="sd">        passed. This choice doesn&#39;t hurt performance. To be called using a lambda function for defining grad_fn.</span>
<span class="sd">        :param i_minibatch: Number of minibatch update.</span>
<span class="sd">        :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence.</span>
<span class="sd">        :param grad_fn: The gradient function of the training loss.</span>
<span class="sd">        :return: Minibatch runner with an updated TrainState.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">minibatch_runner</span>
        <span class="o">*</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">actor_loss_input</span>
        <span class="n">traj_minibatch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i_minibatch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">traj_batch</span><span class="p">)</span>
        <span class="n">grad_input_minibatch</span> <span class="o">=</span> <span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="o">*</span><span class="n">traj_minibatch</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="o">*</span><span class="n">grad_input_minibatch</span><span class="p">)</span>
        <span class="n">actor_training</span> <span class="o">=</span> <span class="n">actor_training</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_critic_minibatch_update</span><span class="p">(</span>
            <span class="n">i_minibatch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">minibatch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">],</span>
            <span class="n">grad_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">CriticLossInputType</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be</span>
<span class="sd">        passed. This choice doesn&#39;t hurt performance. To be called using a lambda function for defining grad_fn.</span>
<span class="sd">        :param i_minibatch: Number of minibatch update.</span>
<span class="sd">        :param minibatch_runner: A tuple containing the TranState object and the loss input arguments.</span>
<span class="sd">        :param grad_fn: The gradient function of the training loss.</span>
<span class="sd">        :return: Minibatch runner with an updated TrainState.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span> <span class="o">=</span> <span class="n">minibatch_runner</span>
        <span class="o">*</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">critic_loss_input</span>
        <span class="n">traj_minibatch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i_minibatch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">traj_batch</span><span class="p">)</span>
        <span class="n">grad_input_minibatch</span> <span class="o">=</span> <span class="p">(</span><span class="n">critic_training</span><span class="p">,</span> <span class="o">*</span><span class="n">traj_minibatch</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="o">*</span><span class="n">grad_input_minibatch</span><span class="p">)</span>
        <span class="n">critic_training</span> <span class="o">=</span> <span class="n">critic_training</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_actor_epoch</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">epoch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a Gradient Ascent update of the actor.</span>
<span class="sd">        :param epoch_runner: A tuple containing the following information about the update:</span>
<span class="sd">        - actor_training: TrainState object for actor training</span>
<span class="sd">        - actor_loss_input: Tuple with the inputs required by the actor loss function.</span>
<span class="sd">        - kl: The KL divergence collected during the update (used in checking for early stopping).</span>
<span class="sd">        - epoch: The number of the current training epoch.</span>
<span class="sd">        - kl_threshold: The KL divergence threshold for early stopping.</span>
<span class="sd">        :return: The updated epoch runner.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">kl_threshold</span> <span class="o">=</span> <span class="n">epoch_runner</span>
        <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">n_minibatch_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">minibatch_size</span>
        <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_minibatch_updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_minibatch_fn</span><span class="p">,</span> <span class="n">minibatch_runner</span><span class="p">)</span>
        <span class="n">actor_training</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">minibatch_runner</span>

        <span class="k">return</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">kl_threshold</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_actor_training_cond</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">epoch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been</span>
<span class="sd">        met or due to KL divergence early stopping).</span>
<span class="sd">        :param epoch_runner: A tuple containing the following information about the update:</span>
<span class="sd">        - actor_training: TrainState object for actor training</span>
<span class="sd">        - actor_loss_input: Tuple with the inputs required by the actor loss function.</span>
<span class="sd">        - kl: The KL divergence collected during the update (used in checking for early stopping).</span>
<span class="sd">        - epoch: The number of the current training epoch.</span>
<span class="sd">        - kl_threshold: The KL-divergence threshold for early stopping.</span>
<span class="sd">        :return: Whether the lax.while_loop over training epochs finishes.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">kl_threshold</span> <span class="o">=</span> <span class="n">epoch_runner</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">actor_epochs</span><span class="p">),</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="n">kl</span><span class="p">,</span> <span class="n">kl_threshold</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_actor_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the input and performs Gradient Ascent for the actor network.</span>
<span class="sd">        :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :param traj_batch: The batch of trajectories.</span>
<span class="sd">        :return: The actor training object updated after actor_epochs steps of Gradient Ascent.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">actor_loss_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_loss_input</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>

        <span class="n">start_kl</span><span class="p">,</span> <span class="n">start_epoch</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">1</span>
        <span class="n">actor_epoch_runner</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span>
            <span class="n">actor_loss_input</span><span class="p">,</span>
            <span class="n">start_kl</span><span class="p">,</span>
            <span class="n">start_epoch</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="o">.</span><span class="n">kl_threshold</span>
        <span class="p">)</span>
        <span class="n">actor_epoch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor_training_cond</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_epoch</span><span class="p">,</span> <span class="n">actor_epoch_runner</span><span class="p">)</span>
        <span class="n">actor_training</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">actor_epoch_runner</span>

        <span class="n">actor_loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_loss</span><span class="p">(</span>
            <span class="n">actor_training</span><span class="p">,</span>
            <span class="n">traj_batch</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span>
            <span class="n">traj_batch</span><span class="o">.</span><span class="n">action</span><span class="p">,</span>
            <span class="n">traj_batch</span><span class="o">.</span><span class="n">log_prob</span><span class="p">,</span>
            <span class="n">traj_batch</span><span class="o">.</span><span class="n">advantage</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_critic_epoch</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">i_epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">epoch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a Gradient Descent update of the critic.</span>
<span class="sd">        :param: i_epoch: The current training epoch (unused but required by lax.fori_loop).</span>
<span class="sd">        :param epoch_runner: A tuple containing the following information about the update:</span>
<span class="sd">        - critic_training: TrainState object for critic training</span>
<span class="sd">        - critic_loss_input: Tuple with the inputs required by the critic loss function.</span>
<span class="sd">        :return: The updated epoch runner.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span> <span class="o">=</span> <span class="n">epoch_runner</span>
        <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span><span class="p">)</span>
        <span class="n">n_minibatch_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">minibatch_size</span>
        <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_minibatch_updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_minibatch_fn</span><span class="p">,</span> <span class="n">minibatch_runner</span><span class="p">)</span>
        <span class="n">critic_training</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">minibatch_runner</span>

        <span class="k">return</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_critic_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span>  <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the input and performs Gradient Descent for the critic network.</span>
<span class="sd">        :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :param traj_batch: The batch of trajectories.</span>
<span class="sd">        :return: The critic training object updated after actor_epochs steps of Gradient Ascent.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">critic_loss_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_loss_input</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>
        <span class="n">critic_epoch_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">update_runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span><span class="p">)</span>
        <span class="n">critic_epoch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">critic_epochs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_epoch</span><span class="p">,</span> <span class="n">critic_epoch_runner</span><span class="p">)</span>
        <span class="n">critic_training</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">critic_epoch_runner</span>

        <span class="n">critic_targets</span> <span class="o">=</span> <span class="n">critic_loss_input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rollout_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">)</span>
        <span class="n">critic_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_loss</span><span class="p">(</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span> <span class="n">critic_targets</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_update_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_update_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runner</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        An update step of the actor and critic networks. This entails:</span>
<span class="sd">        - performing rollout for sampling a batch of trajectories.</span>
<span class="sd">        - assessing the value of the last state per trajectory using the critic.</span>
<span class="sd">        - evaluating the advantage per trajectory.</span>
<span class="sd">        - updating the actor and critic network parameters via the respective loss functions.</span>
<span class="sd">        - generating in-training performance metrics.</span>
<span class="sd">        In this approach, the update_runner already has a batch of environments initialized. The environments are not</span>
<span class="sd">        initialized in the beginning of every update step, which means that trajectories to not necessarily start from</span>
<span class="sd">        an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan</span>
<span class="sd">        for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be</span>
<span class="sd">        truncated in trajectory sampling).</span>
<span class="sd">        :param i_update_step: Unused, required for progressbar.</span>
<span class="sd">        :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :return: The updated runner</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rollout_runners</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_rollout_runners</span><span class="p">(</span><span class="n">update_runner</span><span class="p">)</span>
        <span class="n">scan_rollout_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_rollout</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rollout_length</span><span class="p">)</span>
        <span class="n">rollout_runners</span><span class="p">,</span> <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">scan_rollout_fn</span><span class="p">)(</span><span class="n">rollout_runners</span><span class="p">)</span>
        <span class="n">last_envstate</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">rollout_runners</span>
        <span class="n">traj_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_trajectory</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">)</span>

        <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_update</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>
        <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_update</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update runner as a dataclass.&quot;&quot;&quot;</span>
        <span class="n">update_runner</span> <span class="o">=</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
            <span class="n">envstate</span><span class="o">=</span><span class="n">last_envstate</span><span class="p">,</span>
            <span class="n">obs</span><span class="o">=</span><span class="n">last_obs</span><span class="p">,</span>
            <span class="n">actor_training</span><span class="o">=</span><span class="n">actor_training</span><span class="p">,</span>
            <span class="n">critic_training</span><span class="o">=</span><span class="n">critic_training</span><span class="p">,</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
            <span class="n">actor_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">critic_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">update_runner</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_checkpoint</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span>
            <span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]],</span>
            <span class="n">i_training_step</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps the base checkpointing method in a Python callback.</span>
<span class="sd">        :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :param metrics: Dictionary of evaluation metrics (return per environment evaluation)</span>
<span class="sd">        :param i_training_step: Training step</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">jax</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">io_callback</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_base</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">i_training_step</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_checkpoint_base</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span>
            <span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]],</span>
            <span class="n">i_training_step</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following:</span>
<span class="sd">        - The training runner object.</span>
<span class="sd">        - Returns of the evaluation episodes</span>
<span class="sd">        The average return over the evaluated episodes is used as the checkpoint metric.</span>
<span class="sd">        :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :param metrics: Dictionary of evaluation metrics (return per episode evaluation)</span>
<span class="sd">        :param i_training_step: Training step</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>

            <span class="n">ckpt</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;runner&quot;</span><span class="p">:</span> <span class="n">update_runner</span><span class="p">,</span>
                <span class="s2">&quot;terminated&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">],</span>
                <span class="s2">&quot;truncated&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">],</span>
                <span class="s2">&quot;final_rewards&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;final_rewards&quot;</span><span class="p">],</span>
                <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;returns&quot;</span><span class="p">]</span>
            <span class="p">}</span>

            <span class="n">save_args</span> <span class="o">=</span> <span class="n">orbax_utils</span><span class="o">.</span><span class="n">save_args_from_target</span><span class="p">(</span><span class="n">ckpt</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                <span class="c1"># Use maximum number of steps reached in previous training. Set to zero by default during agent</span>
                <span class="c1"># initialization if a new training is executed. In case of continuing training, the checkpoint of step</span>
                <span class="c1"># zero replaces the last checkpoint of the previous training. The two checkpoints are the same.</span>
                <span class="n">i_training_step</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">previous_training_max_step</span><span class="p">,</span>
                <span class="n">ckpt</span><span class="p">,</span>
                <span class="n">save_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;save_args&#39;</span><span class="p">:</span> <span class="n">save_args</span><span class="p">},</span>
            <span class="p">)</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_training_step</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span>
            <span class="n">i_training_batch</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Runner</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs trainings steps to update the agent per training batch.</span>
<span class="sd">        :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">        :param i_training_batch: Training batch loop counter.</span>
<span class="sd">        :return: Tuple with updated runner and dictionary of metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">n_training_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span> <span class="o">*</span> <span class="n">i_training_batch</span>
        <span class="n">n_training_steps</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">n_training_steps</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span><span class="p">)</span>

        <span class="n">update_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_training_steps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_step</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_metrics</span><span class="p">(</span><span class="n">runner</span><span class="o">=</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">update_step</span><span class="o">=</span><span class="n">i_training_batch</span><span class="p">)</span>
            <span class="n">i_training_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span> <span class="o">*</span> <span class="p">(</span><span class="n">i_training_batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">i_training_step</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">i_training_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">i_training_step</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">return</span> <span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
            <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Runner</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains the agents. A jax_tqdm progressbar has been added in the lax.scan loop.</span>
<span class="sd">        :param rng: Random key for initialization. This is the original key for training.</span>
<span class="sd">        :param hyperparams: An instance of HyperParameters for training.</span>
<span class="sd">        :return: The final state of the step runner after training and the training metrics accumulated over all</span>
<span class="sd">                 training batches and steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rng</span><span class="p">,</span> <span class="o">*</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">actor_init_rng</span><span class="p">,</span> <span class="n">critic_init_rng</span><span class="p">,</span> <span class="n">runner_rng</span> <span class="o">=</span> <span class="n">_rng</span>

        <span class="n">actor_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_training</span><span class="p">(</span>
            <span class="n">actor_init_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">actor_network</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">actor_optimizer_params</span>
        <span class="p">)</span>
        <span class="n">critic_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_training</span><span class="p">(</span>
            <span class="n">critic_init_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">critic_network</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">critic_optimizer_params</span>
        <span class="p">)</span>

        <span class="n">update_runner</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_update_runner</span><span class="p">(</span><span class="n">runner_rng</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>

        <span class="c1"># Checkpoint initial state</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
            <span class="n">metrics_start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_metrics</span><span class="p">(</span><span class="n">runner</span><span class="o">=</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">update_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics_start</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">previous_training_max_step</span><span class="p">)</span>

        <span class="c1"># Initialize agent updating functions, which can be avoided to be done within the training loops.</span>
        <span class="n">actor_grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor_loss</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_int</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_actor_minibatch_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_minibatch_update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">actor_grad_fn</span><span class="p">)</span>

        <span class="n">critic_grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_critic_loss</span><span class="p">,</span> <span class="n">allow_int</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_critic_minibatch_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_minibatch_update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">critic_grad_fn</span><span class="p">)</span>

        <span class="c1"># Train, evaluate, checkpoint</span>
        <span class="n">n_training_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span>
        <span class="n">progressbar_desc</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Training batch (training steps = batch x </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span><span class="si">}</span><span class="s1">)&#39;</span>

        <span class="n">runner</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
            <span class="n">scan_tqdm</span><span class="p">(</span><span class="n">n_training_batches</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="n">progressbar_desc</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">_training_step</span><span class="p">),</span>
            <span class="n">update_runner</span><span class="p">,</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_training_batches</span><span class="p">),</span>
            <span class="n">n_training_batches</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">key</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">metrics_start</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">jnp</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span> <span class="n">metrics</span><span class="p">[</span><span class="n">key</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">metrics</span><span class="o">=</span> <span class="p">{}</span>

        <span class="k">return</span> <span class="n">runner</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_trajectory_returns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">traj</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the returns per episode step over a batch of trajectories.</span>
<span class="sd">        :param value: The values of the steps in the trajectory according to the critic (including the one of the last</span>
<span class="sd">         state).</span>
<span class="sd">        :param traj: The trajectory batch.</span>
<span class="sd">        :return: A tuple of returns.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_trajectory_advantages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">traj</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the advantages per episode step over a batch of trajectories.</span>
<span class="sd">        :param value: The values of the steps in the trajectory according to the critic (including the one of the last</span>
<span class="sd">         state).</span>
<span class="sd">        :param traj: The trajectory batch.</span>
<span class="sd">        :return: An array of returns.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_actor_loss</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">obs</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size obs_size&quot;</span><span class="p">],</span>
            <span class="n">action</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size&quot;</span><span class="p">],</span>
            <span class="n">log_prob_old</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size&quot;</span><span class="p">],</span>
            <span class="n">advantage</span><span class="p">:</span> <span class="n">ReturnsType</span><span class="p">,</span>
            <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
    <span class="p">)</span><span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the</span>
<span class="sd">        discounted returns and the value as estimated by the critic.</span>
<span class="sd">        :param training: The actor TrainState object.</span>
<span class="sd">        :param obs: The obs in the trajectory batch.</span>
<span class="sd">        :param action: The actions in the trajectory batch.</span>
<span class="sd">        :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.</span>
<span class="sd">        :param advantage: The advantage over the trajectory batch.</span>
<span class="sd">        :param hyperparams: The HyperParameters object used for training.</span>
<span class="sd">        :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_critic_loss</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">obs</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size obs_size&quot;</span><span class="p">],</span>
            <span class="n">targets</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size n_rollout&quot;</span><span class="p">],</span>
            <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the critic loss.</span>
<span class="sd">        :param training: The critic TrainState object.</span>
<span class="sd">        :param obs: The obs in the trajectory batch.</span>
<span class="sd">        :param targets: The returns over the trajectory batch, which act as the targets for training the critic.</span>
<span class="sd">        :param hyperparams: The HyperParameters object used for training.</span>
<span class="sd">        :return: The critic loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_actor_loss_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ActorLossInputType</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the input required by the actor loss function. The input is reshaped so that it is split into</span>
<span class="sd">        minibatches.</span>
<span class="sd">        :param update_runner: The runner object used in training.</span>
<span class="sd">        :param traj_batch: The batch of trajectories.</span>
<span class="sd">        :return: A tuple of input to the actor loss function.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_critic_loss_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CriticLossInputType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the input required by the critic loss function. The input is reshaped so that it is split into</span>
<span class="sd">        minibatches.</span>
<span class="sd">        :param update_runner: The Runner object used in training.</span>
<span class="sd">        :param traj_batch: The batch of trajectories.</span>
<span class="sd">        :return: A tuple of input to the critic loss function.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">)</span><span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="bp">NotImplemented</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_log_prob</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">params</span><span class="p">:</span> <span class="n">FrozenDict</span><span class="p">,</span>
            <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
            <span class="n">action</span><span class="p">:</span> <span class="n">ActionType</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="bp">NotImplemented</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_sample_actions</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
            <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActionType</span><span class="p">:</span>
        <span class="k">raise</span> <span class="bp">NotImplemented</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; METHODS FOR APPLYING AGENT&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActionType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state.</span>
<span class="sd">        :param obs: The current obs of the episode step in array format.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="bp">NotImplemented</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_eval_agent</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
            <span class="n">actor_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">critic_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
            <span class="n">n_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the agents for n_episodes complete episodes using &#39;lax.while_loop&#39;.</span>
<span class="sd">        :param rng: A random key used for evaluating the agent.</span>
<span class="sd">        :param actor_training: The actor TrainState object (either mid- or post-training).</span>
<span class="sd">        :param critic_training: The critic TrainState object (either mid- or post-training).</span>
<span class="sd">        :param n_episodes: The update_runner object used during training.</span>
<span class="sd">        :return: The sum of rewards collected over n_episodes episodes.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rng_eval</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_episodes</span><span class="p">)</span>
        <span class="n">rng</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">)(</span><span class="n">rng_eval</span><span class="p">)</span>

        <span class="n">eval_runner</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">envstate</span><span class="p">,</span>
            <span class="n">obs</span><span class="p">,</span>
            <span class="n">actor_training</span><span class="p">,</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">),</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">),</span>
            <span class="n">rng</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">eval_runners</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span>
            <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)(</span><span class="o">*</span><span class="n">eval_runner</span><span class="p">)</span>

        <span class="n">eval_runner</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">lax</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_cond</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_body</span><span class="p">,</span> <span class="n">x</span><span class="p">))(</span><span class="n">eval_runners</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">final_rewards</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">eval_runner</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_metrics</span><span class="p">(</span><span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">final_rewards</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_eval_metrics</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">terminated</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
            <span class="n">truncated</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
            <span class="n">final_rewards</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
            <span class="n">returns</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the metrics.</span>
<span class="sd">        :param terminated: Whether the episode finished by termination.</span>
<span class="sd">        :param truncated: Whether the episode finished by truncation.</span>
<span class="sd">        :param final_rewards: The rewards collected in the final step of the episode.</span>
<span class="sd">        :param returns: The sum of rewards collected during the episode.</span>
<span class="sd">        :return: Dictionary combining the input arguments and the case-specific special metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;terminated&quot;</span><span class="p">:</span> <span class="n">terminated</span><span class="p">,</span>
            <span class="s2">&quot;truncated&quot;</span><span class="p">:</span> <span class="n">truncated</span><span class="p">,</span>
            <span class="s2">&quot;final_rewards&quot;</span><span class="p">:</span> <span class="n">final_rewards</span><span class="p">,</span>
            <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="n">returns</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">metrics</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_eval_body</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_runner</span><span class="p">:</span> <span class="n">EvalRunnerType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvalRunnerType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A step in the episode to be used with &#39;lax.while_loop&#39; for evaluation of the agent in a complete episode.</span>
<span class="sd">        :param eval_runner: A tuple containing information about the environment state, the actor and critic training</span>
<span class="sd">        states, whether the episode is terminated (for checking the condition in &#39;lax.while_loop&#39;), the sum of rewards</span>
<span class="sd">        over the episode and a random key.</span>
<span class="sd">        :return: The updated eval_runner tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">envstate</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">eval_runner</span>

        <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_step</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">envstate</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

        <span class="n">terminated</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">]</span>
        <span class="n">truncated</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">]</span>

        <span class="n">returns</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="n">eval_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_envstate</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">eval_runner</span>

    <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_eval_cond</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_runner</span><span class="p">:</span> <span class="n">EvalRunnerType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks whether the episode is terminated, meaning that the &#39;lax.while_loop&#39; can stop.</span>
<span class="sd">        :param eval_runner: A tuple containing information about the environment state, the actor and critic training</span>
<span class="sd">        states, whether the episode is terminated (for checking the condition in &#39;lax.while_loop&#39;), the sum of rewards</span>
<span class="sd">        over the episode and a random key.</span>
<span class="sd">        :return: Whether the episode is terminated, which means that the while loop must stop.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">eval_runner</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">terminated</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">truncated</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span> <span class="n">n_evals</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_evals&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the trained agent&#39;s performance post-training using the trained agent&#39;s actor and critic.</span>
<span class="sd">        :param rng: Random key for evaluation.</span>
<span class="sd">        :param n_evals: Number of steps in agent evaluation.</span>
<span class="sd">        :return: Dictionary of evaluation metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">eval_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_agent</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">n_evals</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">eval_metrics</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; METHODS FOR POST-PROCESSING &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_hyperparams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Logs training hyperparameters in a text file. To be used outside training.</span>
<span class="sd">        :param hyperparams: An instance of HyperParameters for training.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">field</span> <span class="o">+</span> <span class="s1">&#39;: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">,</span> <span class="n">field</span><span class="p">))</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">_fields</span><span class="p">]</span>
        <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Hyperparameters:&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_lst</span>
        <span class="n">output_lst</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_lst</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s1">&#39;hyperparameters.txt&#39;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output_lst</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">collect_training</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">runner</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Runner</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">previous_training_max_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Collects training or restored checkpoint of output (the final state of the runner after training and the</span>
<span class="sd">        collected metrics).</span>
<span class="sd">        :param runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">        critic&#39;s training, the state of the environment and training hyperparameters. This is at the state reached at</span>
<span class="sd">        the end of training.</span>
<span class="sd">        :param metrics: Dictionary of evaluation metrics (return per environment evaluation)</span>
<span class="sd">        :param previous_training_max_step: Maximum step reached during training.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">agent_trained</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">previous_training_max_step</span> <span class="o">=</span> <span class="n">previous_training_max_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_runner</span> <span class="o">=</span> <span class="n">runner</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_metrics</span> <span class="o">=</span> <span class="n">metrics</span>
        <span class="n">n_evals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_steps_in_training</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_evals</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pp</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_pp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Post-processes the training results, which includes:</span>
<span class="sd">            - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored).</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">actor_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_runner</span><span class="o">.</span><span class="n">actor_training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_runner</span><span class="o">.</span><span class="n">critic_training</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">summarize</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">metrics</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="s2">&quot;size_metrics&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;size_metrics&quot;</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MetricStats</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Summarizes collection of per-episode metrics.</span>
<span class="sd">        :param metrics: Metric per episode.</span>
<span class="sd">        :return: Summary of metric per episode.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">MetricStats</span><span class="p">(</span>
            <span class="n">episode_metric</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
            <span class="n">mean</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">var</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">std</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="nb">min</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="nb">max</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">median</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">has_nans</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">metrics</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase._actor_minibatch_fn" class="doc doc-heading">
            <code class=" language-python"><span class="n">_actor_minibatch_fn</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._actor_minibatch_fn" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase._critic_minibatch_fn" class="doc doc-heading">
            <code class=" language-python"><span class="n">_critic_minibatch_fn</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._critic_minibatch_fn" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.actor_training" class="doc doc-heading">
            <code class=" language-python"><span class="n">actor_training</span> <span class="o">=</span> <span class="kc">None</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.actor_training" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.agent_trained" class="doc doc-heading">
            <code class=" language-python"><span class="n">agent_trained</span> <span class="o">=</span> <span class="kc">False</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.agent_trained" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.config" class="doc doc-heading">
            <code class=" language-python"><span class="n">config</span> <span class="o">=</span> <span class="n">config</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.config" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.critic_training" class="doc doc-heading">
            <code class=" language-python"><span class="n">critic_training</span> <span class="o">=</span> <span class="kc">None</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.critic_training" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.eval_during_training" class="doc doc-heading">
            <code class=" language-python"><span class="n">eval_during_training</span> <span class="o">=</span> <span class="n">eval_during_training</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.eval_during_training" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.n_actors" class="doc doc-heading">
            <code class=" language-python"><span class="n">n_actors</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">n_actors</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.n_actors" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.previous_training_max_step" class="doc doc-heading">
            <code class=" language-python"><span class="n">previous_training_max_step</span> <span class="o">=</span> <span class="mi">0</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.previous_training_max_step" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.training_metrics" class="doc doc-heading">
            <code class=" language-python"><span class="n">training_metrics</span> <span class="o">=</span> <span class="kc">None</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.training_metrics" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h2 id="jaxagents.ippo.IPPOBase.training_runner" class="doc doc-heading">
            <code class=" language-python"><span class="n">training_runner</span> <span class="o">=</span> <span class="kc">None</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.training_runner" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

    </div>

</div>



<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_params</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">eval_during_training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.__init__" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>:param env: A gymnax or custom environment that inherits from the basic gymnax class.
:param env_params: A dataclass named "EnvParams" containing the parametrization of the environment.
:param config: The configuration of the agent as and AgentConfig object (from vpf_utils).
:param eval_during_training: Whether evaluation should be performed during training.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">env</span><span class="p">:</span> <span class="n">Environment</span><span class="p">,</span>
        <span class="n">env_params</span><span class="p">:</span> <span class="n">EnvParams</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AgentConfig</span><span class="p">,</span>
        <span class="n">eval_during_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :param env: A gymnax or custom environment that inherits from the basic gymnax class.</span>
<span class="sd">    :param env_params: A dataclass named &quot;EnvParams&quot; containing the parametrization of the environment.</span>
<span class="sd">    :param config: The configuration of the agent as and AgentConfig object (from vpf_utils).</span>
<span class="sd">    :param eval_during_training: Whether evaluation should be performed during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">n_actors</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span> <span class="o">=</span> <span class="n">eval_during_training</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_init_checkpointer</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_init_env</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_params</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.__str__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__str__</span><span class="p">()</span></code>

<a href="#jaxagents.ippo.IPPOBase.__str__" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Returns a string containing only the non-default field values.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a string containing only the non-default field values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">field</span> <span class="o">+</span> <span class="s1">&#39;: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">field</span><span class="p">))</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_fields</span><span class="p">]</span>
    <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Agent configuration:&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_lst</span>

    <span class="k">return</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_lst</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._actor_epoch" class="doc doc-heading">
            <code class=" language-python"><span class="n">_actor_epoch</span><span class="p">(</span><span class="n">epoch_runner</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._actor_epoch" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Performs a Gradient Ascent update of the actor.
:param epoch_runner: A tuple containing the following information about the update:
- actor_training: TrainState object for actor training
- actor_loss_input: Tuple with the inputs required by the actor loss function.
- kl: The KL divergence collected during the update (used in checking for early stopping).
- epoch: The number of the current training epoch.
- kl_threshold: The KL divergence threshold for early stopping.
:return: The updated epoch runner.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_actor_epoch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">epoch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a Gradient Ascent update of the actor.</span>
<span class="sd">    :param epoch_runner: A tuple containing the following information about the update:</span>
<span class="sd">    - actor_training: TrainState object for actor training</span>
<span class="sd">    - actor_loss_input: Tuple with the inputs required by the actor loss function.</span>
<span class="sd">    - kl: The KL divergence collected during the update (used in checking for early stopping).</span>
<span class="sd">    - epoch: The number of the current training epoch.</span>
<span class="sd">    - kl_threshold: The KL divergence threshold for early stopping.</span>
<span class="sd">    :return: The updated epoch runner.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">kl_threshold</span> <span class="o">=</span> <span class="n">epoch_runner</span>
    <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">n_minibatch_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">minibatch_size</span>
    <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_minibatch_updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_minibatch_fn</span><span class="p">,</span> <span class="n">minibatch_runner</span><span class="p">)</span>
    <span class="n">actor_training</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">minibatch_runner</span>

    <span class="k">return</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">kl_threshold</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._actor_loss" class="doc doc-heading">
            <code class=" language-python"><span class="n">_actor_loss</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">log_prob_old</span><span class="p">,</span> <span class="n">advantage</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._actor_loss" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the
discounted returns and the value as estimated by the critic.
:param training: The actor TrainState object.
:param obs: The obs in the trajectory batch.
:param action: The actions in the trajectory batch.
:param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.
:param advantage: The advantage over the trajectory batch.
:param hyperparams: The HyperParameters object used for training.
:return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_actor_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size obs_size&quot;</span><span class="p">],</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size&quot;</span><span class="p">],</span>
        <span class="n">log_prob_old</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size&quot;</span><span class="p">],</span>
        <span class="n">advantage</span><span class="p">:</span> <span class="n">ReturnsType</span><span class="p">,</span>
        <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
<span class="p">)</span><span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the</span>
<span class="sd">    discounted returns and the value as estimated by the critic.</span>
<span class="sd">    :param training: The actor TrainState object.</span>
<span class="sd">    :param obs: The obs in the trajectory batch.</span>
<span class="sd">    :param action: The actions in the trajectory batch.</span>
<span class="sd">    :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.</span>
<span class="sd">    :param advantage: The advantage over the trajectory batch.</span>
<span class="sd">    :param hyperparams: The HyperParameters object used for training.</span>
<span class="sd">    :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._actor_loss_input" class="doc doc-heading">
            <code class=" language-python"><span class="n">_actor_loss_input</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._actor_loss_input" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Prepares the input required by the actor loss function. The input is reshaped so that it is split into
minibatches.
:param update_runner: The runner object used in training.
:param traj_batch: The batch of trajectories.
:return: A tuple of input to the actor loss function.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_actor_loss_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ActorLossInputType</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares the input required by the actor loss function. The input is reshaped so that it is split into</span>
<span class="sd">    minibatches.</span>
<span class="sd">    :param update_runner: The runner object used in training.</span>
<span class="sd">    :param traj_batch: The batch of trajectories.</span>
<span class="sd">    :return: A tuple of input to the actor loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._actor_minibatch_update" class="doc doc-heading">
            <code class=" language-python"><span class="n">_actor_minibatch_update</span><span class="p">(</span><span class="n">i_minibatch</span><span class="p">,</span> <span class="n">minibatch_runner</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._actor_minibatch_update" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be
passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.
:param i_minibatch: Number of minibatch update.
:param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence.
:param grad_fn: The gradient function of the training loss.
:return: Minibatch runner with an updated TrainState.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_actor_minibatch_update</span><span class="p">(</span>
        <span class="n">i_minibatch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">minibatch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
        <span class="n">grad_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">ActorLossInputType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;n_minibatch&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be</span>
<span class="sd">    passed. This choice doesn&#39;t hurt performance. To be called using a lambda function for defining grad_fn.</span>
<span class="sd">    :param i_minibatch: Number of minibatch update.</span>
<span class="sd">    :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence.</span>
<span class="sd">    :param grad_fn: The gradient function of the training loss.</span>
<span class="sd">    :return: Minibatch runner with an updated TrainState.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">minibatch_runner</span>
    <span class="o">*</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">actor_loss_input</span>
    <span class="n">traj_minibatch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i_minibatch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">traj_batch</span><span class="p">)</span>
    <span class="n">grad_input_minibatch</span> <span class="o">=</span> <span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="o">*</span><span class="n">traj_minibatch</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="o">*</span><span class="n">grad_input_minibatch</span><span class="p">)</span>
    <span class="n">actor_training</span> <span class="o">=</span> <span class="n">actor_training</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss_input</span><span class="p">,</span> <span class="n">kl</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._actor_training_cond" class="doc doc-heading">
            <code class=" language-python"><span class="n">_actor_training_cond</span><span class="p">(</span><span class="n">epoch_runner</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._actor_training_cond" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been
met or due to KL divergence early stopping).
:param epoch_runner: A tuple containing the following information about the update:
- actor_training: TrainState object for actor training
- actor_loss_input: Tuple with the inputs required by the actor loss function.
- kl: The KL divergence collected during the update (used in checking for early stopping).
- epoch: The number of the current training epoch.
- kl_threshold: The KL-divergence threshold for early stopping.
:return: Whether the lax.while_loop over training epochs finishes.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_actor_training_cond</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">epoch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">ActorLossInputType</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been</span>
<span class="sd">    met or due to KL divergence early stopping).</span>
<span class="sd">    :param epoch_runner: A tuple containing the following information about the update:</span>
<span class="sd">    - actor_training: TrainState object for actor training</span>
<span class="sd">    - actor_loss_input: Tuple with the inputs required by the actor loss function.</span>
<span class="sd">    - kl: The KL divergence collected during the update (used in checking for early stopping).</span>
<span class="sd">    - epoch: The number of the current training epoch.</span>
<span class="sd">    - kl_threshold: The KL-divergence threshold for early stopping.</span>
<span class="sd">    :return: Whether the lax.while_loop over training epochs finishes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">kl_threshold</span> <span class="o">=</span> <span class="n">epoch_runner</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">actor_epochs</span><span class="p">),</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="n">kl</span><span class="p">,</span> <span class="n">kl_threshold</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._actor_update" class="doc doc-heading">
            <code class=" language-python"><span class="n">_actor_update</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._actor_update" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Prepares the input and performs Gradient Ascent for the actor network.
:param update_runner: The Runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:param traj_batch: The batch of trajectories.
:return: The actor training object updated after actor_epochs steps of Gradient Ascent.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_actor_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares the input and performs Gradient Ascent for the actor network.</span>
<span class="sd">    :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :param traj_batch: The batch of trajectories.</span>
<span class="sd">    :return: The actor training object updated after actor_epochs steps of Gradient Ascent.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">actor_loss_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_loss_input</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>

    <span class="n">start_kl</span><span class="p">,</span> <span class="n">start_epoch</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">1</span>
    <span class="n">actor_epoch_runner</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span>
        <span class="n">actor_loss_input</span><span class="p">,</span>
        <span class="n">start_kl</span><span class="p">,</span>
        <span class="n">start_epoch</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="o">.</span><span class="n">kl_threshold</span>
    <span class="p">)</span>
    <span class="n">actor_epoch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor_training_cond</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_epoch</span><span class="p">,</span> <span class="n">actor_epoch_runner</span><span class="p">)</span>
    <span class="n">actor_training</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">actor_epoch_runner</span>

    <span class="n">actor_loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_loss</span><span class="p">(</span>
        <span class="n">actor_training</span><span class="p">,</span>
        <span class="n">traj_batch</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span>
        <span class="n">traj_batch</span><span class="o">.</span><span class="n">action</span><span class="p">,</span>
        <span class="n">traj_batch</span><span class="o">.</span><span class="n">log_prob</span><span class="p">,</span>
        <span class="n">traj_batch</span><span class="o">.</span><span class="n">advantage</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._add_advantages" class="doc doc-heading">
            <code class=" language-python"><span class="n">_add_advantages</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">advantage</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._add_advantages" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_add_advantages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span> <span class="n">advantage</span><span class="p">:</span> <span class="n">ReturnsType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>

    <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">advantage</span><span class="o">=</span><span class="n">advantage</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">traj_batch</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._add_next_values" class="doc doc-heading">
            <code class=" language-python"><span class="n">_add_next_values</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._add_next_values" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_add_next_values</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span>
        <span class="n">last_obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
        <span class="n">critic_training</span><span class="p">:</span> <span class="n">TrainState</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>

    <span class="n">last_state_value_vmap</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">critic_training</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">last_state_value</span> <span class="o">=</span> <span class="n">last_state_value_vmap</span><span class="p">(</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">critic_training</span><span class="o">.</span><span class="n">params</span><span class="p">),</span> <span class="n">last_obs</span><span class="p">)</span>
    <span class="n">last_state_value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">last_state_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;Remove first entry so that the next state values per step are in sync with the state rewards.&quot;&quot;&quot;</span>
    <span class="n">next_values_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
        <span class="n">traj_batch</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
        <span class="n">last_state_value</span>
    <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>

    <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">next_value</span><span class="o">=</span><span class="n">next_values_t</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">traj_batch</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._advantages" class="doc doc-heading">
            <code class=" language-python"><span class="n">_advantages</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._advantages" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the
trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with
episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling
step, trajectories do not start at the initial state.
:param traj_batch: The batch of trajectories.
:param last_next_state_value: The value of the last next state in each trajectory.
:param gamma: Discount factor
:param gae_lambda: The GAE  factor.
:return: The advantages over the episodes of the trajectory batch.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_advantages</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">gae_lambda</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ReturnsType</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the</span>
<span class="sd">    trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with</span>
<span class="sd">    episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling</span>
<span class="sd">    step, trajectories do not start at the initial state.</span>
<span class="sd">    :param traj_batch: The batch of trajectories.</span>
<span class="sd">    :param last_next_state_value: The value of the last next state in each trajectory.</span>
<span class="sd">    :param gamma: Discount factor</span>
<span class="sd">    :param gae_lambda: The GAE  factor.</span>
<span class="sd">    :return: The advantages over the episodes of the trajectory batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rewards_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">values_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">terminated_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">traj_batch</span><span class="o">.</span><span class="n">terminated</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">next_state_values_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">next_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">gamma_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">terminated_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span>
    <span class="n">gae_lambda_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">terminated_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">gae_lambda</span>

    <span class="n">rewards_t</span><span class="p">,</span> <span class="n">values_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">terminated_t</span><span class="p">,</span> <span class="n">gamma_t</span><span class="p">,</span> <span class="n">gae_lambda_t</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">values_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">terminated_t</span><span class="p">,</span> <span class="n">gamma_t</span><span class="p">,</span> <span class="n">gae_lambda_t</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">traj_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">values_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">terminated_t</span><span class="p">,</span> <span class="n">gamma_t</span><span class="p">,</span> <span class="n">gae_lambda_t</span><span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TODO:</span>
<span class="sd">    Advantage of last step is taken from the critic, in contrast to traditional approaches, where the rollout </span>
<span class="sd">    ends with episode termination and the advantage is zero. Training is still successful and the influence of this</span>
<span class="sd">    implementation choice is negligible.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">end_advantage</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">))</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">advantages</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trajectory_advantages</span><span class="p">,</span> <span class="n">end_advantage</span><span class="p">,</span> <span class="n">traj_runner</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">advantages</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">advantages</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">advantages</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._checkpoint" class="doc doc-heading">
            <code class=" language-python"><span class="n">_checkpoint</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">i_training_step</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._checkpoint" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Wraps the base checkpointing method in a Python callback.
:param update_runner: The runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:param metrics: Dictionary of evaluation metrics (return per environment evaluation)
:param i_training_step: Training step
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_checkpoint</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]],</span>
        <span class="n">i_training_step</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wraps the base checkpointing method in a Python callback.</span>
<span class="sd">    :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :param metrics: Dictionary of evaluation metrics (return per environment evaluation)</span>
<span class="sd">    :param i_training_step: Training step</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">jax</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">io_callback</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_base</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">i_training_step</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._checkpoint_base" class="doc doc-heading">
            <code class=" language-python"><span class="n">_checkpoint_base</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">i_training_step</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._checkpoint_base" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following:
- The training runner object.
- Returns of the evaluation episodes
The average return over the evaluated episodes is used as the checkpoint metric.
:param update_runner: The runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:param metrics: Dictionary of evaluation metrics (return per episode evaluation)
:param i_training_step: Training step
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span>
<span class="normal">978</span>
<span class="normal">979</span>
<span class="normal">980</span>
<span class="normal">981</span>
<span class="normal">982</span>
<span class="normal">983</span>
<span class="normal">984</span>
<span class="normal">985</span>
<span class="normal">986</span>
<span class="normal">987</span>
<span class="normal">988</span>
<span class="normal">989</span>
<span class="normal">990</span>
<span class="normal">991</span>
<span class="normal">992</span>
<span class="normal">993</span>
<span class="normal">994</span>
<span class="normal">995</span>
<span class="normal">996</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_checkpoint_base</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]],</span>
        <span class="n">i_training_step</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following:</span>
<span class="sd">    - The training runner object.</span>
<span class="sd">    - Returns of the evaluation episodes</span>
<span class="sd">    The average return over the evaluated episodes is used as the checkpoint metric.</span>
<span class="sd">    :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :param metrics: Dictionary of evaluation metrics (return per episode evaluation)</span>
<span class="sd">    :param i_training_step: Training step</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>

        <span class="n">ckpt</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;runner&quot;</span><span class="p">:</span> <span class="n">update_runner</span><span class="p">,</span>
            <span class="s2">&quot;terminated&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">],</span>
            <span class="s2">&quot;truncated&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">],</span>
            <span class="s2">&quot;final_rewards&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;final_rewards&quot;</span><span class="p">],</span>
            <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;returns&quot;</span><span class="p">]</span>
        <span class="p">}</span>

        <span class="n">save_args</span> <span class="o">=</span> <span class="n">orbax_utils</span><span class="o">.</span><span class="n">save_args_from_target</span><span class="p">(</span><span class="n">ckpt</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="c1"># Use maximum number of steps reached in previous training. Set to zero by default during agent</span>
            <span class="c1"># initialization if a new training is executed. In case of continuing training, the checkpoint of step</span>
            <span class="c1"># zero replaces the last checkpoint of the previous training. The two checkpoints are the same.</span>
            <span class="n">i_training_step</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">previous_training_max_step</span><span class="p">,</span>
            <span class="n">ckpt</span><span class="p">,</span>
            <span class="n">save_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;save_args&#39;</span><span class="p">:</span> <span class="n">save_args</span><span class="p">},</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._create_empty_trainstate" class="doc doc-heading">
            <code class=" language-python"><span class="n">_create_empty_trainstate</span><span class="p">(</span><span class="n">network</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._create_empty_trainstate" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Creates an empty TrainState object for restoring checkpoints.
:param network: The actor or critic network.
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_create_empty_trainstate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainState</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates an empty TrainState object for restoring checkpoints.</span>
<span class="sd">    :param network: The actor or critic network.</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Just a dummy PRNGKey for initializing the networks parameters.</span>
    <span class="n">network</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_network</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">network</span><span class="p">)</span>

    <span class="n">optimizer_params</span> <span class="o">=</span> <span class="n">OptimizerParams</span><span class="p">()</span>  <span class="c1"># Use the default values of the OptimizerParams object.</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_optimizer</span><span class="p">(</span><span class="n">optimizer_params</span><span class="p">)</span>

    <span class="n">empty_training</span> <span class="o">=</span> <span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">empty_training</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._create_training" class="doc doc-heading">
            <code class=" language-python"><span class="n">_create_training</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer_params</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._create_training" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Creates a TrainState object for the actor or the critic.
:param rng: Random key for initialization.
:param network: The actor or critic network.
:param optimizer_params: A NamedTuple containing the parametrization of the optimizer.
:return: A TrainState object to be used in training the actor and cirtic networks.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_create_training</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">network</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">flax</span><span class="o">.</span><span class="n">linen</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">optimizer_params</span><span class="p">:</span> <span class="n">OptimizerParams</span>
<span class="p">)</span><span class="o">-&gt;</span> <span class="n">TrainState</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     Creates a TrainState object for the actor or the critic.</span>
<span class="sd">    :param rng: Random key for initialization.</span>
<span class="sd">    :param network: The actor or critic network.</span>
<span class="sd">    :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.</span>
<span class="sd">    :return: A TrainState object to be used in training the actor and cirtic networks.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">network</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_network</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">network</span><span class="p">)</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_optimizer</span><span class="p">(</span><span class="n">optimizer_params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span> <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._create_update_runner" class="doc doc-heading">
            <code class=" language-python"><span class="n">_create_update_runner</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._create_update_runner" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Initializes the update runner as a Runner object. The runner contains batch_size initializations of the
environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and
one for the critic network, so that trajectory batches are used to train the same parameters.
:param rng: Random key for initialization.
:param actor_training: The actor TrainState objects used in training.
:param critic_training: The critic TrainState objects used in training.
:param hyperparams: An instance of HyperParameters for training.
:return: An update runner object to be used in trajectory sampling and training.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_create_update_runner</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">actor_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">critic_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runner</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the update runner as a Runner object. The runner contains batch_size initializations of the</span>
<span class="sd">    environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and</span>
<span class="sd">    one for the critic network, so that trajectory batches are used to train the same parameters.</span>
<span class="sd">    :param rng: Random key for initialization.</span>
<span class="sd">    :param actor_training: The actor TrainState objects used in training.</span>
<span class="sd">    :param critic_training: The critic TrainState objects used in training.</span>
<span class="sd">    :param hyperparams: An instance of HyperParameters for training.</span>
<span class="sd">    :return: An update runner object to be used in trajectory sampling and training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rng</span><span class="p">,</span> <span class="n">reset_rng</span><span class="p">,</span> <span class="n">runner_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">reset_rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">reset_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">runner_rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">runner_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">)(</span><span class="n">reset_rngs</span><span class="p">)</span>

    <span class="n">update_runner</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span>
        <span class="n">actor_training</span><span class="o">=</span><span class="n">actor_training</span><span class="p">,</span>
        <span class="n">critic_training</span><span class="o">=</span><span class="n">critic_training</span><span class="p">,</span>
        <span class="n">envstate</span><span class="o">=</span><span class="n">envstate</span><span class="p">,</span>
        <span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="p">,</span>
        <span class="n">rng</span><span class="o">=</span><span class="n">runner_rngs</span><span class="p">,</span>
        <span class="n">hyperparams</span><span class="o">=</span><span class="n">hyperparams</span><span class="p">,</span>
        <span class="n">actor_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">critic_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">update_runner</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._critic_epoch" class="doc doc-heading">
            <code class=" language-python"><span class="n">_critic_epoch</span><span class="p">(</span><span class="n">i_epoch</span><span class="p">,</span> <span class="n">epoch_runner</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._critic_epoch" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Performs a Gradient Descent update of the critic.
:param: i_epoch: The current training epoch (unused but required by lax.fori_loop).
:param epoch_runner: A tuple containing the following information about the update:
- critic_training: TrainState object for critic training
- critic_loss_input: Tuple with the inputs required by the critic loss function.
:return: The updated epoch runner.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_critic_epoch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">i_epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">epoch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a Gradient Descent update of the critic.</span>
<span class="sd">    :param: i_epoch: The current training epoch (unused but required by lax.fori_loop).</span>
<span class="sd">    :param epoch_runner: A tuple containing the following information about the update:</span>
<span class="sd">    - critic_training: TrainState object for critic training</span>
<span class="sd">    - critic_loss_input: Tuple with the inputs required by the critic loss function.</span>
<span class="sd">    :return: The updated epoch runner.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span> <span class="o">=</span> <span class="n">epoch_runner</span>
    <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span><span class="p">)</span>
    <span class="n">n_minibatch_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">minibatch_size</span>
    <span class="n">minibatch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_minibatch_updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_minibatch_fn</span><span class="p">,</span> <span class="n">minibatch_runner</span><span class="p">)</span>
    <span class="n">critic_training</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">minibatch_runner</span>

    <span class="k">return</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._critic_loss" class="doc doc-heading">
            <code class=" language-python"><span class="n">_critic_loss</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._critic_loss" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Calculates the critic loss.
:param training: The critic TrainState object.
:param obs: The obs in the trajectory batch.
:param targets: The returns over the trajectory batch, which act as the targets for training the critic.
:param hyperparams: The HyperParameters object used for training.
:return: The critic loss.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_critic_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_rollout batch_size obs_size&quot;</span><span class="p">],</span>
        <span class="n">targets</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size n_rollout&quot;</span><span class="p">],</span>
        <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the critic loss.</span>
<span class="sd">    :param training: The critic TrainState object.</span>
<span class="sd">    :param obs: The obs in the trajectory batch.</span>
<span class="sd">    :param targets: The returns over the trajectory batch, which act as the targets for training the critic.</span>
<span class="sd">    :param hyperparams: The HyperParameters object used for training.</span>
<span class="sd">    :return: The critic loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._critic_loss_input" class="doc doc-heading">
            <code class=" language-python"><span class="n">_critic_loss_input</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._critic_loss_input" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Prepares the input required by the critic loss function. The input is reshaped so that it is split into
minibatches.
:param update_runner: The Runner object used in training.
:param traj_batch: The batch of trajectories.
:return: A tuple of input to the critic loss function.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_critic_loss_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CriticLossInputType</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares the input required by the critic loss function. The input is reshaped so that it is split into</span>
<span class="sd">    minibatches.</span>
<span class="sd">    :param update_runner: The Runner object used in training.</span>
<span class="sd">    :param traj_batch: The batch of trajectories.</span>
<span class="sd">    :return: A tuple of input to the critic loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._critic_minibatch_update" class="doc doc-heading">
            <code class=" language-python"><span class="n">_critic_minibatch_update</span><span class="p">(</span><span class="n">i_minibatch</span><span class="p">,</span> <span class="n">minibatch_runner</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._critic_minibatch_update" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be
passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.
:param i_minibatch: Number of minibatch update.
:param minibatch_runner: A tuple containing the TranState object and the loss input arguments.
:param grad_fn: The gradient function of the training loss.
:return: Minibatch runner with an updated TrainState.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_critic_minibatch_update</span><span class="p">(</span>
        <span class="n">i_minibatch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">minibatch_runner</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">],</span>
        <span class="n">grad_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">CriticLossInputType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">CriticLossInputType</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be</span>
<span class="sd">    passed. This choice doesn&#39;t hurt performance. To be called using a lambda function for defining grad_fn.</span>
<span class="sd">    :param i_minibatch: Number of minibatch update.</span>
<span class="sd">    :param minibatch_runner: A tuple containing the TranState object and the loss input arguments.</span>
<span class="sd">    :param grad_fn: The gradient function of the training loss.</span>
<span class="sd">    :return: Minibatch runner with an updated TrainState.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span> <span class="o">=</span> <span class="n">minibatch_runner</span>
    <span class="o">*</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">critic_loss_input</span>
    <span class="n">traj_minibatch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i_minibatch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">traj_batch</span><span class="p">)</span>
    <span class="n">grad_input_minibatch</span> <span class="o">=</span> <span class="p">(</span><span class="n">critic_training</span><span class="p">,</span> <span class="o">*</span><span class="n">traj_minibatch</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="o">*</span><span class="n">grad_input_minibatch</span><span class="p">)</span>
    <span class="n">critic_training</span> <span class="o">=</span> <span class="n">critic_training</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._critic_update" class="doc doc-heading">
            <code class=" language-python"><span class="n">_critic_update</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._critic_update" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Prepares the input and performs Gradient Descent for the critic network.
:param update_runner: The Runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:param traj_batch: The batch of trajectories.
:return: The critic training object updated after actor_epochs steps of Gradient Ascent.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_critic_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span>  <span class="n">Tuple</span><span class="p">[</span><span class="n">TrainState</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares the input and performs Gradient Descent for the critic network.</span>
<span class="sd">    :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :param traj_batch: The batch of trajectories.</span>
<span class="sd">    :return: The critic training object updated after actor_epochs steps of Gradient Ascent.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">critic_loss_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_loss_input</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>
    <span class="n">critic_epoch_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">update_runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss_input</span><span class="p">)</span>
    <span class="n">critic_epoch_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">critic_epochs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_epoch</span><span class="p">,</span> <span class="n">critic_epoch_runner</span><span class="p">)</span>
    <span class="n">critic_training</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">critic_epoch_runner</span>

    <span class="n">critic_targets</span> <span class="o">=</span> <span class="n">critic_loss_input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rollout_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">)</span>
    <span class="n">critic_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_loss</span><span class="p">(</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span> <span class="n">critic_targets</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._entropy" class="doc doc-heading">
            <code class=" language-python"><span class="n">_entropy</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._entropy" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">)</span><span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]:</span>
    <span class="k">raise</span> <span class="bp">NotImplemented</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._eval_agent" class="doc doc-heading">
            <code class=" language-python"><span class="n">_eval_agent</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">n_episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._eval_agent" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Evaluates the agents for n_episodes complete episodes using 'lax.while_loop'.
:param rng: A random key used for evaluating the agent.
:param actor_training: The actor TrainState object (either mid- or post-training).
:param critic_training: The critic TrainState object (either mid- or post-training).
:param n_episodes: The update_runner object used during training.
:return: The sum of rewards collected over n_episodes episodes.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_eval_agent</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">actor_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">critic_training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">n_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the agents for n_episodes complete episodes using &#39;lax.while_loop&#39;.</span>
<span class="sd">    :param rng: A random key used for evaluating the agent.</span>
<span class="sd">    :param actor_training: The actor TrainState object (either mid- or post-training).</span>
<span class="sd">    :param critic_training: The critic TrainState object (either mid- or post-training).</span>
<span class="sd">    :param n_episodes: The update_runner object used during training.</span>
<span class="sd">    :return: The sum of rewards collected over n_episodes episodes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rng_eval</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_episodes</span><span class="p">)</span>
    <span class="n">rng</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">)(</span><span class="n">rng_eval</span><span class="p">)</span>

    <span class="n">eval_runner</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">envstate</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">,</span>
        <span class="n">actor_training</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">),</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actors</span><span class="p">),</span>
        <span class="n">rng</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">eval_runners</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span>
        <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">)(</span><span class="o">*</span><span class="n">eval_runner</span><span class="p">)</span>

    <span class="n">eval_runner</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">lax</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_cond</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_body</span><span class="p">,</span> <span class="n">x</span><span class="p">))(</span><span class="n">eval_runners</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">final_rewards</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">eval_runner</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_metrics</span><span class="p">(</span><span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">final_rewards</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._eval_body" class="doc doc-heading">
            <code class=" language-python"><span class="n">_eval_body</span><span class="p">(</span><span class="n">eval_runner</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._eval_body" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>A step in the episode to be used with 'lax.while_loop' for evaluation of the agent in a complete episode.
:param eval_runner: A tuple containing information about the environment state, the actor and critic training
states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards
over the episode and a random key.
:return: The updated eval_runner tuple.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_eval_body</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_runner</span><span class="p">:</span> <span class="n">EvalRunnerType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvalRunnerType</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A step in the episode to be used with &#39;lax.while_loop&#39; for evaluation of the agent in a complete episode.</span>
<span class="sd">    :param eval_runner: A tuple containing information about the environment state, the actor and critic training</span>
<span class="sd">    states, whether the episode is terminated (for checking the condition in &#39;lax.while_loop&#39;), the sum of rewards</span>
<span class="sd">    over the episode and a random key.</span>
<span class="sd">    :return: The updated eval_runner tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">envstate</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">eval_runner</span>

    <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>

    <span class="n">rng</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_step</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">envstate</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

    <span class="n">terminated</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">]</span>
    <span class="n">truncated</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">]</span>

    <span class="n">returns</span> <span class="o">+=</span> <span class="n">reward</span>

    <span class="n">eval_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_envstate</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">eval_runner</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._eval_cond" class="doc doc-heading">
            <code class=" language-python"><span class="n">_eval_cond</span><span class="p">(</span><span class="n">eval_runner</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._eval_cond" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Checks whether the episode is terminated, meaning that the 'lax.while_loop' can stop.
:param eval_runner: A tuple containing information about the environment state, the actor and critic training
states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards
over the episode and a random key.
:return: Whether the episode is terminated, which means that the while loop must stop.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_eval_cond</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_runner</span><span class="p">:</span> <span class="n">EvalRunnerType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether the episode is terminated, meaning that the &#39;lax.while_loop&#39; can stop.</span>
<span class="sd">    :param eval_runner: A tuple containing information about the environment state, the actor and critic training</span>
<span class="sd">    states, whether the episode is terminated (for checking the condition in &#39;lax.while_loop&#39;), the sum of rewards</span>
<span class="sd">    over the episode and a random key.</span>
<span class="sd">    :return: Whether the episode is terminated, which means that the while loop must stop.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">eval_runner</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">terminated</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">truncated</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._eval_metrics" class="doc doc-heading">
            <code class=" language-python"><span class="n">_eval_metrics</span><span class="p">(</span><span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">final_rewards</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._eval_metrics" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Evaluate the metrics.
:param terminated: Whether the episode finished by termination.
:param truncated: Whether the episode finished by truncation.
:param final_rewards: The rewards collected in the final step of the episode.
:param returns: The sum of rewards collected during the episode.
:return: Dictionary combining the input arguments and the case-specific special metrics.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_eval_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">terminated</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
        <span class="n">truncated</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
        <span class="n">final_rewards</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
        <span class="n">returns</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the metrics.</span>
<span class="sd">    :param terminated: Whether the episode finished by termination.</span>
<span class="sd">    :param truncated: Whether the episode finished by truncation.</span>
<span class="sd">    :param final_rewards: The rewards collected in the final step of the episode.</span>
<span class="sd">    :param returns: The sum of rewards collected during the episode.</span>
<span class="sd">    :return: Dictionary combining the input arguments and the case-specific special metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;terminated&quot;</span><span class="p">:</span> <span class="n">terminated</span><span class="p">,</span>
        <span class="s2">&quot;truncated&quot;</span><span class="p">:</span> <span class="n">truncated</span><span class="p">,</span>
        <span class="s2">&quot;final_rewards&quot;</span><span class="p">:</span> <span class="n">final_rewards</span><span class="p">,</span>
        <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="n">returns</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._generate_metrics" class="doc doc-heading">
            <code class=" language-python"><span class="n">_generate_metrics</span><span class="p">(</span><span class="n">runner</span><span class="p">,</span> <span class="n">update_step</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._generate_metrics" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Generates metrics for on-policy learning. The agent performance during training is evaluated by running
n_evals episodes (until termination). If the user selects not to generate metrics (leading to faster training),
an empty dictionary is returned.
:param runner: The update runner object, containing information about the current status of the actor's/critic's
training, the state of the environment and training hyperparameters.
:param update_step: The number of the update step.
:return: A dictionary of the sum of rewards collected over 'n_evals' episodes, or empty dictionary.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_generate_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">update_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates metrics for on-policy learning. The agent performance during training is evaluated by running</span>
<span class="sd">    n_evals episodes (until termination). If the user selects not to generate metrics (leading to faster training),</span>
<span class="sd">    an empty dictionary is returned.</span>
<span class="sd">    :param runner: The update runner object, containing information about the current status of the actor&#39;s/critic&#39;s</span>
<span class="sd">    training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :param update_step: The number of the update step.</span>
<span class="sd">    :return: A dictionary of the sum of rewards collected over &#39;n_evals&#39; episodes, or empty dictionary.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">metric</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_agent</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_rng</span><span class="p">,</span>
            <span class="n">runner</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span>
            <span class="n">runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_evals</span>
        <span class="p">)</span>

    <span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s2">&quot;actor_loss&quot;</span><span class="p">:</span> <span class="n">runner</span><span class="o">.</span><span class="n">actor_loss</span><span class="p">,</span>
        <span class="s2">&quot;critic_loss&quot;</span><span class="p">:</span> <span class="n">runner</span><span class="o">.</span><span class="n">critic_loss</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">metric</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._init_checkpointer" class="doc doc-heading">
            <code class=" language-python"><span class="n">_init_checkpointer</span><span class="p">()</span></code>

<a href="#jaxagents.ippo.IPPOBase._init_checkpointer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If
so, sets the checkpoint manager using orbax.
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_init_checkpointer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If</span>
<span class="sd">    so, sets the checkpoint manager using orbax.</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">restore_agent</span><span class="p">:</span>

            <span class="n">dir_exists</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">dir_exists</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">)</span>

            <span class="n">dir_files</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">file</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dir_files</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">dir_files</span><span class="p">:</span>
                    <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
                    <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

            <span class="c1"># Log training configuration</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s1">&#39;training_configuration.txt&#39;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">())</span>

        <span class="n">orbax_checkpointer</span> <span class="o">=</span> <span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">Checkpointer</span><span class="p">(</span><span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">PyTreeCheckpointHandler</span><span class="p">())</span>

        <span class="n">options</span> <span class="o">=</span> <span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">CheckpointManagerOptions</span><span class="p">(</span>
            <span class="n">create</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">step_prefix</span><span class="o">=</span><span class="s1">&#39;trainingstep&#39;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">orbax</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span>
            <span class="n">orbax_checkpointer</span><span class="p">,</span>
            <span class="n">options</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._init_env" class="doc doc-heading">
            <code class=" language-python"><span class="n">_init_env</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_params</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._init_env" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Environment initialization.
:param env: A gymnax or custom environment that inherits from the basic gymnax class.
:param env_params: A dataclass containing the parametrization of the environment.
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_init_env</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">Environment</span><span class="p">,</span> <span class="n">env_params</span><span class="p">:</span> <span class="n">EnvParams</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Environment initialization.</span>
<span class="sd">    :param env: A gymnax or custom environment that inherits from the basic gymnax class.</span>
<span class="sd">    :param env_params: A dataclass containing the parametrization of the environment.</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">TruncationWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_episode_steps</span><span class="p">)</span>
    <span class="c1"># env = FlattenObservationWrapper(env)</span>
    <span class="c1"># self.env = LogWrapper(env)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">env_params</span> <span class="o">=</span> <span class="n">env_params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._init_network" class="doc doc-heading">
            <code class=" language-python"><span class="n">_init_network</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">network</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._init_network" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Initialization of the actor or critic network.
:param rng: Random key for initialization.
:param network: The actor or critic network.
:return: A random key after splitting the input and the initial parameters of the policy network.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_init_network</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">network</span><span class="p">:</span> <span class="n">flax</span><span class="o">.</span><span class="n">linen</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">flax</span><span class="o">.</span><span class="n">linen</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">FrozenDict</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialization of the actor or critic network.</span>
<span class="sd">    :param rng: Random key for initialization.</span>
<span class="sd">    :param network: The actor or critic network.</span>
<span class="sd">    :return: A random key after splitting the input and the initial parameters of the policy network.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">network</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

    <span class="n">rng</span><span class="p">,</span> <span class="o">*</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">dummy_reset_rng</span><span class="p">,</span> <span class="n">network_init_rng</span> <span class="o">=</span> <span class="n">_rng</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">dummy_obs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">(</span><span class="n">dummy_reset_rng</span><span class="p">)</span>
    <span class="n">init_x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">dummy_obs</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">network_init_rng</span><span class="p">,</span> <span class="n">init_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">network</span><span class="p">,</span> <span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._init_optimizer" class="doc doc-heading">
            <code class=" language-python"><span class="n">_init_optimizer</span><span class="p">(</span><span class="n">optimizer_params</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._init_optimizer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to
initialize the appropriate optimizer. In this way, the optimizer can be initialized within the "train" method,
and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary.
:param optimizer_params: A NamedTuple containing the parametrization of the optimizer.
:return: An optimizer in optax.chain.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_init_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_params</span><span class="p">:</span> <span class="n">OptimizerParams</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">optax</span><span class="o">.</span><span class="n">chain</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to</span>
<span class="sd">    initialize the appropriate optimizer. In this way, the optimizer can be initialized within the &quot;train&quot; method,</span>
<span class="sd">    and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary.</span>
<span class="sd">    :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.</span>
<span class="sd">    :return: An optimizer in optax.chain.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">optimizer_params_dict</span> <span class="o">=</span> <span class="n">optimizer_params</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span>  <span class="c1"># Transform from NamedTuple to dict</span>
    <span class="n">optimizer_params_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;grad_clip&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Remove &#39;grad_clip&#39;, since it is not part of the optimizer args.</span>


<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get dictionary of optimizer parameters to pass in optimizer. The procedure preserves parameters that:</span>
<span class="sd">        - are given in the OptimizerParams NamedTuple and are requested as args by the optimizer</span>
<span class="sd">        - are requested as args by the optimizer and are given in the OptimizerParams NamedTuple</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">optimizer_arg_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="vm">__code__</span><span class="o">.</span><span class="n">co_varnames</span>  <span class="c1"># List names of args of optimizer.</span>

    <span class="c1"># Keep only the optimizer arg names that are also part of the OptimizerParams (dict from NamedTuple)</span>
    <span class="n">optimizer_arg_names</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">arg_name</span> <span class="k">for</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="n">optimizer_arg_names</span> <span class="k">if</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">optimizer_params_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer_arg_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;The defined optimizer parameters do not include relevant arguments for this optimizer.&quot;</span>
            <span class="s2">&quot;The optimizer has not been implemented yet. Define your own OptimizerParams object.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Keep only the optimizer params that are arg names for the specific optimizer</span>
    <span class="n">optimizer_params_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg_name</span><span class="p">:</span> <span class="n">optimizer_params_dict</span><span class="p">[</span><span class="n">arg_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="n">optimizer_arg_names</span><span class="p">}</span>

    <span class="c1"># No need to scale by -1.0. &#39;TrainState.apply_gradients&#39; is used for training, which subtracts the update.</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
        <span class="n">optax</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">optimizer_params</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="o">**</span><span class="n">optimizer_params_dict</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">tx</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._log_prob" class="doc doc-heading">
            <code class=" language-python"><span class="n">_log_prob</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._log_prob" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">FrozenDict</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">ActionType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">]:</span>
    <span class="k">raise</span> <span class="bp">NotImplemented</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._make_rollout_runners" class="doc doc-heading">
            <code class=" language-python"><span class="n">_make_rollout_runners</span><span class="p">(</span><span class="n">update_runner</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._make_rollout_runners" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner
object and broadcasting the TrainState object for the critic and the network in the update_runner object to the
same dimension.
:param update_runner: The Runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:return: Tuple with step runners to be used in rollout.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_make_rollout_runners</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">StepRunnerType</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner</span>
<span class="sd">    object and broadcasting the TrainState object for the critic and the network in the update_runner object to the</span>
<span class="sd">    same dimension.</span>
<span class="sd">    :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :return: Tuple with step runners to be used in rollout.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rollout_runner</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">envstate</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="o">.</span><span class="n">rng</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">rollout_runners</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">)(</span><span class="o">*</span><span class="n">rollout_runner</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rollout_runners</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._make_transition" class="doc doc-heading">
            <code class=" language-python"><span class="n">_make_transition</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">terminated</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._make_transition" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Creates a transition object based on the input and output of an episode step.
:param obs: The current state of the episode step in array format.
:param actions: The action selected per actor.
:param value: The value of the state per critic.
:param log_prob: The actor log-probability of the selected action.
:param reward: The collected reward after executing the action per actor.
:param next_obs: The next state of the episode step in array format.
:param terminated: Episode termination.
:return: A transition object storing information about the state before and after executing the episode step,
         the executed action, the collected reward, episode termination and optional additional information.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_make_transition</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">ActionType</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">],</span>
        <span class="n">log_prob</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">],</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_actors&quot;</span><span class="p">],</span>
        <span class="n">next_obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">,</span>
        <span class="n">terminated</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a transition object based on the input and output of an episode step.</span>
<span class="sd">    :param obs: The current state of the episode step in array format.</span>
<span class="sd">    :param actions: The action selected per actor.</span>
<span class="sd">    :param value: The value of the state per critic.</span>
<span class="sd">    :param log_prob: The actor log-probability of the selected action.</span>
<span class="sd">    :param reward: The collected reward after executing the action per actor.</span>
<span class="sd">    :param next_obs: The next state of the episode step in array format.</span>
<span class="sd">    :param terminated: Episode termination.</span>
<span class="sd">    :return: A transition object storing information about the state before and after executing the episode step,</span>
<span class="sd">             the executed action, the collected reward, episode termination and optional additional information.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">transition</span> <span class="o">=</span> <span class="n">Transition</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">actions</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">terminated</span><span class="p">)</span>
    <span class="n">transition</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">transition</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">transition</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._pp" class="doc doc-heading">
            <code class=" language-python"><span class="n">_pp</span><span class="p">()</span></code>

<a href="#jaxagents.ippo.IPPOBase._pp" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Post-processes the training results, which includes:
    - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored).
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_pp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Post-processes the training results, which includes:</span>
<span class="sd">        - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored).</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">actor_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_runner</span><span class="o">.</span><span class="n">actor_training</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">critic_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_runner</span><span class="o">.</span><span class="n">critic_training</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._process_trajectory" class="doc doc-heading">
            <code class=" language-python"><span class="n">_process_trajectory</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._process_trajectory" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not
guaranteed to end with termination, the value is estimated using the critic network. This assumption has been
shown to have no influence by the end of training.
:param update_runner: The Runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:param traj_batch: The batch of trajectories, as collected by in rollout.
:param last_state: The state at the end of every trajectory in the batch.
:return: A batch of trajectories that includes an estimate of values and advantages.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_process_trajectory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transition</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not</span>
<span class="sd">    guaranteed to end with termination, the value is estimated using the critic network. This assumption has been</span>
<span class="sd">    shown to have no influence by the end of training.</span>
<span class="sd">    :param update_runner: The Runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :param traj_batch: The batch of trajectories, as collected by in rollout.</span>
<span class="sd">    :param last_state: The state at the end of every trajectory in the batch.</span>
<span class="sd">    :return: A batch of trajectories that includes an estimate of values and advantages.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">traj_batch</span><span class="p">)</span>
    <span class="n">traj_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_next_values</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">critic_training</span><span class="p">)</span>

    <span class="n">advantages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_advantages</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">hyperparams</span><span class="o">.</span><span class="n">gae_lambda</span><span class="p">)</span>
    <span class="n">traj_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_advantages</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">advantages</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">traj_batch</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._returns" class="doc doc-heading">
            <code class=" language-python"><span class="n">_returns</span><span class="p">(</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">last_next_state_value</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._returns" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the
trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with
episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling
step, trajectories do not start at the initial state.
:param traj_batch: The batch of trajectories.
:param last_next_state_value: The value of the last next state in each trajectory.
:param gamma: Discount factor
:param gae_lambda: The GAE  factor.
:return: The returns over the episodes of the trajectory batch.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_returns</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">traj_batch</span><span class="p">:</span> <span class="n">Transition</span><span class="p">,</span>
        <span class="n">last_next_state_value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">],</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">gae_lambda</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ReturnsType</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the</span>
<span class="sd">    trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with</span>
<span class="sd">    episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling</span>
<span class="sd">    step, trajectories do not start at the initial state.</span>
<span class="sd">    :param traj_batch: The batch of trajectories.</span>
<span class="sd">    :param last_next_state_value: The value of the last next state in each trajectory.</span>
<span class="sd">    :param gamma: Discount factor</span>
<span class="sd">    :param gae_lambda: The GAE  factor.</span>
<span class="sd">    :return: The returns over the episodes of the trajectory batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rewards_t</span> <span class="o">=</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">terminated_t</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">traj_batch</span><span class="o">.</span><span class="n">terminated</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">discounts_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">terminated_t</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;Remove first entry so that the next state values per step are in sync with the state rewards.&quot;&quot;&quot;</span>
    <span class="n">next_state_values_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span><span class="n">traj_batch</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">last_next_state_value</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]],</span>
        <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">:]</span>

    <span class="n">rewards_t</span><span class="p">,</span> <span class="n">discounts_t</span><span class="p">,</span> <span class="n">next_state_values_t</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">discounts_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">gae_lambda</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">discounts_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">gae_lambda</span>

    <span class="n">traj_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards_t</span><span class="p">,</span> <span class="n">discounts_t</span><span class="p">,</span> <span class="n">next_state_values_t</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">)</span>
    <span class="n">end_value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">next_state_values_t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Start from end of trajectory and work in reverse.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">returns</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trajectory_returns</span><span class="p">,</span> <span class="n">end_value</span><span class="p">,</span> <span class="n">traj_runner</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">returns</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">returns</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">returns</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._rollout" class="doc doc-heading">
            <code class=" language-python"><span class="n">_rollout</span><span class="p">(</span><span class="n">step_runner</span><span class="p">,</span> <span class="n">i_step</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._rollout" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Evaluation of trajectory rollout. In each step the agent:
- evaluates policy and value
- selects action
- performs environment step
- creates step transition
:param step_runner: A tuple containing information on the environment state, the actor and critic training
(parameters and networks) and a random key.
:param i_step: Unused, required for lax.scan.
:return: The updated step_runner tuple and the rollout step transition.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_rollout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_runner</span><span class="p">:</span> <span class="n">StepRunnerType</span><span class="p">,</span> <span class="n">i_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">StepRunnerType</span><span class="p">,</span> <span class="n">Transition</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluation of trajectory rollout. In each step the agent:</span>
<span class="sd">    - evaluates policy and value</span>
<span class="sd">    - selects action</span>
<span class="sd">    - performs environment step</span>
<span class="sd">    - creates step transition</span>
<span class="sd">    :param step_runner: A tuple containing information on the environment state, the actor and critic training</span>
<span class="sd">    (parameters and networks) and a random key.</span>
<span class="sd">    :param i_step: Unused, required for lax.scan.</span>
<span class="sd">    :return: The updated step_runner tuple and the rollout step transition.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">envstate</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">step_runner</span>

    <span class="n">rng</span><span class="p">,</span> <span class="n">rng_action</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_actions</span><span class="p">(</span><span class="n">rng_action</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">critic_training</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">critic_training</span><span class="o">.</span><span class="n">params</span><span class="p">),</span> <span class="n">obs</span><span class="p">)</span>

    <span class="n">log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob</span><span class="p">(</span><span class="n">actor_training</span><span class="p">,</span> <span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">actor_training</span><span class="o">.</span><span class="n">params</span><span class="p">),</span> <span class="n">obs</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

    <span class="n">rng</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_step</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">envstate</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

    <span class="n">step_runner</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_envstate</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

    <span class="n">terminated</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">]</span>

    <span class="n">transition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_transition</span><span class="p">(</span>
        <span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="p">,</span>
        <span class="n">actions</span><span class="o">=</span><span class="n">actions</span><span class="p">,</span>
        <span class="n">value</span><span class="o">=</span><span class="n">values</span><span class="p">,</span>
        <span class="n">log_prob</span><span class="o">=</span><span class="n">log_prob</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
        <span class="n">next_obs</span><span class="o">=</span><span class="n">next_obs</span><span class="p">,</span>
        <span class="n">terminated</span><span class="o">=</span><span class="n">terminated</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">step_runner</span><span class="p">,</span> <span class="n">transition</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._sample_actions" class="doc doc-heading">
            <code class=" language-python"><span class="n">_sample_actions</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._sample_actions" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_sample_actions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActionType</span><span class="p">:</span>
    <span class="k">raise</span> <span class="bp">NotImplemented</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._training_step" class="doc doc-heading">
            <code class=" language-python"><span class="n">_training_step</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">i_training_batch</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._training_step" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Performs trainings steps to update the agent per training batch.
:param update_runner: The runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:param i_training_batch: Training batch loop counter.
:return: Tuple with updated runner and dictionary of metrics.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_training_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">,</span>
        <span class="n">i_training_batch</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Runner</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs trainings steps to update the agent per training batch.</span>
<span class="sd">    :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :param i_training_batch: Training batch loop counter.</span>
<span class="sd">    :return: Tuple with updated runner and dictionary of metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_training_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span> <span class="o">*</span> <span class="n">i_training_batch</span>
    <span class="n">n_training_steps</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">n_training_steps</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span><span class="p">)</span>

    <span class="n">update_runner</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_training_steps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_step</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_metrics</span><span class="p">(</span><span class="n">runner</span><span class="o">=</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">update_step</span><span class="o">=</span><span class="n">i_training_batch</span><span class="p">)</span>
        <span class="n">i_training_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span> <span class="o">*</span> <span class="p">(</span><span class="n">i_training_batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">i_training_step</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">i_training_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">i_training_step</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">return</span> <span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._trajectory_advantages" class="doc doc-heading">
            <code class=" language-python"><span class="n">_trajectory_advantages</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">traj</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._trajectory_advantages" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Calculates the advantages per episode step over a batch of trajectories.
:param value: The values of the steps in the trajectory according to the critic (including the one of the last
 state).
:param traj: The trajectory batch.
:return: An array of returns.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_trajectory_advantages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">traj</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the advantages per episode step over a batch of trajectories.</span>
<span class="sd">    :param value: The values of the steps in the trajectory according to the critic (including the one of the last</span>
<span class="sd">     state).</span>
<span class="sd">    :param traj: The trajectory batch.</span>
<span class="sd">    :return: An array of returns.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._trajectory_returns" class="doc doc-heading">
            <code class=" language-python"><span class="n">_trajectory_returns</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">traj</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase._trajectory_returns" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Calculates the returns per episode step over a batch of trajectories.
:param value: The values of the steps in the trajectory according to the critic (including the one of the last
 state).
:param traj: The trajectory batch.
:return: A tuple of returns.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_trajectory_returns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">traj</span><span class="p">:</span> <span class="n">Transition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the returns per episode step over a batch of trajectories.</span>
<span class="sd">    :param value: The values of the steps in the trajectory according to the critic (including the one of the last</span>
<span class="sd">     state).</span>
<span class="sd">    :param traj: The trajectory batch.</span>
<span class="sd">    :return: A tuple of returns.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase._update_step" class="doc doc-heading">
            <code class=" language-python"><span class="n">_update_step</span><span class="p">(</span><span class="n">i_update_step</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase._update_step" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>An update step of the actor and critic networks. This entails:
- performing rollout for sampling a batch of trajectories.
- assessing the value of the last state per trajectory using the critic.
- evaluating the advantage per trajectory.
- updating the actor and critic network parameters via the respective loss functions.
- generating in-training performance metrics.
In this approach, the update_runner already has a batch of environments initialized. The environments are not
initialized in the beginning of every update step, which means that trajectories to not necessarily start from
an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan
for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be
truncated in trajectory sampling).
:param i_update_step: Unused, required for progressbar.
:param update_runner: The runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters.
:return: The updated runner</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_update_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_update_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">update_runner</span><span class="p">:</span> <span class="n">Runner</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runner</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An update step of the actor and critic networks. This entails:</span>
<span class="sd">    - performing rollout for sampling a batch of trajectories.</span>
<span class="sd">    - assessing the value of the last state per trajectory using the critic.</span>
<span class="sd">    - evaluating the advantage per trajectory.</span>
<span class="sd">    - updating the actor and critic network parameters via the respective loss functions.</span>
<span class="sd">    - generating in-training performance metrics.</span>
<span class="sd">    In this approach, the update_runner already has a batch of environments initialized. The environments are not</span>
<span class="sd">    initialized in the beginning of every update step, which means that trajectories to not necessarily start from</span>
<span class="sd">    an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan</span>
<span class="sd">    for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be</span>
<span class="sd">    truncated in trajectory sampling).</span>
<span class="sd">    :param i_update_step: Unused, required for progressbar.</span>
<span class="sd">    :param update_runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters.</span>
<span class="sd">    :return: The updated runner</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rollout_runners</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_rollout_runners</span><span class="p">(</span><span class="n">update_runner</span><span class="p">)</span>
    <span class="n">scan_rollout_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_rollout</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rollout_length</span><span class="p">)</span>
    <span class="n">rollout_runners</span><span class="p">,</span> <span class="n">traj_batch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">scan_rollout_fn</span><span class="p">)(</span><span class="n">rollout_runners</span><span class="p">)</span>
    <span class="n">last_envstate</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">rollout_runners</span>
    <span class="n">traj_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_trajectory</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">)</span>

    <span class="n">actor_training</span><span class="p">,</span> <span class="n">actor_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_update</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>
    <span class="n">critic_training</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_update</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">traj_batch</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update runner as a dataclass.&quot;&quot;&quot;</span>
    <span class="n">update_runner</span> <span class="o">=</span> <span class="n">update_runner</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
        <span class="n">envstate</span><span class="o">=</span><span class="n">last_envstate</span><span class="p">,</span>
        <span class="n">obs</span><span class="o">=</span><span class="n">last_obs</span><span class="p">,</span>
        <span class="n">actor_training</span><span class="o">=</span><span class="n">actor_training</span><span class="p">,</span>
        <span class="n">critic_training</span><span class="o">=</span><span class="n">critic_training</span><span class="p">,</span>
        <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
        <span class="n">actor_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">critic_loss</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">update_runner</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.collect_training" class="doc doc-heading">
            <code class=" language-python"><span class="n">collect_training</span><span class="p">(</span><span class="n">runner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">previous_training_max_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.collect_training" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Collects training or restored checkpoint of output (the final state of the runner after training and the
collected metrics).
:param runner: The runner object, containing information about the current status of the actor's/
critic's training, the state of the environment and training hyperparameters. This is at the state reached at
the end of training.
:param metrics: Dictionary of evaluation metrics (return per environment evaluation)
:param previous_training_max_step: Maximum step reached during training.
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">collect_training</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">runner</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Runner</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">previous_training_max_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collects training or restored checkpoint of output (the final state of the runner after training and the</span>
<span class="sd">    collected metrics).</span>
<span class="sd">    :param runner: The runner object, containing information about the current status of the actor&#39;s/</span>
<span class="sd">    critic&#39;s training, the state of the environment and training hyperparameters. This is at the state reached at</span>
<span class="sd">    the end of training.</span>
<span class="sd">    :param metrics: Dictionary of evaluation metrics (return per environment evaluation)</span>
<span class="sd">    :param previous_training_max_step: Maximum step reached during training.</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">agent_trained</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">previous_training_max_step</span> <span class="o">=</span> <span class="n">previous_training_max_step</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_runner</span> <span class="o">=</span> <span class="n">runner</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_metrics</span> <span class="o">=</span> <span class="n">metrics</span>
    <span class="n">n_evals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eval_steps_in_training</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_evals</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pp</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.env_reset" class="doc doc-heading">
            <code class=" language-python"><span class="n">env_reset</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.env_reset" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Environment reset.
:param rng: Random key for initialization.
:return: A random key after splitting the input, the reset environment in array and LogEnvState formats.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">env_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">PRNGKeyArray</span><span class="p">,</span> <span class="n">ObsType</span><span class="p">,</span> <span class="n">LogEnvState</span> <span class="o">|</span> <span class="n">EnvState</span> <span class="o">|</span> <span class="n">TruncationEnvState</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Environment reset.</span>
<span class="sd">    :param rng: Random key for initialization.</span>
<span class="sd">    :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rng</span><span class="p">,</span> <span class="n">reset_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">reset_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_params</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rng</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.env_step" class="doc doc-heading">
            <code class=" language-python"><span class="n">env_step</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">envstate</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.env_step" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Environment step.
:param rng: Random key for initialization.
:param envstate: The environment state in LogEnvState format.
:param actions: The actions selected per actor.
:return: A tuple of: a random key after splitting the input, the next state in array and LogEnvState formats,
         the collected reward after executing the action, episode termination and a dictionary of optional
         additional information.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">env_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">envstate</span><span class="p">:</span> <span class="n">LogEnvState</span> <span class="o">|</span> <span class="n">EnvState</span> <span class="o">|</span> <span class="n">TruncationEnvState</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">ActionType</span>
<span class="p">)</span><span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
    <span class="n">PRNGKeyArray</span><span class="p">,</span>
    <span class="n">ObsType</span><span class="p">,</span>
    <span class="n">LogEnvState</span> <span class="o">|</span> <span class="n">EnvState</span> <span class="o">|</span> <span class="n">TruncationEnvState</span><span class="p">,</span>
    <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
    <span class="n">Bool</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
    <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">bool</span><span class="p">]</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Environment step.</span>
<span class="sd">    :param rng: Random key for initialization.</span>
<span class="sd">    :param envstate: The environment state in LogEnvState format.</span>
<span class="sd">    :param actions: The actions selected per actor.</span>
<span class="sd">    :return: A tuple of: a random key after splitting the input, the next state in array and LogEnvState formats,</span>
<span class="sd">             the collected reward after executing the action, episode termination and a dictionary of optional</span>
<span class="sd">             additional information.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rng</span><span class="p">,</span> <span class="n">step_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> \
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">step_rng</span><span class="p">,</span> <span class="n">envstate</span><span class="p">,</span> <span class="n">actions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_params</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rng</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">next_envstate</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.eval" class="doc doc-heading">
            <code class=" language-python"><span class="nb">eval</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_evals</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.eval" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Evaluates the trained agent's performance post-training using the trained agent's actor and critic.
:param rng: Random key for evaluation.
:param n_evals: Number of steps in agent evaluation.
:return: Dictionary of evaluation metrics.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span> <span class="n">n_evals</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_evals&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the trained agent&#39;s performance post-training using the trained agent&#39;s actor and critic.</span>
<span class="sd">    :param rng: Random key for evaluation.</span>
<span class="sd">    :param n_evals: Number of steps in agent evaluation.</span>
<span class="sd">    :return: Dictionary of evaluation metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">eval_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_agent</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_training</span><span class="p">,</span> <span class="n">n_evals</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">eval_metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.log_hyperparams" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_hyperparams</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.log_hyperparams" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Logs training hyperparameters in a text file. To be used outside training.
:param hyperparams: An instance of HyperParameters for training.
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">log_hyperparams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logs training hyperparameters in a text file. To be used outside training.</span>
<span class="sd">    :param hyperparams: An instance of HyperParameters for training.</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">field</span> <span class="o">+</span> <span class="s1">&#39;: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">,</span> <span class="n">field</span><span class="p">))</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">_fields</span><span class="p">]</span>
    <span class="n">output_lst</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Hyperparameters:&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_lst</span>
    <span class="n">output_lst</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_lst</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s1">&#39;hyperparameters.txt&#39;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output_lst</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.policy" class="doc doc-heading">
            <code class=" language-python"><span class="n">policy</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#jaxagents.ippo.IPPOBase.policy" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state.
:param obs: The current obs of the episode step in array format.
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">ObsType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActionType</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state.</span>
<span class="sd">    :param obs: The current obs of the episode step in array format.</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="bp">NotImplemented</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.restore" class="doc doc-heading">
            <code class=" language-python"><span class="n">restore</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">best_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.restore" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then,
post-processes the restored checkpoint.
:param mode: Determines whether the best performing or latest checkpoint should be restored.
:param best_fn: The function that should be used in determining the best performing checkpoint.
:return:</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">restore</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;best&quot;</span><span class="p">,</span>
        <span class="n">best_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">]]],</span> <span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then,</span>
<span class="sd">    post-processes the restored checkpoint.</span>
<span class="sd">    :param mode: Determines whether the best performing or latest checkpoint should be restored.</span>
<span class="sd">    :param best_fn: The function that should be used in determining the best performing checkpoint.</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">all_steps</span><span class="p">()</span>

    <span class="c1"># Log keys in checkpoints</span>
    <span class="n">ckpt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ckpt_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">ckpt</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;runner&quot;</span><span class="p">]</span>

    <span class="c1"># Collect history of metrics in training. Useful for continuing training.</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ckpt_keys</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">ckpt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ckpt_keys</span><span class="p">:</span>
            <span class="n">metrics</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">jnp</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;best&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">best_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">=</span> <span class="n">steps</span><span class="p">[</span><span class="n">best_fn</span><span class="p">(</span><span class="n">metrics</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Function for determining best checkpoint not provided&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;last&quot;</span><span class="p">:</span>
        <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">latest_step</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Unknown method for selecting a checkpoint.&quot;</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create an empty target for restoring the checkpoint.</span>
<span class="sd">    Some of the arguments come from restoring one of the ckpts.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">empty_actor_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_empty_trainstate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">actor_network</span><span class="p">)</span>
    <span class="n">empty_critic_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_empty_trainstate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">critic_network</span><span class="p">)</span>

    <span class="c1"># Get some state and envstate for restoring the checkpoint.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">envstate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_reset</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">empty_runner</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span>
        <span class="n">actor_training</span><span class="o">=</span><span class="n">empty_actor_training</span><span class="p">,</span>
        <span class="n">critic_training</span><span class="o">=</span><span class="n">empty_critic_training</span><span class="p">,</span>
        <span class="n">envstate</span><span class="o">=</span><span class="n">envstate</span><span class="p">,</span>
        <span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="p">,</span>
        <span class="n">rng</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">),</span>  <span class="c1"># Just a dummy PRNGKey for initializing the networks parameters.</span>
        <span class="c1"># Hyperparams can be loaded as a dict. If training continues, new hyperparams will be provided.</span>
        <span class="n">hyperparams</span><span class="o">=</span><span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;runner&quot;</span><span class="p">][</span><span class="s2">&quot;hyperparams&quot;</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">target_ckpt</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;runner&quot;</span><span class="p">:</span> <span class="n">empty_runner</span><span class="p">,</span>
        <span class="s2">&quot;terminated&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;terminated&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
        <span class="s2">&quot;truncated&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
        <span class="s2">&quot;final_rewards&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;final_rewards&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
        <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;returns&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
    <span class="p">}</span>

    <span class="n">ckpt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">items</span><span class="o">=</span><span class="n">target_ckpt</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">collect_training</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;runner&quot;</span><span class="p">],</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">previous_training_max_step</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">steps</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.summarize" class="doc doc-heading">
            <code class=" language-python"><span class="n">summarize</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.summarize" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Summarizes collection of per-episode metrics.
:param metrics: Metric per episode.
:return: Summary of metric per episode.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">summarize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="s2">&quot;size_metrics&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;size_metrics&quot;</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MetricStats</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summarizes collection of per-episode metrics.</span>
<span class="sd">    :param metrics: Metric per episode.</span>
<span class="sd">    :return: Summary of metric per episode.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">MetricStats</span><span class="p">(</span>
        <span class="n">episode_metric</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">mean</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">var</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">std</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="nb">min</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="nb">max</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">median</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">has_nans</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">metrics</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="jaxagents.ippo.IPPOBase.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span></code>

<a href="#jaxagents.ippo.IPPOBase.train" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Trains the agents. A jax_tqdm progressbar has been added in the lax.scan loop.
:param rng: Random key for initialization. This is the original key for training.
:param hyperparams: An instance of HyperParameters for training.
:return: The final state of the step runner after training and the training metrics accumulated over all
         training batches and steps.</p>


            <details class="quote">
              <summary>Source code in <code>jaxagents\ippo.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">PRNGKeyArray</span><span class="p">,</span>
        <span class="n">hyperparams</span><span class="p">:</span> <span class="n">HyperParameters</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Runner</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;n_agents&quot;</span><span class="p">]]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains the agents. A jax_tqdm progressbar has been added in the lax.scan loop.</span>
<span class="sd">    :param rng: Random key for initialization. This is the original key for training.</span>
<span class="sd">    :param hyperparams: An instance of HyperParameters for training.</span>
<span class="sd">    :return: The final state of the step runner after training and the training metrics accumulated over all</span>
<span class="sd">             training batches and steps.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">rng</span><span class="p">,</span> <span class="o">*</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">actor_init_rng</span><span class="p">,</span> <span class="n">critic_init_rng</span><span class="p">,</span> <span class="n">runner_rng</span> <span class="o">=</span> <span class="n">_rng</span>

    <span class="n">actor_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_training</span><span class="p">(</span>
        <span class="n">actor_init_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">actor_network</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">actor_optimizer_params</span>
    <span class="p">)</span>
    <span class="n">critic_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_training</span><span class="p">(</span>
        <span class="n">critic_init_rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">critic_network</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">critic_optimizer_params</span>
    <span class="p">)</span>

    <span class="n">update_runner</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_update_runner</span><span class="p">(</span><span class="n">runner_rng</span><span class="p">,</span> <span class="n">actor_training</span><span class="p">,</span> <span class="n">critic_training</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>

    <span class="c1"># Checkpoint initial state</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
        <span class="n">metrics_start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_metrics</span><span class="p">(</span><span class="n">runner</span><span class="o">=</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">update_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint</span><span class="p">(</span><span class="n">update_runner</span><span class="p">,</span> <span class="n">metrics_start</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">previous_training_max_step</span><span class="p">)</span>

    <span class="c1"># Initialize agent updating functions, which can be avoided to be done within the training loops.</span>
    <span class="n">actor_grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor_loss</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_int</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_actor_minibatch_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_minibatch_update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">actor_grad_fn</span><span class="p">)</span>

    <span class="n">critic_grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_critic_loss</span><span class="p">,</span> <span class="n">allow_int</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_critic_minibatch_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_critic_minibatch_update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">critic_grad_fn</span><span class="p">)</span>

    <span class="c1"># Train, evaluate, checkpoint</span>
    <span class="n">n_training_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span>
    <span class="n">progressbar_desc</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Training batch (training steps = batch x </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eval_frequency</span><span class="si">}</span><span class="s1">)&#39;</span>

    <span class="n">runner</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
        <span class="n">scan_tqdm</span><span class="p">(</span><span class="n">n_training_batches</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="n">progressbar_desc</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">_training_step</span><span class="p">),</span>
        <span class="n">update_runner</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_training_batches</span><span class="p">),</span>
        <span class="n">n_training_batches</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_during_training</span><span class="p">:</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">metrics_start</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">jnp</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span> <span class="n">metrics</span><span class="p">[</span><span class="n">key</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">metrics</span><span class="o">=</span> <span class="p">{}</span>

    <span class="k">return</span> <span class="n">runner</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/amavrits/jax-agents" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tabs", "search.highlight", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>