{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JAXAgents","text":"<p>JAXAgents is a high-performance (Multi-Agent) Reinforcement Learning library built on JAX, designed for rapid experimentation, scalable training of RL agents and fast hyperparameter tuning. It supports a variety of algorithms and environments, making it suitable for both research and practical applications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>RL: Implementations of popular RL algorithms, including:</li> <li>Q-learning:<ul> <li>Deep Q Networks (DQN)</li> <li>Double Deep Q Networks (DDQN)</li> <li>Categorical DQN (C51)</li> <li>Quantile Regression DQN (QRDQN)</li> </ul> </li> <li>Policy Gradient:<ul> <li>REINFORCE</li> <li>Proximal Policy Optimization (PPO) with Generalized Advantage Estimation (GAE)</li> </ul> </li> <li> <p>Multi-Agent RL:</p> <ul> <li>Independent PPO (IPPO)</li> </ul> </li> <li> <p>High Performance: Leveraging JAX's capabilities for just-in-time compilation and automatic differentiation, enabling efficient computation on CPUs and GPUs.</p> </li> <li> <p>Modular Design: Structured for easy extension and customization, facilitating experimentation with new algorithms and environments.</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Ensure you have Python 3.10 or higher installed. Then, install JAX Agents via pip:</p> <pre><code>pip install jaxagents\n</code></pre> <p>Note: Also available on PyPI</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Here's a simple example to train a PPO agent:</p> <pre><code>import jaxagents\n\n# Initialize environment and agent\nenv = jaxagents.environments.make('CartPole-v1')\nagent = jaxagents.agents.PPO(env)\n\n# Train the agent\nagent.train(num_episodes=1000)\n</code></pre> <p>For more detailed examples and usage, refer to the documentation.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Comprehensive documentation is available at jax-agents.readthedocs.io, covering:</p> <ul> <li>Installation and setup</li> <li>Detailed API references</li> <li>Tutorials and examples</li> <li>Advanced topics and customization</li> </ul>"},{"location":"#development","title":"Development","text":"<p>To contribute or modify the library:</p> <pre><code>git clone https://github.com/amavrits/jax-agents.git\ncd jax-agents\npip install -e .\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under a proprietary license. For more information, please refer to the LICENSE file.</p> <p>For any questions or contributions, feel free to open an issue or submit a pull request on the GitHub repository.</p>"},{"location":"reference/ippo/","title":"IPPO","text":"<p>               Bases: <code>IPPOBase</code></p> <p>IPPO clip agent using the GAE (PPO2) for calculating the advantage. The actor loss function standardizes the advantage.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>class IPPO(IPPOBase):\n\n    \"\"\"\n    IPPO clip agent using the GAE (PPO2) for calculating the advantage. The actor loss function standardizes the\n    advantage.\n    \"\"\"\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the returns per episode step over a batch of trajectories.\n        :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n        state). In the begining of the method, 'value' is the value of the state in the next step in the trajectory\n        (not the reverse iteration), and after calculation it is the value of the examined state in the examined step.\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n        rewards, discounts, next_state_values, gae_lambda = traj\n        value = rewards + discounts * ((1 - gae_lambda) * next_state_values + gae_lambda * value)\n        return value, value\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _trajectory_advantages(self, advantage: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the GAE per episode step over a batch of trajectories.\n        :param advantage: The GAE advantages of the steps in the trajectory according to the critic (including the one\n        of the last state). In the beginning of the method, 'advantage' is the advantage of the state in the next step\n        in the trajectory (not the reverse iteration), and after calculation it is the advantage of the examined state\n        in each step.\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n        rewards, values, next_state_values, terminated, gamma, gae_lambda = traj\n        d_t = rewards + (1 - terminated) * gamma * next_state_values - values  # Temporal difference residual at time t\n        advantage = d_t + gamma * gae_lambda * (1 - terminated) * advantage\n        return advantage, advantage\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_loss(\n            self,\n            training: TrainState,\n            obs: Annotated[ObsType, \"n_rollout batch_size\"],\n            actions: Annotated[ActionType, \"batch_size\"],\n            log_prob_old: Float[Array, \"n_rollout batch_size\"],\n            advantage: ReturnsType,\n            hyperparams: HyperParameters\n    )-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n        \"\"\"\n        Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n        discounted returns and the value as estimated by the critic.\n        :param training: The actor TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param actions: The actions in the trajectory batch.\n        :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n        :param advantage: The GAE over the trajectory batch.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n        \"\"\"\n\n        \"\"\" Standardize GAE, greatly improves behaviour\"\"\"\n        advantage = (advantage - advantage.mean(axis=0)) / (advantage.std(axis=0) + 1e-8)\n\n        log_prob_vmap = jax.vmap(jax.vmap(self._log_prob, in_axes=(None, None, 0, 0)), in_axes=(None, None, 0, 0))\n        log_prob = log_prob_vmap(training, training.params, obs, actions)\n        log_policy_ratio = log_prob - log_prob_old\n        policy_ratio = jnp.exp(log_policy_ratio)\n        kl = jnp.sum(-log_policy_ratio)\n\n        \"\"\"\n        Adopt simplified formulation of clipped policy ratio * advantage as explained in the note of:\n        https://spinningup.openai.com/en/latest/algorithms/ppo.html#id2\n        \"\"\"\n        clip = jnp.where(jnp.greater(advantage, 0), 1 + hyperparams.eps_clip, 1 - hyperparams.eps_clip)\n        advantage_clip = advantage * clip\n\n        \"\"\"Actual clip calculation - not used but left here for comparison to simplified version\"\"\"\n        # advantage_clip = jnp.clip(policy_ratio, 1 - hyperparams.eps_clip, 1 + hyperparams.eps_clip) * advantage\n\n        loss_actor = jnp.minimum(policy_ratio * advantage, advantage_clip)\n\n        entropy_vmap = jax.vmap(jax.vmap(self._entropy, in_axes=(None, 0)), in_axes=(None, 0))\n        entropy = entropy_vmap(training, obs)\n\n        total_loss_actor = loss_actor.mean() + hyperparams.ent_coeff * entropy.mean()\n\n        \"\"\" Negative loss, because we want ascent but 'apply_gradients' applies descent \"\"\"\n        return -total_loss_actor, kl\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss(\n            self,\n            training: TrainState,\n            obs: Annotated[ObsType, \"n_rollout batch_size\"],\n            targets: ReturnsType,\n            hyperparams: HyperParameters\n    ) -&gt; Float[Array, \"1\"]:\n        \"\"\"\n        Calculates the critic loss.\n        :param training: The critic TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param targets: The targets over the trajectory batch for training the critic.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: The critic loss.\n        \"\"\"\n\n        value_vmap = jax.vmap(jax.vmap(training.apply_fn, in_axes=(None, 0)), in_axes=(None, 0))\n        value = value_vmap(training.params, obs)\n        residuals = value - targets\n        value_loss = jnp.mean(residuals ** 2)\n        critic_total_loss = hyperparams.vf_coeff * value_loss\n\n        return critic_total_loss\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; ActorLossInputType:\n        \"\"\"\n        Prepares the input required by the actor loss function. For the PPO agent, this entails the:\n        - the actions collected over the trajectory batch.\n        - the log-probability of the actions collected over the trajectory batch.\n        - the returns over the trajectory batch.\n        - the values over the trajectory batch as evaluated by the critic.\n        - the training hyperparameters.\n        The input is reshaped so that it is split into minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the actor loss function.\n        \"\"\"\n\n        # Shuffle the trajectory batch to collect minibatches.\n        # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n        # the batch are used per epoch.\n        minibatch_idx = jax.random.choice(\n            jax.random.PRNGKey(1),\n            jnp.arange(self.config.batch_size),\n            replace=False,\n            shape=(self.config.batch_size,)\n        )\n\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n        traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n        return (\n            traj_minibatch.obs,\n            traj_minibatch.action,\n            traj_minibatch.log_prob,\n            traj_minibatch.advantage,\n            update_runner.hyperparams\n        )\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n        \"\"\"\n        Prepares the input required by the critic loss function. For the PPO agent, this entails the:\n        - the states collected over the trajectory batch.\n        - the targets (returns = GAE + next_value) over the trajectory batch.\n        - the training hyperparameters.\n        The input is reshaped so that it is split into minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the critic loss function.\n        \"\"\"\n\n        # Shuffle the trajectory batch to collect minibatches.\n        # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n        # the batch are used per epoch.\n        minibatch_idx = jax.random.choice(\n            jax.random.PRNGKey(1),\n            jnp.arange(self.config.batch_size),\n            replace=False,\n            shape=(self.config.batch_size,)\n        )\n\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n        traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n        return (\n            traj_minibatch.obs,\n            traj_minibatch.advantage + traj_minibatch.value,\n            update_runner.hyperparams\n        )\n</code></pre>"},{"location":"reference/ippo/#jaxagents.ippo.IPPO._actor_loss","title":"<code>_actor_loss(training, obs, actions, log_prob_old, advantage, hyperparams)</code>","text":"<p>Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the discounted returns and the value as estimated by the critic. :param training: The actor TrainState object. :param obs: The obs in the trajectory batch. :param actions: The actions in the trajectory batch. :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch. :param advantage: The GAE over the trajectory batch. :param hyperparams: The HyperParameters object used for training. :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_loss(\n        self,\n        training: TrainState,\n        obs: Annotated[ObsType, \"n_rollout batch_size\"],\n        actions: Annotated[ActionType, \"batch_size\"],\n        log_prob_old: Float[Array, \"n_rollout batch_size\"],\n        advantage: ReturnsType,\n        hyperparams: HyperParameters\n)-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n    \"\"\"\n    Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n    discounted returns and the value as estimated by the critic.\n    :param training: The actor TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param actions: The actions in the trajectory batch.\n    :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n    :param advantage: The GAE over the trajectory batch.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n    \"\"\"\n\n    \"\"\" Standardize GAE, greatly improves behaviour\"\"\"\n    advantage = (advantage - advantage.mean(axis=0)) / (advantage.std(axis=0) + 1e-8)\n\n    log_prob_vmap = jax.vmap(jax.vmap(self._log_prob, in_axes=(None, None, 0, 0)), in_axes=(None, None, 0, 0))\n    log_prob = log_prob_vmap(training, training.params, obs, actions)\n    log_policy_ratio = log_prob - log_prob_old\n    policy_ratio = jnp.exp(log_policy_ratio)\n    kl = jnp.sum(-log_policy_ratio)\n\n    \"\"\"\n    Adopt simplified formulation of clipped policy ratio * advantage as explained in the note of:\n    https://spinningup.openai.com/en/latest/algorithms/ppo.html#id2\n    \"\"\"\n    clip = jnp.where(jnp.greater(advantage, 0), 1 + hyperparams.eps_clip, 1 - hyperparams.eps_clip)\n    advantage_clip = advantage * clip\n\n    \"\"\"Actual clip calculation - not used but left here for comparison to simplified version\"\"\"\n    # advantage_clip = jnp.clip(policy_ratio, 1 - hyperparams.eps_clip, 1 + hyperparams.eps_clip) * advantage\n\n    loss_actor = jnp.minimum(policy_ratio * advantage, advantage_clip)\n\n    entropy_vmap = jax.vmap(jax.vmap(self._entropy, in_axes=(None, 0)), in_axes=(None, 0))\n    entropy = entropy_vmap(training, obs)\n\n    total_loss_actor = loss_actor.mean() + hyperparams.ent_coeff * entropy.mean()\n\n    \"\"\" Negative loss, because we want ascent but 'apply_gradients' applies descent \"\"\"\n    return -total_loss_actor, kl\n</code></pre>"},{"location":"reference/ippo/#jaxagents.ippo.IPPO._actor_loss_input","title":"<code>_actor_loss_input(update_runner, traj_batch)</code>","text":"<p>Prepares the input required by the actor loss function. For the PPO agent, this entails the: - the actions collected over the trajectory batch. - the log-probability of the actions collected over the trajectory batch. - the returns over the trajectory batch. - the values over the trajectory batch as evaluated by the critic. - the training hyperparameters. The input is reshaped so that it is split into minibatches. :param update_runner: The Runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the actor loss function.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; ActorLossInputType:\n    \"\"\"\n    Prepares the input required by the actor loss function. For the PPO agent, this entails the:\n    - the actions collected over the trajectory batch.\n    - the log-probability of the actions collected over the trajectory batch.\n    - the returns over the trajectory batch.\n    - the values over the trajectory batch as evaluated by the critic.\n    - the training hyperparameters.\n    The input is reshaped so that it is split into minibatches.\n    :param update_runner: The Runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the actor loss function.\n    \"\"\"\n\n    # Shuffle the trajectory batch to collect minibatches.\n    # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n    # the batch are used per epoch.\n    minibatch_idx = jax.random.choice(\n        jax.random.PRNGKey(1),\n        jnp.arange(self.config.batch_size),\n        replace=False,\n        shape=(self.config.batch_size,)\n    )\n\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n    traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n    return (\n        traj_minibatch.obs,\n        traj_minibatch.action,\n        traj_minibatch.log_prob,\n        traj_minibatch.advantage,\n        update_runner.hyperparams\n    )\n</code></pre>"},{"location":"reference/ippo/#jaxagents.ippo.IPPO._critic_loss","title":"<code>_critic_loss(training, obs, targets, hyperparams)</code>","text":"<p>Calculates the critic loss. :param training: The critic TrainState object. :param obs: The obs in the trajectory batch. :param targets: The targets over the trajectory batch for training the critic. :param hyperparams: The HyperParameters object used for training. :return: The critic loss.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_loss(\n        self,\n        training: TrainState,\n        obs: Annotated[ObsType, \"n_rollout batch_size\"],\n        targets: ReturnsType,\n        hyperparams: HyperParameters\n) -&gt; Float[Array, \"1\"]:\n    \"\"\"\n    Calculates the critic loss.\n    :param training: The critic TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param targets: The targets over the trajectory batch for training the critic.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: The critic loss.\n    \"\"\"\n\n    value_vmap = jax.vmap(jax.vmap(training.apply_fn, in_axes=(None, 0)), in_axes=(None, 0))\n    value = value_vmap(training.params, obs)\n    residuals = value - targets\n    value_loss = jnp.mean(residuals ** 2)\n    critic_total_loss = hyperparams.vf_coeff * value_loss\n\n    return critic_total_loss\n</code></pre>"},{"location":"reference/ippo/#jaxagents.ippo.IPPO._critic_loss_input","title":"<code>_critic_loss_input(update_runner, traj_batch)</code>","text":"<p>Prepares the input required by the critic loss function. For the PPO agent, this entails the: - the states collected over the trajectory batch. - the targets (returns = GAE + next_value) over the trajectory batch. - the training hyperparameters. The input is reshaped so that it is split into minibatches. :param update_runner: The Runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the critic loss function.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n    \"\"\"\n    Prepares the input required by the critic loss function. For the PPO agent, this entails the:\n    - the states collected over the trajectory batch.\n    - the targets (returns = GAE + next_value) over the trajectory batch.\n    - the training hyperparameters.\n    The input is reshaped so that it is split into minibatches.\n    :param update_runner: The Runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the critic loss function.\n    \"\"\"\n\n    # Shuffle the trajectory batch to collect minibatches.\n    # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n    # the batch are used per epoch.\n    minibatch_idx = jax.random.choice(\n        jax.random.PRNGKey(1),\n        jnp.arange(self.config.batch_size),\n        replace=False,\n        shape=(self.config.batch_size,)\n    )\n\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n    traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n    return (\n        traj_minibatch.obs,\n        traj_minibatch.advantage + traj_minibatch.value,\n        update_runner.hyperparams\n    )\n</code></pre>"},{"location":"reference/ippo/#jaxagents.ippo.IPPO._trajectory_advantages","title":"<code>_trajectory_advantages(advantage, traj)</code>","text":"<p>Calculates the GAE per episode step over a batch of trajectories. :param advantage: The GAE advantages of the steps in the trajectory according to the critic (including the one of the last state). In the beginning of the method, 'advantage' is the advantage of the state in the next step in the trajectory (not the reverse iteration), and after calculation it is the advantage of the examined state in each step. :param traj: The trajectory batch. :return: An array of returns.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _trajectory_advantages(self, advantage: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the GAE per episode step over a batch of trajectories.\n    :param advantage: The GAE advantages of the steps in the trajectory according to the critic (including the one\n    of the last state). In the beginning of the method, 'advantage' is the advantage of the state in the next step\n    in the trajectory (not the reverse iteration), and after calculation it is the advantage of the examined state\n    in each step.\n    :param traj: The trajectory batch.\n    :return: An array of returns.\n    \"\"\"\n    rewards, values, next_state_values, terminated, gamma, gae_lambda = traj\n    d_t = rewards + (1 - terminated) * gamma * next_state_values - values  # Temporal difference residual at time t\n    advantage = d_t + gamma * gae_lambda * (1 - terminated) * advantage\n    return advantage, advantage\n</code></pre>"},{"location":"reference/ippo/#jaxagents.ippo.IPPO._trajectory_returns","title":"<code>_trajectory_returns(value, traj)</code>","text":"<p>Calculates the returns per episode step over a batch of trajectories. :param value: The values of the steps in the trajectory according to the critic (including the one of the last state). In the begining of the method, 'value' is the value of the state in the next step in the trajectory (not the reverse iteration), and after calculation it is the value of the examined state in the examined step. :param traj: The trajectory batch. :return: An array of returns.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the returns per episode step over a batch of trajectories.\n    :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n    state). In the begining of the method, 'value' is the value of the state in the next step in the trajectory\n    (not the reverse iteration), and after calculation it is the value of the examined state in the examined step.\n    :param traj: The trajectory batch.\n    :return: An array of returns.\n    \"\"\"\n    rewards, discounts, next_state_values, gae_lambda = traj\n    value = rewards + discounts * ((1 - gae_lambda) * next_state_values + gae_lambda * value)\n    return value, value\n</code></pre>"},{"location":"reference/ippobase/","title":"IPPOBase","text":"<p>               Bases: <code>ABC</code></p> <p>Base for PPO agents. Can be used for both discrete or continuous action environments, and its use depends on the provided actor network. Follows the instructions of: https://spinningup.openai.com/en/latest/algorithms/ppo.html Uses lax.scan for rollout, so trajectories may be truncated.</p> <p>Training relies on jitting several methods by treating the 'self' arg as static. According to suggested practice, this can prove dangerous (https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods - How to use jit with methods?); if attrs of 'self' change during training, the changes will not be registered in jit. In this case, neither agent training nor evaluation change any 'self' attrs, so using Strategy 2 of the suggested practice is valid. Otherwise, strategy 3 should have been used.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>class IPPOBase(ABC):\n    \"\"\"\n        Base for PPO agents.\n        Can be used for both discrete or continuous action environments, and its use depends on the provided actor network.\n        Follows the instructions of: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n        Uses lax.scan for rollout, so trajectories may be truncated.\n\n        Training relies on jitting several methods by treating the 'self' arg as static. According to suggested practice,\n        this can prove dangerous (https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods -\n        How to use jit with methods?); if attrs of 'self' change during training, the changes will not be registered in\n        jit. In this case, neither agent training nor evaluation change any 'self' attrs, so using Strategy 2 of the\n        suggested practice is valid. Otherwise, strategy 3 should have been used.\n        \"\"\"\n\n    # Number of actors in environment\n    n_actors: int\n    # Function for performing a minibatch update of the actor network.\n    _actor_minibatch_fn: ClassVar[Callable[\n        [Tuple[TrainState, ActorLossInputType, float]],\n        Tuple[TrainState, ActorLossInputType, float]]\n    ]\n    # Function for performing a minibatch update of the critic network.\n    _critic_minibatch_fn: ClassVar[Callable[\n        [Tuple[TrainState, CriticLossInputType]],\n        Tuple[TrainState, CriticLossInputType]]\n    ]\n    agent_trained: ClassVar[bool] = False  # Whether the agent has been trained.\n    training_runner: ClassVar[Optional[Runner]] = None  # Runner object after training.\n    actor_training: ClassVar[Optional[TrainState]] = None  # Actor training object.\n    critic_training: ClassVar[Optional[TrainState]] = None  # Critic training object.\n    training_metrics: ClassVar[Optional[Dict]] = None  # Metrics collected during training.\n    eval_during_training: ClassVar[bool] = True  # Whether the agent's performance is evaluated during training\n    # The maximum step reached in precious training. Zero by default for starting a new training. Will be set by\n    # restoring or passing a trained agent (from serial training or restoring)\n    previous_training_max_step: ClassVar[int] = 0\n\n    def __init__(\n            self,\n            env: Environment,\n            env_params: EnvParams,\n            config: AgentConfig,\n            eval_during_training: bool = True\n    ) -&gt; None:\n\n        \"\"\"\n        :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n        :param env_params: A dataclass named \"EnvParams\" containing the parametrization of the environment.\n        :param config: The configuration of the agent as and AgentConfig object (from vpf_utils).\n        :param eval_during_training: Whether evaluation should be performed during training.\n        \"\"\"\n        self.n_actors = env.n_actors\n        self.config = config\n        self.eval_during_training = eval_during_training\n        self._init_checkpointer()\n        self._init_env(env, env_params)\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Returns a string containing only the non-default field values.\n        \"\"\"\n\n        output_lst = [field + ': ' + str(getattr(self.config, field)) for field in self.config._fields]\n        output_lst = ['Agent configuration:'] + output_lst\n\n        return '\\n'.join(output_lst)\n\n    \"\"\" GENERAL METHODS\"\"\"\n\n    def _init_env(self, env: Environment, env_params: EnvParams) -&gt; None:\n        \"\"\"\n        Environment initialization.\n        :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n        :param env_params: A dataclass containing the parametrization of the environment.\n        :return:\n        \"\"\"\n\n        env = TruncationWrapper(env, self.config.max_episode_steps)\n        # env = FlattenObservationWrapper(env)\n        # self.env = LogWrapper(env)\n        self.env = env\n        self.env_params = env_params\n\n    def _init_checkpointer(self) -&gt; None:\n        \"\"\"\n        Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If\n        so, sets the checkpoint manager using orbax.\n        :return:\n        \"\"\"\n\n        self.checkpointing = self.config.checkpoint_dir is not None\n\n        if self.checkpointing:\n\n            if not self.config.restore_agent:\n\n                dir_exists = os.path.exists(self.config.checkpoint_dir)\n                if not dir_exists:\n                    os.makedirs(self.config.checkpoint_dir)\n\n                dir_files = [\n                    file for file in os.listdir(self.config.checkpoint_dir)\n                    if os.path.isdir(os.path.join(self.config.checkpoint_dir, file))\n                ]\n                if len(dir_files) &gt; 0:\n                    for file in dir_files:\n                        file_path = os.path.join(self.config.checkpoint_dir, file)\n                        shutil.rmtree(file_path)\n\n                # Log training configuration\n                with open(os.path.join(self.config.checkpoint_dir, 'training_configuration.txt'), \"w\") as f:\n                    f.write(self.__str__())\n\n            orbax_checkpointer = orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler())\n\n            options = orbax.checkpoint.CheckpointManagerOptions(\n                create=True,\n                step_prefix='trainingstep',\n            )\n\n            self.checkpoint_manager = orbax.checkpoint.CheckpointManager(\n                self.config.checkpoint_dir,\n                orbax_checkpointer,\n                options\n            )\n\n        else:\n\n            self.checkpoint_manager = None\n\n    def _create_empty_trainstate(self, network) -&gt; TrainState:\n        \"\"\"\n        Creates an empty TrainState object for restoring checkpoints.\n        :param network: The actor or critic network.\n        :return:\n        \"\"\"\n\n        rng = jax.random.PRNGKey(1)  # Just a dummy PRNGKey for initializing the networks parameters.\n        network, params = self._init_network(rng, network)\n\n        optimizer_params = OptimizerParams()  # Use the default values of the OptimizerParams object.\n        tx = self._init_optimizer(optimizer_params)\n\n        empty_training = TrainState.create(apply_fn=network.apply, params=params, tx=tx)\n\n        return empty_training\n\n    def restore(\n            self,\n            mode: str = \"best\",\n            best_fn: Optional[Callable[[Dict[str, Float[Array, \"1\"]]], [int]]] = None\n    ) -&gt; None:\n        \"\"\"\n        Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then,\n        post-processes the restored checkpoint.\n        :param mode: Determines whether the best performing or latest checkpoint should be restored.\n        :param best_fn: The function that should be used in determining the best performing checkpoint.\n        :return:\n        \"\"\"\n\n        steps = self.checkpoint_manager.all_steps()\n\n        # Log keys in checkpoints\n        ckpt = self.checkpoint_manager.restore(steps[0])\n        ckpt_keys = [key for key in list(ckpt.keys()) if key != \"runner\"]\n\n        # Collect history of metrics in training. Useful for continuing training.\n        metrics = {key: [None] * len(steps) for key in ckpt_keys}\n        for i, step in enumerate(steps):\n            ckpt = self.checkpoint_manager.restore(step)\n            for key in ckpt_keys:\n                metrics[key][i] = ckpt[key][jnp.newaxis, :]\n        metrics = {key: jnp.concatenate(val, axis=0) for (key, val) in metrics.items()}\n\n        if mode == \"best\":\n            if best_fn is not None:\n                step = steps[best_fn(metrics)]\n            else:\n                raise Exception(\"Function for determining best checkpoint not provided\")\n        elif mode == \"last\":\n            step = self.checkpoint_manager.latest_step()\n        else:\n            raise Exception(\"Unknown method for selecting a checkpoint.\")\n\n        \"\"\"\n        Create an empty target for restoring the checkpoint.\n        Some of the arguments come from restoring one of the ckpts.\n        \"\"\"\n\n        empty_actor_training = self._create_empty_trainstate(self.config.actor_network)\n        empty_critic_training = self._create_empty_trainstate(self.config.critic_network)\n\n        # Get some state and envstate for restoring the checkpoint.\n        _, obs, envstate = self.env_reset(jax.random.PRNGKey(1))\n\n        empty_runner = Runner(\n            actor_training=empty_actor_training,\n            critic_training=empty_critic_training,\n            envstate=envstate,\n            obs=obs,\n            rng=jax.random.split(jax.random.PRNGKey(1), self.config.batch_size),  # Just a dummy PRNGKey for initializing the networks parameters.\n            # Hyperparams can be loaded as a dict. If training continues, new hyperparams will be provided.\n            hyperparams=ckpt[\"runner\"][\"hyperparams\"]\n        )\n\n        target_ckpt = {\n            \"runner\": empty_runner,\n            \"terminated\": jnp.zeros(metrics[\"terminated\"].shape[1]),\n            \"truncated\": jnp.zeros(metrics[\"truncated\"].shape[1]),\n            \"final_rewards\": jnp.zeros(metrics[\"final_rewards\"].shape[1]),\n            \"returns\": jnp.zeros(metrics[\"returns\"].shape[1]),\n        }\n\n        ckpt = self.checkpoint_manager.restore(step, items=target_ckpt)\n\n        self.collect_training(ckpt[\"runner\"], metrics, previous_training_max_step=max(steps))\n\n    def _init_optimizer(self, optimizer_params: OptimizerParams) -&gt; optax.chain:\n        \"\"\"\n        Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to\n        initialize the appropriate optimizer. In this way, the optimizer can be initialized within the \"train\" method,\n        and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary.\n        :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n        :return: An optimizer in optax.chain.\n        \"\"\"\n\n        optimizer_params_dict = optimizer_params._asdict()  # Transform from NamedTuple to dict\n        optimizer_params_dict.pop('grad_clip', None)  # Remove 'grad_clip', since it is not part of the optimizer args.\n\n\n        \"\"\"\n        Get dictionary of optimizer parameters to pass in optimizer. The procedure preserves parameters that:\n            - are given in the OptimizerParams NamedTuple and are requested as args by the optimizer\n            - are requested as args by the optimizer and are given in the OptimizerParams NamedTuple\n        \"\"\"\n\n        optimizer_arg_names = self.config.optimizer.__code__.co_varnames  # List names of args of optimizer.\n\n        # Keep only the optimizer arg names that are also part of the OptimizerParams (dict from NamedTuple)\n        optimizer_arg_names = [\n            arg_name for arg_name in optimizer_arg_names if arg_name in list(optimizer_params_dict.keys())\n        ]\n        if len(optimizer_arg_names) == 0:\n            raise Exception(\n                \"The defined optimizer parameters do not include relevant arguments for this optimizer.\"\n                \"The optimizer has not been implemented yet. Define your own OptimizerParams object.\"\n            )\n\n        # Keep only the optimizer params that are arg names for the specific optimizer\n        optimizer_params_dict = {arg_name: optimizer_params_dict[arg_name] for arg_name in optimizer_arg_names}\n\n        # No need to scale by -1.0. 'TrainState.apply_gradients' is used for training, which subtracts the update.\n        tx = optax.chain(\n            optax.clip_by_global_norm(optimizer_params.grad_clip),\n            self.config.optimizer(**optimizer_params_dict)\n        )\n\n        return tx\n\n    def _init_network(\n            self,\n            rng: PRNGKeyArray,\n            network: flax.linen.Module\n    ) -&gt; Tuple[flax.linen.Module, FrozenDict]:\n        \"\"\"\n        Initialization of the actor or critic network.\n        :param rng: Random key for initialization.\n        :param network: The actor or critic network.\n        :return: A random key after splitting the input and the initial parameters of the policy network.\n        \"\"\"\n\n        network = network(self.config)\n\n        rng, *_rng = jax.random.split(rng, 3)\n        dummy_reset_rng, network_init_rng = _rng\n\n        _, dummy_obs, _ = self.env_reset(dummy_reset_rng)\n        init_x = jnp.zeros((1, dummy_obs.size))\n\n        params = network.init(network_init_rng, init_x)\n\n        return network, params\n\n    @partial(jax.jit, static_argnums=(0,))\n    def env_reset(self, rng: PRNGKeyArray) -&gt; Tuple[PRNGKeyArray, ObsType, LogEnvState | EnvState | TruncationEnvState]:\n        \"\"\"\n        Environment reset.\n        :param rng: Random key for initialization.\n        :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.\n        \"\"\"\n\n        rng, reset_rng = jax.random.split(rng)\n        obs, envstate = self.env.reset(reset_rng, self.env_params)\n\n        return rng, obs, envstate\n\n    @partial(jax.jit, static_argnums=(0,))\n    def env_step(\n            self,\n            rng: PRNGKeyArray,\n            envstate: LogEnvState | EnvState | TruncationEnvState,\n            actions: ActionType\n    )-&gt; Tuple[\n        PRNGKeyArray,\n        ObsType,\n        LogEnvState | EnvState | TruncationEnvState,\n        Float[Array, \"1\"],\n        Bool[Array, \"1\"],\n        Dict[str, float | bool]\n    ]:\n        \"\"\"\n        Environment step.\n        :param rng: Random key for initialization.\n        :param envstate: The environment state in LogEnvState format.\n        :param actions: The actions selected per actor.\n        :return: A tuple of: a random key after splitting the input, the next state in array and LogEnvState formats,\n                 the collected reward after executing the action, episode termination and a dictionary of optional\n                 additional information.\n        \"\"\"\n\n        rng, step_rng = jax.random.split(rng)\n        next_obs, next_envstate, reward, done, info = \\\n            self.env.step(step_rng, envstate, actions.squeeze(), self.env_params)\n\n        return rng, next_obs, next_envstate, reward, done, info\n\n    \"\"\" METHODS FOR TRAINING \"\"\"\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _make_transition(\n            self,\n            obs: ObsType,\n            actions: ActionType,\n            value: Float[Array, \"n_actors\"],\n            log_prob: Float[Array, \"n_actors\"],\n            reward: Float[Array, \"n_actors\"],\n            next_obs: ObsType,\n            terminated: Bool[Array, \"1\"],\n            ) -&gt; Transition:\n        \"\"\"\n        Creates a transition object based on the input and output of an episode step.\n        :param obs: The current state of the episode step in array format.\n        :param actions: The action selected per actor.\n        :param value: The value of the state per critic.\n        :param log_prob: The actor log-probability of the selected action.\n        :param reward: The collected reward after executing the action per actor.\n        :param next_obs: The next state of the episode step in array format.\n        :param terminated: Episode termination.\n        :return: A transition object storing information about the state before and after executing the episode step,\n                 the executed action, the collected reward, episode termination and optional additional information.\n        \"\"\"\n\n        transition = Transition(obs.squeeze(), actions, value, log_prob, reward, next_obs, terminated)\n        transition = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), transition)\n\n        return transition\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _generate_metrics(self, runner: Runner, update_step: int) -&gt; Dict[str, Float[Array, \"n_agents\"]]:\n        \"\"\"\n        Generates metrics for on-policy learning. The agent performance during training is evaluated by running\n        n_evals episodes (until termination). If the user selects not to generate metrics (leading to faster training),\n        an empty dictionary is returned.\n        :param runner: The update runner object, containing information about the current status of the actor's/critic's\n        training, the state of the environment and training hyperparameters.\n        :param update_step: The number of the update step.\n        :return: A dictionary of the sum of rewards collected over 'n_evals' episodes, or empty dictionary.\n        \"\"\"\n\n        metric = {}\n        if self.eval_during_training:\n            metric = self._eval_agent(\n                self.config.eval_rng,\n                runner.actor_training,\n                runner.critic_training,\n                self.config.n_evals\n            )\n\n        metric.update({\n            \"actor_loss\": runner.actor_loss,\n            \"critic_loss\": runner.critic_loss\n        })\n\n        return metric\n\n    def _create_training(\n            self,\n            rng: PRNGKeyArray,\n            network: type[flax.linen.Module],\n            optimizer_params: OptimizerParams\n    )-&gt; TrainState:\n        \"\"\"\n         Creates a TrainState object for the actor or the critic.\n        :param rng: Random key for initialization.\n        :param network: The actor or critic network.\n        :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n        :return: A TrainState object to be used in training the actor and cirtic networks.\n        \"\"\"\n\n        network, params = self._init_network(rng, network)\n        tx = self._init_optimizer(optimizer_params)\n        return TrainState.create(apply_fn=network.apply, tx=tx, params=params)\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _create_update_runner(\n            self,\n            rng: PRNGKeyArray,\n            actor_training: TrainState,\n            critic_training: TrainState,\n            hyperparams: HyperParameters\n    ) -&gt; Runner:\n        \"\"\"\n        Initializes the update runner as a Runner object. The runner contains batch_size initializations of the\n        environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and\n        one for the critic network, so that trajectory batches are used to train the same parameters.\n        :param rng: Random key for initialization.\n        :param actor_training: The actor TrainState objects used in training.\n        :param critic_training: The critic TrainState objects used in training.\n        :param hyperparams: An instance of HyperParameters for training.\n        :return: An update runner object to be used in trajectory sampling and training.\n        \"\"\"\n\n        rng, reset_rng, runner_rng = jax.random.split(rng, 3)\n        reset_rngs = jax.random.split(reset_rng, self.config.batch_size)\n        runner_rngs = jax.random.split(runner_rng, self.config.batch_size)\n\n        _, obs, envstate = jax.vmap(self.env_reset)(reset_rngs)\n\n        update_runner = Runner(\n            actor_training=actor_training,\n            critic_training=critic_training,\n            envstate=envstate,\n            obs=obs,\n            rng=runner_rngs,\n            hyperparams=hyperparams,\n            actor_loss=jnp.zeros(1),\n            critic_loss=jnp.zeros(1),\n        )\n\n        return update_runner\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _add_next_values(\n            self,\n            traj_batch: Transition,\n            last_obs: ObsType,\n            critic_training: TrainState\n    ) -&gt; Transition:\n\n        last_state_value_vmap = jax.vmap(critic_training.apply_fn, in_axes=(None, 0))\n        last_state_value = last_state_value_vmap(lax.stop_gradient(critic_training.params), last_obs)\n        last_state_value = jnp.expand_dims(last_state_value, axis=1)\n\n        \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n        next_values_t = jnp.concatenate([\n            traj_batch.value.squeeze(),\n            last_state_value\n        ], axis=1)[:, 1:, :]\n\n        traj_batch = traj_batch._replace(next_value=next_values_t)\n\n        return traj_batch\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _add_advantages(self, traj_batch: Transition, advantage: ReturnsType) -&gt; Transition:\n\n        traj_batch = traj_batch._replace(advantage=advantage)\n\n        return traj_batch\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _returns(\n            self,\n            traj_batch: Transition,\n            last_next_state_value: Float[Array, \"batch_size\"],\n            gamma: float,\n            gae_lambda: float\n    ) -&gt; ReturnsType:\n        \"\"\"\n        Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the\n        trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n        episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n        step, trajectories do not start at the initial state.\n        :param traj_batch: The batch of trajectories.\n        :param last_next_state_value: The value of the last next state in each trajectory.\n        :param gamma: Discount factor\n        :param gae_lambda: The GAE \u03bb factor.\n        :return: The returns over the episodes of the trajectory batch.\n        \"\"\"\n\n        rewards_t = traj_batch.reward.squeeze()\n        terminated_t = 1.0 - traj_batch.terminated.astype(jnp.float32).squeeze()\n        discounts_t = (terminated_t * gamma).astype(jnp.float32)\n\n        \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n        next_state_values_t = jnp.concatenate(\n            [traj_batch.value.squeeze(), last_next_state_value[..., jnp.newaxis]],\n            axis=-1)[:, 1:]\n\n        rewards_t, discounts_t, next_state_values_t = jax.tree_util.tree_map(\n            lambda x: jnp.swapaxes(x, 0, 1), (rewards_t, discounts_t, next_state_values_t)\n        )\n\n        gae_lambda = jnp.ones_like(discounts_t) * gae_lambda\n\n        traj_runner = (rewards_t, discounts_t, next_state_values_t, gae_lambda)\n        end_value = jnp.take(next_state_values_t, -1, axis=0)  # Start from end of trajectory and work in reverse.\n        _, returns = lax.scan(self._trajectory_returns, end_value, traj_runner, reverse=True)\n\n        returns = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), returns)\n\n        return returns\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _advantages(\n            self,\n            traj_batch: Transition,\n            gamma: float,\n            gae_lambda: float\n    ) -&gt; ReturnsType:\n        \"\"\"\n        Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the\n        trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n        episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n        step, trajectories do not start at the initial state.\n        :param traj_batch: The batch of trajectories.\n        :param last_next_state_value: The value of the last next state in each trajectory.\n        :param gamma: Discount factor\n        :param gae_lambda: The GAE \u03bb factor.\n        :return: The advantages over the episodes of the trajectory batch.\n        \"\"\"\n\n        rewards_t = traj_batch.reward.squeeze()\n        values_t = traj_batch.value.squeeze()\n        terminated_t = jnp.expand_dims(traj_batch.terminated, axis=-1)\n        next_state_values_t = traj_batch.next_value.squeeze()\n        gamma_t = jnp.ones_like(terminated_t) * gamma\n        gae_lambda_t = jnp.ones_like(terminated_t) * gae_lambda\n\n        rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t = jax.tree_util.tree_map(\n            lambda x: jnp.swapaxes(x, 0, 1),\n            (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n        )\n\n        traj_runner = (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n        \"\"\"\n        TODO:\n        Advantage of last step is taken from the critic, in contrast to traditional approaches, where the rollout \n        ends with episode termination and the advantage is zero. Training is still successful and the influence of this\n        implementation choice is negligible.\n        \"\"\"\n        end_advantage = jnp.zeros((self.config.batch_size, self.n_actors))\n        _, advantages = jax.lax.scan(self._trajectory_advantages, end_advantage, traj_runner, reverse=True)\n        advantages = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), advantages)\n\n        return advantages\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _make_rollout_runners(self, update_runner: Runner) -&gt; Tuple[StepRunnerType, ...]:\n        \"\"\"\n        Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner\n        object and broadcasting the TrainState object for the critic and the network in the update_runner object to the\n        same dimension.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :return: Tuple with step runners to be used in rollout.\n        \"\"\"\n\n        rollout_runner = (\n            update_runner.envstate,\n            update_runner.obs,\n            update_runner.actor_training,\n            update_runner.critic_training,\n            update_runner.rng,\n        )\n        rollout_runners = jax.vmap(\n            lambda v, w, x, y, z: (v, w, x, y, z), in_axes=(0, 0, None, None, 0)\n        )(*rollout_runner)\n        return rollout_runners\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _rollout(self, step_runner: StepRunnerType, i_step: int) -&gt; Tuple[StepRunnerType, Transition]:\n        \"\"\"\n        Evaluation of trajectory rollout. In each step the agent:\n        - evaluates policy and value\n        - selects action\n        - performs environment step\n        - creates step transition\n        :param step_runner: A tuple containing information on the environment state, the actor and critic training\n        (parameters and networks) and a random key.\n        :param i_step: Unused, required for lax.scan.\n        :return: The updated step_runner tuple and the rollout step transition.\n        \"\"\"\n\n        envstate, obs, actor_training, critic_training, rng = step_runner\n\n        rng, rng_action = jax.random.split(rng)\n        actions = self._sample_actions(rng_action, actor_training, obs)\n\n        values = critic_training.apply_fn(lax.stop_gradient(critic_training.params), obs)\n\n        log_prob = self._log_prob(actor_training, lax.stop_gradient(actor_training.params), obs, actions)\n\n        rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, actions)\n\n        step_runner = (next_envstate, next_obs, actor_training, critic_training, rng)\n\n        terminated = info[\"terminated\"]\n\n        transition = self._make_transition(\n            obs=obs,\n            actions=actions,\n            value=values,\n            log_prob=log_prob,\n            reward=reward,\n            next_obs=next_obs,\n            terminated=terminated,\n        )\n\n        return step_runner, transition\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _process_trajectory(self, update_runner: Runner, traj_batch: Transition, last_obs: ObsType) -&gt; Transition:\n        \"\"\"\n        Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not\n        guaranteed to end with termination, the value is estimated using the critic network. This assumption has been\n        shown to have no influence by the end of training.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param traj_batch: The batch of trajectories, as collected by in rollout.\n        :param last_state: The state at the end of every trajectory in the batch.\n        :return: A batch of trajectories that includes an estimate of values and advantages.\n        \"\"\"\n\n        traj_batch = jax.tree_util.tree_map(lambda x: x.squeeze(), traj_batch)\n        traj_batch = self._add_next_values(traj_batch, last_obs, update_runner.critic_training)\n\n        advantages = self._advantages(traj_batch, update_runner.hyperparams.gamma, update_runner.hyperparams.gae_lambda)\n        traj_batch = self._add_advantages(traj_batch, advantages)\n\n        return traj_batch\n\n    @staticmethod\n    def _actor_minibatch_update(\n            i_minibatch: int,\n            minibatch_runner: Tuple[TrainState, ActorLossInputType, float],\n            grad_fn: Callable[[Any], ActorLossInputType]\n    ) -&gt; Annotated[Tuple[TrainState, ActorLossInputType, float], \"n_minibatch\"]:\n        \"\"\"\n        Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be\n        passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n        :param i_minibatch: Number of minibatch update.\n        :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence.\n        :param grad_fn: The gradient function of the training loss.\n        :return: Minibatch runner with an updated TrainState.\n        \"\"\"\n\n        actor_training, actor_loss_input, kl = minibatch_runner\n        *traj_batch, hyperparams = actor_loss_input\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n        grad_input_minibatch = (actor_training, *traj_minibatch, hyperparams)\n        grads, kl = grad_fn(*grad_input_minibatch)\n        actor_training = actor_training.apply_gradients(grads=grads.params)\n        return actor_training, actor_loss_input, kl\n\n    @staticmethod\n    def _critic_minibatch_update(\n            i_minibatch: int,\n            minibatch_runner: Tuple[TrainState, CriticLossInputType],\n            grad_fn: Callable[[Any], CriticLossInputType]\n    ) -&gt; Tuple[TrainState, CriticLossInputType]:\n        \"\"\"\n        Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be\n        passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n        :param i_minibatch: Number of minibatch update.\n        :param minibatch_runner: A tuple containing the TranState object and the loss input arguments.\n        :param grad_fn: The gradient function of the training loss.\n        :return: Minibatch runner with an updated TrainState.\n        \"\"\"\n\n        critic_training, critic_loss_input = minibatch_runner\n        *traj_batch, hyperparams = critic_loss_input\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n        grad_input_minibatch = (critic_training, *traj_minibatch, hyperparams)\n        grads = grad_fn(*grad_input_minibatch)\n        critic_training = critic_training.apply_gradients(grads=grads.params)\n        return critic_training, critic_loss_input\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_epoch(\n            self,\n            epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]\n    ) -&gt; Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]:\n        \"\"\"\n        Performs a Gradient Ascent update of the actor.\n        :param epoch_runner: A tuple containing the following information about the update:\n        - actor_training: TrainState object for actor training\n        - actor_loss_input: Tuple with the inputs required by the actor loss function.\n        - kl: The KL divergence collected during the update (used in checking for early stopping).\n        - epoch: The number of the current training epoch.\n        - kl_threshold: The KL divergence threshold for early stopping.\n        :return: The updated epoch runner.\n        \"\"\"\n\n        actor_training, actor_loss_input, kl, epoch, kl_threshold = epoch_runner\n        minibatch_runner = (actor_training, actor_loss_input, 0)\n        n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n        minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._actor_minibatch_fn, minibatch_runner)\n        actor_training, _, kl = minibatch_runner\n\n        return actor_training, actor_loss_input, kl, epoch+1, kl_threshold\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_training_cond(\n            self,\n            epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, Float[Array, \"1\"]]\n    ) -&gt; Bool[Array, \"1\"]:\n        \"\"\"\n        Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been\n        met or due to KL divergence early stopping).\n        :param epoch_runner: A tuple containing the following information about the update:\n        - actor_training: TrainState object for actor training\n        - actor_loss_input: Tuple with the inputs required by the actor loss function.\n        - kl: The KL divergence collected during the update (used in checking for early stopping).\n        - epoch: The number of the current training epoch.\n        - kl_threshold: The KL-divergence threshold for early stopping.\n        :return: Whether the lax.while_loop over training epochs finishes.\n        \"\"\"\n\n        _, _, kl, epoch, kl_threshold = epoch_runner\n        return jnp.logical_and(\n            jnp.less(epoch, self.config.actor_epochs),\n            jnp.less_equal(kl, kl_threshold)\n        )\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_update(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[TrainState, Float[Array, \"1\"]]:\n        \"\"\"\n        Prepares the input and performs Gradient Ascent for the actor network.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param traj_batch: The batch of trajectories.\n        :return: The actor training object updated after actor_epochs steps of Gradient Ascent.\n        \"\"\"\n\n        actor_loss_input = self._actor_loss_input(update_runner, traj_batch)\n\n        start_kl, start_epoch = -jnp.inf, 1\n        actor_epoch_runner = (\n            update_runner.actor_training,\n            actor_loss_input,\n            start_kl,\n            start_epoch,\n            update_runner.hyperparams.kl_threshold\n        )\n        actor_epoch_runner = lax.while_loop(self._actor_training_cond, self._actor_epoch, actor_epoch_runner)\n        actor_training, _, _, _, _ = actor_epoch_runner\n\n        actor_loss, _ = self._actor_loss(\n            actor_training,\n            traj_batch.obs,\n            traj_batch.action,\n            traj_batch.log_prob,\n            traj_batch.advantage,\n            update_runner.hyperparams\n        )\n\n        return actor_training, actor_loss\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_epoch(\n            self,\n            i_epoch: int,\n            epoch_runner: Tuple[TrainState, CriticLossInputType]\n    ) -&gt; Tuple[TrainState, CriticLossInputType]:\n        \"\"\"\n        Performs a Gradient Descent update of the critic.\n        :param: i_epoch: The current training epoch (unused but required by lax.fori_loop).\n        :param epoch_runner: A tuple containing the following information about the update:\n        - critic_training: TrainState object for critic training\n        - critic_loss_input: Tuple with the inputs required by the critic loss function.\n        :return: The updated epoch runner.\n        \"\"\"\n\n        critic_training, critic_loss_input = epoch_runner\n        minibatch_runner = (critic_training, critic_loss_input)\n        n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n        minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._critic_minibatch_fn, minibatch_runner)\n        critic_training, _ = minibatch_runner\n\n        return critic_training, critic_loss_input\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_update(self, update_runner: Runner, traj_batch: Transition) -&gt;  Tuple[TrainState, Float[Array, \"1\"]]:\n        \"\"\"\n        Prepares the input and performs Gradient Descent for the critic network.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param traj_batch: The batch of trajectories.\n        :return: The critic training object updated after actor_epochs steps of Gradient Ascent.\n        \"\"\"\n\n        critic_loss_input = self._critic_loss_input(update_runner, traj_batch)\n        critic_epoch_runner = (update_runner.critic_training, critic_loss_input)\n        critic_epoch_runner = lax.fori_loop(0, self.config.critic_epochs, self._critic_epoch, critic_epoch_runner)\n        critic_training, _ = critic_epoch_runner\n\n        critic_targets = critic_loss_input[1].reshape(-1, self.config.rollout_length, self.n_actors)\n        critic_loss = self._critic_loss(critic_training, traj_batch.obs, critic_targets, update_runner.hyperparams)\n\n        return critic_training, critic_loss\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _update_step(self, i_update_step: int, update_runner: Runner) -&gt; Runner:\n        \"\"\"\n        An update step of the actor and critic networks. This entails:\n        - performing rollout for sampling a batch of trajectories.\n        - assessing the value of the last state per trajectory using the critic.\n        - evaluating the advantage per trajectory.\n        - updating the actor and critic network parameters via the respective loss functions.\n        - generating in-training performance metrics.\n        In this approach, the update_runner already has a batch of environments initialized. The environments are not\n        initialized in the beginning of every update step, which means that trajectories to not necessarily start from\n        an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan\n        for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be\n        truncated in trajectory sampling).\n        :param i_update_step: Unused, required for progressbar.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :return: The updated runner\n        \"\"\"\n\n        rollout_runners = self._make_rollout_runners(update_runner)\n        scan_rollout_fn = lambda x: lax.scan(self._rollout, x, None, self.config.rollout_length)\n        rollout_runners, traj_batch = jax.vmap(scan_rollout_fn)(rollout_runners)\n        last_envstate, last_obs, _, _, rng = rollout_runners\n        traj_batch = self._process_trajectory(update_runner, traj_batch, last_obs)\n\n        actor_training, actor_loss = self._actor_update(update_runner, traj_batch)\n        critic_training, critic_loss = self._critic_update(update_runner, traj_batch)\n\n        \"\"\"Update runner as a dataclass.\"\"\"\n        update_runner = update_runner.replace(\n            envstate=last_envstate,\n            obs=last_obs,\n            actor_training=actor_training,\n            critic_training=critic_training,\n            rng=rng,\n            actor_loss=jnp.expand_dims(actor_loss, axis=-1),\n            critic_loss=jnp.expand_dims(critic_loss, axis=-1)\n        )\n\n        return update_runner\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _checkpoint(\n            self,\n            update_runner: Runner,\n            metrics: Dict[str, Float[Array, \"n_agents\"]],\n            i_training_step: int\n    ) -&gt; None:\n        \"\"\"\n        Wraps the base checkpointing method in a Python callback.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n        :param i_training_step: Training step\n        :return:\n        \"\"\"\n\n        jax.experimental.io_callback(self._checkpoint_base, None, update_runner, metrics, i_training_step)\n\n    def _checkpoint_base(\n            self,\n            update_runner: Runner,\n            metrics: Dict[str, Float[Array, \"1\"]],\n            i_training_step: int\n    ) -&gt; None:\n        \"\"\"\n        Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following:\n        - The training runner object.\n        - Returns of the evaluation episodes\n        The average return over the evaluated episodes is used as the checkpoint metric.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param metrics: Dictionary of evaluation metrics (return per episode evaluation)\n        :param i_training_step: Training step\n        :return:\n        \"\"\"\n\n        if self.checkpointing:\n\n            ckpt = {\n                \"runner\": update_runner,\n                \"terminated\": metrics[\"terminated\"],\n                \"truncated\": metrics[\"truncated\"],\n                \"final_rewards\": metrics[\"final_rewards\"],\n                \"returns\": metrics[\"returns\"]\n            }\n\n            save_args = orbax_utils.save_args_from_target(ckpt)\n\n            self.checkpoint_manager.save(\n                # Use maximum number of steps reached in previous training. Set to zero by default during agent\n                # initialization if a new training is executed. In case of continuing training, the checkpoint of step\n                # zero replaces the last checkpoint of the previous training. The two checkpoints are the same.\n                i_training_step+self.previous_training_max_step,\n                ckpt,\n                save_kwargs={'save_args': save_args},\n            )\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _training_step(\n            self,\n            update_runner: Runner,\n            i_training_batch: int\n    ) -&gt; Tuple[Runner, Dict[str, Float[Array, \"n_agents\"]]]:\n        \"\"\"\n        Performs trainings steps to update the agent per training batch.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param i_training_batch: Training batch loop counter.\n        :return: Tuple with updated runner and dictionary of metrics.\n        \"\"\"\n\n        n_training_steps = self.config.n_steps - self.config.n_steps // self.config.eval_frequency * i_training_batch\n        n_training_steps = jnp.clip(n_training_steps, 1, self.config.eval_frequency)\n\n        update_runner = lax.fori_loop(0, n_training_steps, self._update_step, update_runner)\n\n        if self.eval_during_training:\n            metrics = self._generate_metrics(runner=update_runner, update_step=i_training_batch)\n            i_training_step = self.config.eval_frequency * (i_training_batch + 1)\n            i_training_step = jnp.minimum(i_training_step, self.config.n_steps)\n            if self.checkpointing:\n                self._checkpoint(update_runner, metrics, i_training_step)\n        else:\n            metrics = {}\n\n        return update_runner, metrics\n\n    @partial(jax.jit, static_argnums=(0,))\n    def train(\n            self,\n            rng: PRNGKeyArray,\n            hyperparams: HyperParameters\n    ) -&gt; Tuple[Runner, Dict[str, Float[Array, \"n_agents\"]]]:\n        \"\"\"\n        Trains the agents. A jax_tqdm progressbar has been added in the lax.scan loop.\n        :param rng: Random key for initialization. This is the original key for training.\n        :param hyperparams: An instance of HyperParameters for training.\n        :return: The final state of the step runner after training and the training metrics accumulated over all\n                 training batches and steps.\n        \"\"\"\n\n        rng, *_rng = jax.random.split(rng, 4)\n        actor_init_rng, critic_init_rng, runner_rng = _rng\n\n        actor_training = self._create_training(\n            actor_init_rng, self.config.actor_network, hyperparams.actor_optimizer_params\n        )\n        critic_training = self._create_training(\n            critic_init_rng, self.config.critic_network, hyperparams.critic_optimizer_params\n        )\n\n        update_runner = self._create_update_runner(runner_rng, actor_training, critic_training, hyperparams)\n\n        # Checkpoint initial state\n        if self.eval_during_training:\n            metrics_start = self._generate_metrics(runner=update_runner, update_step=0)\n            if self.checkpointing:\n                self._checkpoint(update_runner, metrics_start, self.previous_training_max_step)\n\n        # Initialize agent updating functions, which can be avoided to be done within the training loops.\n        actor_grad_fn = jax.grad(self._actor_loss, has_aux=True, allow_int=True)\n        self._actor_minibatch_fn = lambda x, y: self._actor_minibatch_update(x, y, actor_grad_fn)\n\n        critic_grad_fn = jax.grad(self._critic_loss, allow_int=True)\n        self._critic_minibatch_fn = lambda x, y: self._critic_minibatch_update(x, y, critic_grad_fn)\n\n        # Train, evaluate, checkpoint\n        n_training_batches = self.config.n_steps // self.config.eval_frequency\n        progressbar_desc = f'Training batch (training steps = batch x {self.config.eval_frequency})'\n\n        runner, metrics = lax.scan(\n            scan_tqdm(n_training_batches, desc=progressbar_desc)(self._training_step),\n            update_runner,\n            jnp.arange(n_training_batches),\n            n_training_batches\n        )\n\n        if self.eval_during_training:\n            metrics = {\n                key: jnp.concatenate((metrics_start[key][jnp.newaxis, :], metrics[key]), axis=0)\n                for key in metrics.keys()\n            }\n        else:\n            metrics= {}\n\n        return runner, metrics\n\n    @abstractmethod\n    def _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the returns per episode step over a batch of trajectories.\n        :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n         state).\n        :param traj: The trajectory batch.\n        :return: A tuple of returns.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _trajectory_advantages(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the advantages per episode step over a batch of trajectories.\n        :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n         state).\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _actor_loss(\n            self,\n            training: TrainState,\n            obs: Float[Array, \"n_rollout batch_size obs_size\"],\n            action: Float[Array, \"n_rollout batch_size\"],\n            log_prob_old: Float[Array, \"n_rollout batch_size\"],\n            advantage: ReturnsType,\n            hyperparams: HyperParameters\n    )-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n        \"\"\"\n        Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n        discounted returns and the value as estimated by the critic.\n        :param training: The actor TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param action: The actions in the trajectory batch.\n        :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n        :param advantage: The advantage over the trajectory batch.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss(\n            self,\n            training: TrainState,\n            obs: Float[Array, \"n_rollout batch_size obs_size\"],\n            targets: Float[Array, \"batch_size n_rollout\"],\n            hyperparams: HyperParameters\n    ) -&gt; float:\n        \"\"\"\n        Calculates the critic loss.\n        :param training: The critic TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param targets: The returns over the trajectory batch, which act as the targets for training the critic.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: The critic loss.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[ActorLossInputType]:\n        \"\"\"\n        Prepares the input required by the actor loss function. The input is reshaped so that it is split into\n        minibatches.\n        :param update_runner: The runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the actor loss function.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n        \"\"\"\n        Prepares the input required by the critic loss function. The input is reshaped so that it is split into\n        minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the critic loss function.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _entropy(self, training: TrainState, obs: ObsType)-&gt; Float[Array, \"1\"]:\n        raise NotImplemented\n\n    @abstractmethod\n    def _log_prob(\n            self,\n            training: TrainState,\n            params: FrozenDict,\n            obs: ObsType,\n            action: ActionType\n    ) -&gt; Float[Array, \"n_actors\"]:\n        raise NotImplemented\n\n    @abstractmethod\n    def _sample_actions(\n            self,\n            rng: PRNGKeyArray,\n            training: TrainState,\n            obs: ObsType\n    ) -&gt; ActionType:\n        raise NotImplemented\n\n    \"\"\" METHODS FOR APPLYING AGENT\"\"\"\n\n    @abstractmethod\n    def policy(self, training: TrainState, obs: ObsType) -&gt; ActionType:\n        \"\"\"\n        Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state.\n        :param obs: The current obs of the episode step in array format.\n        :return:\n        \"\"\"\n        raise NotImplemented\n\n    def _eval_agent(\n            self,\n            rng: PRNGKeyArray,\n            actor_training: TrainState,\n            critic_training: TrainState,\n            n_episodes: int = 1\n    ) -&gt; Dict[str, Float[Array, \"n_agents\"] | Bool[Array, \"1\"]]:\n        \"\"\"\n        Evaluates the agents for n_episodes complete episodes using 'lax.while_loop'.\n        :param rng: A random key used for evaluating the agent.\n        :param actor_training: The actor TrainState object (either mid- or post-training).\n        :param critic_training: The critic TrainState object (either mid- or post-training).\n        :param n_episodes: The update_runner object used during training.\n        :return: The sum of rewards collected over n_episodes episodes.\n        \"\"\"\n\n        rng_eval = jax.random.split(rng, n_episodes)\n        rng, obs, envstate = jax.vmap(self.env_reset)(rng_eval)\n\n        eval_runner = (\n            envstate,\n            obs,\n            actor_training,\n            jnp.zeros(1, dtype=jnp.bool).squeeze(),\n            jnp.zeros(1, dtype=jnp.bool).squeeze(),\n            jnp.zeros(self.n_actors),\n            jnp.zeros(self.n_actors),\n            rng,\n        )\n\n        eval_runners = jax.vmap(\n            lambda s, t, u, v, w, x, y, z: (s, t, u, v, w, x, y, z),\n            in_axes=(0, 0, None, None, None, None, None, 0)\n        )(*eval_runner)\n\n        eval_runner = jax.vmap(lambda x: lax.while_loop(self._eval_cond, self._eval_body, x))(eval_runners)\n        _, _, _, terminated, truncated, final_rewards, returns, _ = eval_runner\n\n        return self._eval_metrics(terminated, truncated, final_rewards, returns)\n\n    def _eval_metrics(\n            self,\n            terminated: Bool[Array, \"1\"],\n            truncated: Bool[Array, \"1\"],\n            final_rewards: Float[Array, \"1\"],\n            returns: Float[Array, \"1\"]\n    ) -&gt; Dict[str, Float[Array, \"1\"] | Bool[Array, \"1\"]]:\n        \"\"\"\n        Evaluate the metrics.\n        :param terminated: Whether the episode finished by termination.\n        :param truncated: Whether the episode finished by truncation.\n        :param final_rewards: The rewards collected in the final step of the episode.\n        :param returns: The sum of rewards collected during the episode.\n        :return: Dictionary combining the input arguments and the case-specific special metrics.\n        \"\"\"\n        metrics = {\n            \"terminated\": terminated,\n            \"truncated\": truncated,\n            \"final_rewards\": final_rewards,\n            \"returns\": returns\n        }\n\n        return metrics\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _eval_body(self, eval_runner: EvalRunnerType) -&gt; EvalRunnerType:\n        \"\"\"\n        A step in the episode to be used with 'lax.while_loop' for evaluation of the agent in a complete episode.\n        :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n        states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n        over the episode and a random key.\n        :return: The updated eval_runner tuple.\n        \"\"\"\n\n        envstate, obs, actor_training, terminated, truncated, reward, returns, rng = eval_runner\n\n        actions = self.policy(actor_training, obs)\n\n        rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, actions)\n\n        terminated = info[\"terminated\"]\n        truncated = info[\"truncated\"]\n\n        returns += reward\n\n        eval_runner = (next_envstate, next_obs, actor_training, terminated, truncated, reward, returns, rng)\n\n        return eval_runner\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _eval_cond(self, eval_runner: EvalRunnerType) -&gt; Bool[Array, \"1\"]:\n        \"\"\"\n        Checks whether the episode is terminated, meaning that the 'lax.while_loop' can stop.\n        :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n        states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n        over the episode and a random key.\n        :return: Whether the episode is terminated, which means that the while loop must stop.\n        \"\"\"\n\n        _, _, _, terminated, truncated, _, _, _ = eval_runner\n        return jnp.logical_and(jnp.logical_not(terminated), jnp.logical_not(truncated))\n\n    def eval(self, rng: PRNGKeyArray, n_evals: int = 32) -&gt; Float[Array, \"n_evals\"]:\n        \"\"\"\n        Evaluates the trained agent's performance post-training using the trained agent's actor and critic.\n        :param rng: Random key for evaluation.\n        :param n_evals: Number of steps in agent evaluation.\n        :return: Dictionary of evaluation metrics.\n        \"\"\"\n\n        eval_metrics = self._eval_agent(rng, self.actor_training, self.critic_training, n_evals)\n\n        return eval_metrics\n\n    \"\"\" METHODS FOR POST-PROCESSING \"\"\"\n\n    def log_hyperparams(self, hyperparams: HyperParameters) -&gt; None:\n        \"\"\"\n        Logs training hyperparameters in a text file. To be used outside training.\n        :param hyperparams: An instance of HyperParameters for training.\n        :return:\n        \"\"\"\n\n        output_lst = [field + ': ' + str(getattr(hyperparams, field)) for field in hyperparams._fields]\n        output_lst = ['Hyperparameters:'] + output_lst\n        output_lst = '\\n'.join(output_lst)\n\n        if self.checkpointing:\n            with open(os.path.join(self.config.checkpoint_dir, 'hyperparameters.txt'), \"w\") as f:\n                f.write(output_lst)\n\n    def collect_training(\n            self,\n            runner: Optional[Runner] = None,\n            metrics: Optional[Dict[str, Float[Array, \"1\"]]] = None,\n            previous_training_max_step: int = 0\n    ) -&gt; None:\n        \"\"\"\n        Collects training or restored checkpoint of output (the final state of the runner after training and the\n        collected metrics).\n        :param runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters. This is at the state reached at\n        the end of training.\n        :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n        :param previous_training_max_step: Maximum step reached during training.\n        :return:\n        \"\"\"\n\n        self.agent_trained = True\n        self.previous_training_max_step = previous_training_max_step\n        self.training_runner = runner\n        self.training_metrics = metrics\n        n_evals = list(metrics.values())[0].shape[0]\n        self.eval_steps_in_training = jnp.arange(n_evals) * self.config.eval_frequency\n        self._pp()\n\n    def _pp(self) -&gt; None:\n        \"\"\"\n        Post-processes the training results, which includes:\n            - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored).\n        :return:\n        \"\"\"\n\n        self.actor_training = self.training_runner.actor_training\n        self.critic_training = self.training_runner.critic_training\n\n    def summarize(\n            self,\n            metrics: Annotated[NDArray[np.float32], \"size_metrics\"] | Float[Array, \"size_metrics\"]\n    ) -&gt; MetricStats:\n        \"\"\"\n        Summarizes collection of per-episode metrics.\n        :param metrics: Metric per episode.\n        :return: Summary of metric per episode.\n        \"\"\"\n\n        return MetricStats(\n            episode_metric=metrics,\n            mean=metrics.mean(axis=-1),\n            var=metrics.var(axis=-1),\n            std=metrics.std(axis=-1),\n            min=metrics.min(axis=-1),\n            max=metrics.max(axis=-1),\n            median=jnp.median(metrics, axis=-1),\n            has_nans=jnp.any(jnp.isnan(metrics), axis=-1)\n        )\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._actor_minibatch_fn","title":"<code>_actor_minibatch_fn</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._critic_minibatch_fn","title":"<code>_critic_minibatch_fn</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.actor_training","title":"<code>actor_training = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.agent_trained","title":"<code>agent_trained = False</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.critic_training","title":"<code>critic_training = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.eval_during_training","title":"<code>eval_during_training = eval_during_training</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.n_actors","title":"<code>n_actors = env.n_actors</code>  <code>instance-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.previous_training_max_step","title":"<code>previous_training_max_step = 0</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.training_metrics","title":"<code>training_metrics = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.training_runner","title":"<code>training_runner = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.__init__","title":"<code>__init__(env, env_params, config, eval_during_training=True)</code>","text":"<p>:param env: A gymnax or custom environment that inherits from the basic gymnax class. :param env_params: A dataclass named \"EnvParams\" containing the parametrization of the environment. :param config: The configuration of the agent as and AgentConfig object (from vpf_utils). :param eval_during_training: Whether evaluation should be performed during training.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def __init__(\n        self,\n        env: Environment,\n        env_params: EnvParams,\n        config: AgentConfig,\n        eval_during_training: bool = True\n) -&gt; None:\n\n    \"\"\"\n    :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n    :param env_params: A dataclass named \"EnvParams\" containing the parametrization of the environment.\n    :param config: The configuration of the agent as and AgentConfig object (from vpf_utils).\n    :param eval_during_training: Whether evaluation should be performed during training.\n    \"\"\"\n    self.n_actors = env.n_actors\n    self.config = config\n    self.eval_during_training = eval_during_training\n    self._init_checkpointer()\n    self._init_env(env, env_params)\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.__str__","title":"<code>__str__()</code>","text":"<p>Returns a string containing only the non-default field values.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string containing only the non-default field values.\n    \"\"\"\n\n    output_lst = [field + ': ' + str(getattr(self.config, field)) for field in self.config._fields]\n    output_lst = ['Agent configuration:'] + output_lst\n\n    return '\\n'.join(output_lst)\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._actor_epoch","title":"<code>_actor_epoch(epoch_runner)</code>","text":"<p>Performs a Gradient Ascent update of the actor. :param epoch_runner: A tuple containing the following information about the update: - actor_training: TrainState object for actor training - actor_loss_input: Tuple with the inputs required by the actor loss function. - kl: The KL divergence collected during the update (used in checking for early stopping). - epoch: The number of the current training epoch. - kl_threshold: The KL divergence threshold for early stopping. :return: The updated epoch runner.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_epoch(\n        self,\n        epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]\n) -&gt; Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]:\n    \"\"\"\n    Performs a Gradient Ascent update of the actor.\n    :param epoch_runner: A tuple containing the following information about the update:\n    - actor_training: TrainState object for actor training\n    - actor_loss_input: Tuple with the inputs required by the actor loss function.\n    - kl: The KL divergence collected during the update (used in checking for early stopping).\n    - epoch: The number of the current training epoch.\n    - kl_threshold: The KL divergence threshold for early stopping.\n    :return: The updated epoch runner.\n    \"\"\"\n\n    actor_training, actor_loss_input, kl, epoch, kl_threshold = epoch_runner\n    minibatch_runner = (actor_training, actor_loss_input, 0)\n    n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n    minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._actor_minibatch_fn, minibatch_runner)\n    actor_training, _, kl = minibatch_runner\n\n    return actor_training, actor_loss_input, kl, epoch+1, kl_threshold\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._actor_loss","title":"<code>_actor_loss(training, obs, action, log_prob_old, advantage, hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the discounted returns and the value as estimated by the critic. :param training: The actor TrainState object. :param obs: The obs in the trajectory batch. :param action: The actions in the trajectory batch. :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch. :param advantage: The advantage over the trajectory batch. :param hyperparams: The HyperParameters object used for training. :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\ndef _actor_loss(\n        self,\n        training: TrainState,\n        obs: Float[Array, \"n_rollout batch_size obs_size\"],\n        action: Float[Array, \"n_rollout batch_size\"],\n        log_prob_old: Float[Array, \"n_rollout batch_size\"],\n        advantage: ReturnsType,\n        hyperparams: HyperParameters\n)-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n    \"\"\"\n    Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n    discounted returns and the value as estimated by the critic.\n    :param training: The actor TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param action: The actions in the trajectory batch.\n    :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n    :param advantage: The advantage over the trajectory batch.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._actor_loss_input","title":"<code>_actor_loss_input(update_runner, traj_batch)</code>  <code>abstractmethod</code>","text":"<p>Prepares the input required by the actor loss function. The input is reshaped so that it is split into minibatches. :param update_runner: The runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the actor loss function.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\n@partial(jax.jit, static_argnums=(0,))\ndef _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[ActorLossInputType]:\n    \"\"\"\n    Prepares the input required by the actor loss function. The input is reshaped so that it is split into\n    minibatches.\n    :param update_runner: The runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the actor loss function.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._actor_minibatch_update","title":"<code>_actor_minibatch_update(i_minibatch, minibatch_runner, grad_fn)</code>  <code>staticmethod</code>","text":"<p>Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn. :param i_minibatch: Number of minibatch update. :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence. :param grad_fn: The gradient function of the training loss. :return: Minibatch runner with an updated TrainState.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@staticmethod\ndef _actor_minibatch_update(\n        i_minibatch: int,\n        minibatch_runner: Tuple[TrainState, ActorLossInputType, float],\n        grad_fn: Callable[[Any], ActorLossInputType]\n) -&gt; Annotated[Tuple[TrainState, ActorLossInputType, float], \"n_minibatch\"]:\n    \"\"\"\n    Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be\n    passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n    :param i_minibatch: Number of minibatch update.\n    :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence.\n    :param grad_fn: The gradient function of the training loss.\n    :return: Minibatch runner with an updated TrainState.\n    \"\"\"\n\n    actor_training, actor_loss_input, kl = minibatch_runner\n    *traj_batch, hyperparams = actor_loss_input\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n    grad_input_minibatch = (actor_training, *traj_minibatch, hyperparams)\n    grads, kl = grad_fn(*grad_input_minibatch)\n    actor_training = actor_training.apply_gradients(grads=grads.params)\n    return actor_training, actor_loss_input, kl\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._actor_training_cond","title":"<code>_actor_training_cond(epoch_runner)</code>","text":"<p>Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been met or due to KL divergence early stopping). :param epoch_runner: A tuple containing the following information about the update: - actor_training: TrainState object for actor training - actor_loss_input: Tuple with the inputs required by the actor loss function. - kl: The KL divergence collected during the update (used in checking for early stopping). - epoch: The number of the current training epoch. - kl_threshold: The KL-divergence threshold for early stopping. :return: Whether the lax.while_loop over training epochs finishes.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_training_cond(\n        self,\n        epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, Float[Array, \"1\"]]\n) -&gt; Bool[Array, \"1\"]:\n    \"\"\"\n    Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been\n    met or due to KL divergence early stopping).\n    :param epoch_runner: A tuple containing the following information about the update:\n    - actor_training: TrainState object for actor training\n    - actor_loss_input: Tuple with the inputs required by the actor loss function.\n    - kl: The KL divergence collected during the update (used in checking for early stopping).\n    - epoch: The number of the current training epoch.\n    - kl_threshold: The KL-divergence threshold for early stopping.\n    :return: Whether the lax.while_loop over training epochs finishes.\n    \"\"\"\n\n    _, _, kl, epoch, kl_threshold = epoch_runner\n    return jnp.logical_and(\n        jnp.less(epoch, self.config.actor_epochs),\n        jnp.less_equal(kl, kl_threshold)\n    )\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._actor_update","title":"<code>_actor_update(update_runner, traj_batch)</code>","text":"<p>Prepares the input and performs Gradient Ascent for the actor network. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param traj_batch: The batch of trajectories. :return: The actor training object updated after actor_epochs steps of Gradient Ascent.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_update(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[TrainState, Float[Array, \"1\"]]:\n    \"\"\"\n    Prepares the input and performs Gradient Ascent for the actor network.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param traj_batch: The batch of trajectories.\n    :return: The actor training object updated after actor_epochs steps of Gradient Ascent.\n    \"\"\"\n\n    actor_loss_input = self._actor_loss_input(update_runner, traj_batch)\n\n    start_kl, start_epoch = -jnp.inf, 1\n    actor_epoch_runner = (\n        update_runner.actor_training,\n        actor_loss_input,\n        start_kl,\n        start_epoch,\n        update_runner.hyperparams.kl_threshold\n    )\n    actor_epoch_runner = lax.while_loop(self._actor_training_cond, self._actor_epoch, actor_epoch_runner)\n    actor_training, _, _, _, _ = actor_epoch_runner\n\n    actor_loss, _ = self._actor_loss(\n        actor_training,\n        traj_batch.obs,\n        traj_batch.action,\n        traj_batch.log_prob,\n        traj_batch.advantage,\n        update_runner.hyperparams\n    )\n\n    return actor_training, actor_loss\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._add_advantages","title":"<code>_add_advantages(traj_batch, advantage)</code>","text":"Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _add_advantages(self, traj_batch: Transition, advantage: ReturnsType) -&gt; Transition:\n\n    traj_batch = traj_batch._replace(advantage=advantage)\n\n    return traj_batch\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._add_next_values","title":"<code>_add_next_values(traj_batch, last_obs, critic_training)</code>","text":"Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _add_next_values(\n        self,\n        traj_batch: Transition,\n        last_obs: ObsType,\n        critic_training: TrainState\n) -&gt; Transition:\n\n    last_state_value_vmap = jax.vmap(critic_training.apply_fn, in_axes=(None, 0))\n    last_state_value = last_state_value_vmap(lax.stop_gradient(critic_training.params), last_obs)\n    last_state_value = jnp.expand_dims(last_state_value, axis=1)\n\n    \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n    next_values_t = jnp.concatenate([\n        traj_batch.value.squeeze(),\n        last_state_value\n    ], axis=1)[:, 1:, :]\n\n    traj_batch = traj_batch._replace(next_value=next_values_t)\n\n    return traj_batch\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._advantages","title":"<code>_advantages(traj_batch, gamma, gae_lambda)</code>","text":"<p>Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling step, trajectories do not start at the initial state. :param traj_batch: The batch of trajectories. :param last_next_state_value: The value of the last next state in each trajectory. :param gamma: Discount factor :param gae_lambda: The GAE \u03bb factor. :return: The advantages over the episodes of the trajectory batch.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _advantages(\n        self,\n        traj_batch: Transition,\n        gamma: float,\n        gae_lambda: float\n) -&gt; ReturnsType:\n    \"\"\"\n    Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the\n    trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n    episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n    step, trajectories do not start at the initial state.\n    :param traj_batch: The batch of trajectories.\n    :param last_next_state_value: The value of the last next state in each trajectory.\n    :param gamma: Discount factor\n    :param gae_lambda: The GAE \u03bb factor.\n    :return: The advantages over the episodes of the trajectory batch.\n    \"\"\"\n\n    rewards_t = traj_batch.reward.squeeze()\n    values_t = traj_batch.value.squeeze()\n    terminated_t = jnp.expand_dims(traj_batch.terminated, axis=-1)\n    next_state_values_t = traj_batch.next_value.squeeze()\n    gamma_t = jnp.ones_like(terminated_t) * gamma\n    gae_lambda_t = jnp.ones_like(terminated_t) * gae_lambda\n\n    rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t = jax.tree_util.tree_map(\n        lambda x: jnp.swapaxes(x, 0, 1),\n        (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n    )\n\n    traj_runner = (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n    \"\"\"\n    TODO:\n    Advantage of last step is taken from the critic, in contrast to traditional approaches, where the rollout \n    ends with episode termination and the advantage is zero. Training is still successful and the influence of this\n    implementation choice is negligible.\n    \"\"\"\n    end_advantage = jnp.zeros((self.config.batch_size, self.n_actors))\n    _, advantages = jax.lax.scan(self._trajectory_advantages, end_advantage, traj_runner, reverse=True)\n    advantages = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), advantages)\n\n    return advantages\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._checkpoint","title":"<code>_checkpoint(update_runner, metrics, i_training_step)</code>","text":"<p>Wraps the base checkpointing method in a Python callback. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param metrics: Dictionary of evaluation metrics (return per environment evaluation) :param i_training_step: Training step :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _checkpoint(\n        self,\n        update_runner: Runner,\n        metrics: Dict[str, Float[Array, \"n_agents\"]],\n        i_training_step: int\n) -&gt; None:\n    \"\"\"\n    Wraps the base checkpointing method in a Python callback.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n    :param i_training_step: Training step\n    :return:\n    \"\"\"\n\n    jax.experimental.io_callback(self._checkpoint_base, None, update_runner, metrics, i_training_step)\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._checkpoint_base","title":"<code>_checkpoint_base(update_runner, metrics, i_training_step)</code>","text":"<p>Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following: - The training runner object. - Returns of the evaluation episodes The average return over the evaluated episodes is used as the checkpoint metric. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param metrics: Dictionary of evaluation metrics (return per episode evaluation) :param i_training_step: Training step :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _checkpoint_base(\n        self,\n        update_runner: Runner,\n        metrics: Dict[str, Float[Array, \"1\"]],\n        i_training_step: int\n) -&gt; None:\n    \"\"\"\n    Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following:\n    - The training runner object.\n    - Returns of the evaluation episodes\n    The average return over the evaluated episodes is used as the checkpoint metric.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param metrics: Dictionary of evaluation metrics (return per episode evaluation)\n    :param i_training_step: Training step\n    :return:\n    \"\"\"\n\n    if self.checkpointing:\n\n        ckpt = {\n            \"runner\": update_runner,\n            \"terminated\": metrics[\"terminated\"],\n            \"truncated\": metrics[\"truncated\"],\n            \"final_rewards\": metrics[\"final_rewards\"],\n            \"returns\": metrics[\"returns\"]\n        }\n\n        save_args = orbax_utils.save_args_from_target(ckpt)\n\n        self.checkpoint_manager.save(\n            # Use maximum number of steps reached in previous training. Set to zero by default during agent\n            # initialization if a new training is executed. In case of continuing training, the checkpoint of step\n            # zero replaces the last checkpoint of the previous training. The two checkpoints are the same.\n            i_training_step+self.previous_training_max_step,\n            ckpt,\n            save_kwargs={'save_args': save_args},\n        )\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._create_empty_trainstate","title":"<code>_create_empty_trainstate(network)</code>","text":"<p>Creates an empty TrainState object for restoring checkpoints. :param network: The actor or critic network. :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _create_empty_trainstate(self, network) -&gt; TrainState:\n    \"\"\"\n    Creates an empty TrainState object for restoring checkpoints.\n    :param network: The actor or critic network.\n    :return:\n    \"\"\"\n\n    rng = jax.random.PRNGKey(1)  # Just a dummy PRNGKey for initializing the networks parameters.\n    network, params = self._init_network(rng, network)\n\n    optimizer_params = OptimizerParams()  # Use the default values of the OptimizerParams object.\n    tx = self._init_optimizer(optimizer_params)\n\n    empty_training = TrainState.create(apply_fn=network.apply, params=params, tx=tx)\n\n    return empty_training\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._create_training","title":"<code>_create_training(rng, network, optimizer_params)</code>","text":"<p>Creates a TrainState object for the actor or the critic. :param rng: Random key for initialization. :param network: The actor or critic network. :param optimizer_params: A NamedTuple containing the parametrization of the optimizer. :return: A TrainState object to be used in training the actor and cirtic networks.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _create_training(\n        self,\n        rng: PRNGKeyArray,\n        network: type[flax.linen.Module],\n        optimizer_params: OptimizerParams\n)-&gt; TrainState:\n    \"\"\"\n     Creates a TrainState object for the actor or the critic.\n    :param rng: Random key for initialization.\n    :param network: The actor or critic network.\n    :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n    :return: A TrainState object to be used in training the actor and cirtic networks.\n    \"\"\"\n\n    network, params = self._init_network(rng, network)\n    tx = self._init_optimizer(optimizer_params)\n    return TrainState.create(apply_fn=network.apply, tx=tx, params=params)\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._create_update_runner","title":"<code>_create_update_runner(rng, actor_training, critic_training, hyperparams)</code>","text":"<p>Initializes the update runner as a Runner object. The runner contains batch_size initializations of the environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and one for the critic network, so that trajectory batches are used to train the same parameters. :param rng: Random key for initialization. :param actor_training: The actor TrainState objects used in training. :param critic_training: The critic TrainState objects used in training. :param hyperparams: An instance of HyperParameters for training. :return: An update runner object to be used in trajectory sampling and training.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _create_update_runner(\n        self,\n        rng: PRNGKeyArray,\n        actor_training: TrainState,\n        critic_training: TrainState,\n        hyperparams: HyperParameters\n) -&gt; Runner:\n    \"\"\"\n    Initializes the update runner as a Runner object. The runner contains batch_size initializations of the\n    environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and\n    one for the critic network, so that trajectory batches are used to train the same parameters.\n    :param rng: Random key for initialization.\n    :param actor_training: The actor TrainState objects used in training.\n    :param critic_training: The critic TrainState objects used in training.\n    :param hyperparams: An instance of HyperParameters for training.\n    :return: An update runner object to be used in trajectory sampling and training.\n    \"\"\"\n\n    rng, reset_rng, runner_rng = jax.random.split(rng, 3)\n    reset_rngs = jax.random.split(reset_rng, self.config.batch_size)\n    runner_rngs = jax.random.split(runner_rng, self.config.batch_size)\n\n    _, obs, envstate = jax.vmap(self.env_reset)(reset_rngs)\n\n    update_runner = Runner(\n        actor_training=actor_training,\n        critic_training=critic_training,\n        envstate=envstate,\n        obs=obs,\n        rng=runner_rngs,\n        hyperparams=hyperparams,\n        actor_loss=jnp.zeros(1),\n        critic_loss=jnp.zeros(1),\n    )\n\n    return update_runner\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._critic_epoch","title":"<code>_critic_epoch(i_epoch, epoch_runner)</code>","text":"<p>Performs a Gradient Descent update of the critic. :param: i_epoch: The current training epoch (unused but required by lax.fori_loop). :param epoch_runner: A tuple containing the following information about the update: - critic_training: TrainState object for critic training - critic_loss_input: Tuple with the inputs required by the critic loss function. :return: The updated epoch runner.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_epoch(\n        self,\n        i_epoch: int,\n        epoch_runner: Tuple[TrainState, CriticLossInputType]\n) -&gt; Tuple[TrainState, CriticLossInputType]:\n    \"\"\"\n    Performs a Gradient Descent update of the critic.\n    :param: i_epoch: The current training epoch (unused but required by lax.fori_loop).\n    :param epoch_runner: A tuple containing the following information about the update:\n    - critic_training: TrainState object for critic training\n    - critic_loss_input: Tuple with the inputs required by the critic loss function.\n    :return: The updated epoch runner.\n    \"\"\"\n\n    critic_training, critic_loss_input = epoch_runner\n    minibatch_runner = (critic_training, critic_loss_input)\n    n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n    minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._critic_minibatch_fn, minibatch_runner)\n    critic_training, _ = minibatch_runner\n\n    return critic_training, critic_loss_input\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._critic_loss","title":"<code>_critic_loss(training, obs, targets, hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Calculates the critic loss. :param training: The critic TrainState object. :param obs: The obs in the trajectory batch. :param targets: The returns over the trajectory batch, which act as the targets for training the critic. :param hyperparams: The HyperParameters object used for training. :return: The critic loss.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\n@partial(jax.jit, static_argnums=(0,))\ndef _critic_loss(\n        self,\n        training: TrainState,\n        obs: Float[Array, \"n_rollout batch_size obs_size\"],\n        targets: Float[Array, \"batch_size n_rollout\"],\n        hyperparams: HyperParameters\n) -&gt; float:\n    \"\"\"\n    Calculates the critic loss.\n    :param training: The critic TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param targets: The returns over the trajectory batch, which act as the targets for training the critic.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: The critic loss.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._critic_loss_input","title":"<code>_critic_loss_input(update_runner, traj_batch)</code>  <code>abstractmethod</code>","text":"<p>Prepares the input required by the critic loss function. The input is reshaped so that it is split into minibatches. :param update_runner: The Runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the critic loss function.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\n@partial(jax.jit, static_argnums=(0,))\ndef _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n    \"\"\"\n    Prepares the input required by the critic loss function. The input is reshaped so that it is split into\n    minibatches.\n    :param update_runner: The Runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the critic loss function.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._critic_minibatch_update","title":"<code>_critic_minibatch_update(i_minibatch, minibatch_runner, grad_fn)</code>  <code>staticmethod</code>","text":"<p>Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn. :param i_minibatch: Number of minibatch update. :param minibatch_runner: A tuple containing the TranState object and the loss input arguments. :param grad_fn: The gradient function of the training loss. :return: Minibatch runner with an updated TrainState.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@staticmethod\ndef _critic_minibatch_update(\n        i_minibatch: int,\n        minibatch_runner: Tuple[TrainState, CriticLossInputType],\n        grad_fn: Callable[[Any], CriticLossInputType]\n) -&gt; Tuple[TrainState, CriticLossInputType]:\n    \"\"\"\n    Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be\n    passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n    :param i_minibatch: Number of minibatch update.\n    :param minibatch_runner: A tuple containing the TranState object and the loss input arguments.\n    :param grad_fn: The gradient function of the training loss.\n    :return: Minibatch runner with an updated TrainState.\n    \"\"\"\n\n    critic_training, critic_loss_input = minibatch_runner\n    *traj_batch, hyperparams = critic_loss_input\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n    grad_input_minibatch = (critic_training, *traj_minibatch, hyperparams)\n    grads = grad_fn(*grad_input_minibatch)\n    critic_training = critic_training.apply_gradients(grads=grads.params)\n    return critic_training, critic_loss_input\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._critic_update","title":"<code>_critic_update(update_runner, traj_batch)</code>","text":"<p>Prepares the input and performs Gradient Descent for the critic network. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param traj_batch: The batch of trajectories. :return: The critic training object updated after actor_epochs steps of Gradient Ascent.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_update(self, update_runner: Runner, traj_batch: Transition) -&gt;  Tuple[TrainState, Float[Array, \"1\"]]:\n    \"\"\"\n    Prepares the input and performs Gradient Descent for the critic network.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param traj_batch: The batch of trajectories.\n    :return: The critic training object updated after actor_epochs steps of Gradient Ascent.\n    \"\"\"\n\n    critic_loss_input = self._critic_loss_input(update_runner, traj_batch)\n    critic_epoch_runner = (update_runner.critic_training, critic_loss_input)\n    critic_epoch_runner = lax.fori_loop(0, self.config.critic_epochs, self._critic_epoch, critic_epoch_runner)\n    critic_training, _ = critic_epoch_runner\n\n    critic_targets = critic_loss_input[1].reshape(-1, self.config.rollout_length, self.n_actors)\n    critic_loss = self._critic_loss(critic_training, traj_batch.obs, critic_targets, update_runner.hyperparams)\n\n    return critic_training, critic_loss\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._entropy","title":"<code>_entropy(training, obs)</code>  <code>abstractmethod</code>","text":"Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\ndef _entropy(self, training: TrainState, obs: ObsType)-&gt; Float[Array, \"1\"]:\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._eval_agent","title":"<code>_eval_agent(rng, actor_training, critic_training, n_episodes=1)</code>","text":"<p>Evaluates the agents for n_episodes complete episodes using 'lax.while_loop'. :param rng: A random key used for evaluating the agent. :param actor_training: The actor TrainState object (either mid- or post-training). :param critic_training: The critic TrainState object (either mid- or post-training). :param n_episodes: The update_runner object used during training. :return: The sum of rewards collected over n_episodes episodes.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _eval_agent(\n        self,\n        rng: PRNGKeyArray,\n        actor_training: TrainState,\n        critic_training: TrainState,\n        n_episodes: int = 1\n) -&gt; Dict[str, Float[Array, \"n_agents\"] | Bool[Array, \"1\"]]:\n    \"\"\"\n    Evaluates the agents for n_episodes complete episodes using 'lax.while_loop'.\n    :param rng: A random key used for evaluating the agent.\n    :param actor_training: The actor TrainState object (either mid- or post-training).\n    :param critic_training: The critic TrainState object (either mid- or post-training).\n    :param n_episodes: The update_runner object used during training.\n    :return: The sum of rewards collected over n_episodes episodes.\n    \"\"\"\n\n    rng_eval = jax.random.split(rng, n_episodes)\n    rng, obs, envstate = jax.vmap(self.env_reset)(rng_eval)\n\n    eval_runner = (\n        envstate,\n        obs,\n        actor_training,\n        jnp.zeros(1, dtype=jnp.bool).squeeze(),\n        jnp.zeros(1, dtype=jnp.bool).squeeze(),\n        jnp.zeros(self.n_actors),\n        jnp.zeros(self.n_actors),\n        rng,\n    )\n\n    eval_runners = jax.vmap(\n        lambda s, t, u, v, w, x, y, z: (s, t, u, v, w, x, y, z),\n        in_axes=(0, 0, None, None, None, None, None, 0)\n    )(*eval_runner)\n\n    eval_runner = jax.vmap(lambda x: lax.while_loop(self._eval_cond, self._eval_body, x))(eval_runners)\n    _, _, _, terminated, truncated, final_rewards, returns, _ = eval_runner\n\n    return self._eval_metrics(terminated, truncated, final_rewards, returns)\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._eval_body","title":"<code>_eval_body(eval_runner)</code>","text":"<p>A step in the episode to be used with 'lax.while_loop' for evaluation of the agent in a complete episode. :param eval_runner: A tuple containing information about the environment state, the actor and critic training states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards over the episode and a random key. :return: The updated eval_runner tuple.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _eval_body(self, eval_runner: EvalRunnerType) -&gt; EvalRunnerType:\n    \"\"\"\n    A step in the episode to be used with 'lax.while_loop' for evaluation of the agent in a complete episode.\n    :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n    states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n    over the episode and a random key.\n    :return: The updated eval_runner tuple.\n    \"\"\"\n\n    envstate, obs, actor_training, terminated, truncated, reward, returns, rng = eval_runner\n\n    actions = self.policy(actor_training, obs)\n\n    rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, actions)\n\n    terminated = info[\"terminated\"]\n    truncated = info[\"truncated\"]\n\n    returns += reward\n\n    eval_runner = (next_envstate, next_obs, actor_training, terminated, truncated, reward, returns, rng)\n\n    return eval_runner\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._eval_cond","title":"<code>_eval_cond(eval_runner)</code>","text":"<p>Checks whether the episode is terminated, meaning that the 'lax.while_loop' can stop. :param eval_runner: A tuple containing information about the environment state, the actor and critic training states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards over the episode and a random key. :return: Whether the episode is terminated, which means that the while loop must stop.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _eval_cond(self, eval_runner: EvalRunnerType) -&gt; Bool[Array, \"1\"]:\n    \"\"\"\n    Checks whether the episode is terminated, meaning that the 'lax.while_loop' can stop.\n    :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n    states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n    over the episode and a random key.\n    :return: Whether the episode is terminated, which means that the while loop must stop.\n    \"\"\"\n\n    _, _, _, terminated, truncated, _, _, _ = eval_runner\n    return jnp.logical_and(jnp.logical_not(terminated), jnp.logical_not(truncated))\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._eval_metrics","title":"<code>_eval_metrics(terminated, truncated, final_rewards, returns)</code>","text":"<p>Evaluate the metrics. :param terminated: Whether the episode finished by termination. :param truncated: Whether the episode finished by truncation. :param final_rewards: The rewards collected in the final step of the episode. :param returns: The sum of rewards collected during the episode. :return: Dictionary combining the input arguments and the case-specific special metrics.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _eval_metrics(\n        self,\n        terminated: Bool[Array, \"1\"],\n        truncated: Bool[Array, \"1\"],\n        final_rewards: Float[Array, \"1\"],\n        returns: Float[Array, \"1\"]\n) -&gt; Dict[str, Float[Array, \"1\"] | Bool[Array, \"1\"]]:\n    \"\"\"\n    Evaluate the metrics.\n    :param terminated: Whether the episode finished by termination.\n    :param truncated: Whether the episode finished by truncation.\n    :param final_rewards: The rewards collected in the final step of the episode.\n    :param returns: The sum of rewards collected during the episode.\n    :return: Dictionary combining the input arguments and the case-specific special metrics.\n    \"\"\"\n    metrics = {\n        \"terminated\": terminated,\n        \"truncated\": truncated,\n        \"final_rewards\": final_rewards,\n        \"returns\": returns\n    }\n\n    return metrics\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._generate_metrics","title":"<code>_generate_metrics(runner, update_step)</code>","text":"<p>Generates metrics for on-policy learning. The agent performance during training is evaluated by running n_evals episodes (until termination). If the user selects not to generate metrics (leading to faster training), an empty dictionary is returned. :param runner: The update runner object, containing information about the current status of the actor's/critic's training, the state of the environment and training hyperparameters. :param update_step: The number of the update step. :return: A dictionary of the sum of rewards collected over 'n_evals' episodes, or empty dictionary.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _generate_metrics(self, runner: Runner, update_step: int) -&gt; Dict[str, Float[Array, \"n_agents\"]]:\n    \"\"\"\n    Generates metrics for on-policy learning. The agent performance during training is evaluated by running\n    n_evals episodes (until termination). If the user selects not to generate metrics (leading to faster training),\n    an empty dictionary is returned.\n    :param runner: The update runner object, containing information about the current status of the actor's/critic's\n    training, the state of the environment and training hyperparameters.\n    :param update_step: The number of the update step.\n    :return: A dictionary of the sum of rewards collected over 'n_evals' episodes, or empty dictionary.\n    \"\"\"\n\n    metric = {}\n    if self.eval_during_training:\n        metric = self._eval_agent(\n            self.config.eval_rng,\n            runner.actor_training,\n            runner.critic_training,\n            self.config.n_evals\n        )\n\n    metric.update({\n        \"actor_loss\": runner.actor_loss,\n        \"critic_loss\": runner.critic_loss\n    })\n\n    return metric\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._init_checkpointer","title":"<code>_init_checkpointer()</code>","text":"<p>Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If so, sets the checkpoint manager using orbax. :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _init_checkpointer(self) -&gt; None:\n    \"\"\"\n    Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If\n    so, sets the checkpoint manager using orbax.\n    :return:\n    \"\"\"\n\n    self.checkpointing = self.config.checkpoint_dir is not None\n\n    if self.checkpointing:\n\n        if not self.config.restore_agent:\n\n            dir_exists = os.path.exists(self.config.checkpoint_dir)\n            if not dir_exists:\n                os.makedirs(self.config.checkpoint_dir)\n\n            dir_files = [\n                file for file in os.listdir(self.config.checkpoint_dir)\n                if os.path.isdir(os.path.join(self.config.checkpoint_dir, file))\n            ]\n            if len(dir_files) &gt; 0:\n                for file in dir_files:\n                    file_path = os.path.join(self.config.checkpoint_dir, file)\n                    shutil.rmtree(file_path)\n\n            # Log training configuration\n            with open(os.path.join(self.config.checkpoint_dir, 'training_configuration.txt'), \"w\") as f:\n                f.write(self.__str__())\n\n        orbax_checkpointer = orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler())\n\n        options = orbax.checkpoint.CheckpointManagerOptions(\n            create=True,\n            step_prefix='trainingstep',\n        )\n\n        self.checkpoint_manager = orbax.checkpoint.CheckpointManager(\n            self.config.checkpoint_dir,\n            orbax_checkpointer,\n            options\n        )\n\n    else:\n\n        self.checkpoint_manager = None\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._init_env","title":"<code>_init_env(env, env_params)</code>","text":"<p>Environment initialization. :param env: A gymnax or custom environment that inherits from the basic gymnax class. :param env_params: A dataclass containing the parametrization of the environment. :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _init_env(self, env: Environment, env_params: EnvParams) -&gt; None:\n    \"\"\"\n    Environment initialization.\n    :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n    :param env_params: A dataclass containing the parametrization of the environment.\n    :return:\n    \"\"\"\n\n    env = TruncationWrapper(env, self.config.max_episode_steps)\n    # env = FlattenObservationWrapper(env)\n    # self.env = LogWrapper(env)\n    self.env = env\n    self.env_params = env_params\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._init_network","title":"<code>_init_network(rng, network)</code>","text":"<p>Initialization of the actor or critic network. :param rng: Random key for initialization. :param network: The actor or critic network. :return: A random key after splitting the input and the initial parameters of the policy network.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _init_network(\n        self,\n        rng: PRNGKeyArray,\n        network: flax.linen.Module\n) -&gt; Tuple[flax.linen.Module, FrozenDict]:\n    \"\"\"\n    Initialization of the actor or critic network.\n    :param rng: Random key for initialization.\n    :param network: The actor or critic network.\n    :return: A random key after splitting the input and the initial parameters of the policy network.\n    \"\"\"\n\n    network = network(self.config)\n\n    rng, *_rng = jax.random.split(rng, 3)\n    dummy_reset_rng, network_init_rng = _rng\n\n    _, dummy_obs, _ = self.env_reset(dummy_reset_rng)\n    init_x = jnp.zeros((1, dummy_obs.size))\n\n    params = network.init(network_init_rng, init_x)\n\n    return network, params\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._init_optimizer","title":"<code>_init_optimizer(optimizer_params)</code>","text":"<p>Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to initialize the appropriate optimizer. In this way, the optimizer can be initialized within the \"train\" method, and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary. :param optimizer_params: A NamedTuple containing the parametrization of the optimizer. :return: An optimizer in optax.chain.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _init_optimizer(self, optimizer_params: OptimizerParams) -&gt; optax.chain:\n    \"\"\"\n    Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to\n    initialize the appropriate optimizer. In this way, the optimizer can be initialized within the \"train\" method,\n    and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary.\n    :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n    :return: An optimizer in optax.chain.\n    \"\"\"\n\n    optimizer_params_dict = optimizer_params._asdict()  # Transform from NamedTuple to dict\n    optimizer_params_dict.pop('grad_clip', None)  # Remove 'grad_clip', since it is not part of the optimizer args.\n\n\n    \"\"\"\n    Get dictionary of optimizer parameters to pass in optimizer. The procedure preserves parameters that:\n        - are given in the OptimizerParams NamedTuple and are requested as args by the optimizer\n        - are requested as args by the optimizer and are given in the OptimizerParams NamedTuple\n    \"\"\"\n\n    optimizer_arg_names = self.config.optimizer.__code__.co_varnames  # List names of args of optimizer.\n\n    # Keep only the optimizer arg names that are also part of the OptimizerParams (dict from NamedTuple)\n    optimizer_arg_names = [\n        arg_name for arg_name in optimizer_arg_names if arg_name in list(optimizer_params_dict.keys())\n    ]\n    if len(optimizer_arg_names) == 0:\n        raise Exception(\n            \"The defined optimizer parameters do not include relevant arguments for this optimizer.\"\n            \"The optimizer has not been implemented yet. Define your own OptimizerParams object.\"\n        )\n\n    # Keep only the optimizer params that are arg names for the specific optimizer\n    optimizer_params_dict = {arg_name: optimizer_params_dict[arg_name] for arg_name in optimizer_arg_names}\n\n    # No need to scale by -1.0. 'TrainState.apply_gradients' is used for training, which subtracts the update.\n    tx = optax.chain(\n        optax.clip_by_global_norm(optimizer_params.grad_clip),\n        self.config.optimizer(**optimizer_params_dict)\n    )\n\n    return tx\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._log_prob","title":"<code>_log_prob(training, params, obs, action)</code>  <code>abstractmethod</code>","text":"Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\ndef _log_prob(\n        self,\n        training: TrainState,\n        params: FrozenDict,\n        obs: ObsType,\n        action: ActionType\n) -&gt; Float[Array, \"n_actors\"]:\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._make_rollout_runners","title":"<code>_make_rollout_runners(update_runner)</code>","text":"<p>Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner object and broadcasting the TrainState object for the critic and the network in the update_runner object to the same dimension. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :return: Tuple with step runners to be used in rollout.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _make_rollout_runners(self, update_runner: Runner) -&gt; Tuple[StepRunnerType, ...]:\n    \"\"\"\n    Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner\n    object and broadcasting the TrainState object for the critic and the network in the update_runner object to the\n    same dimension.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :return: Tuple with step runners to be used in rollout.\n    \"\"\"\n\n    rollout_runner = (\n        update_runner.envstate,\n        update_runner.obs,\n        update_runner.actor_training,\n        update_runner.critic_training,\n        update_runner.rng,\n    )\n    rollout_runners = jax.vmap(\n        lambda v, w, x, y, z: (v, w, x, y, z), in_axes=(0, 0, None, None, 0)\n    )(*rollout_runner)\n    return rollout_runners\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._make_transition","title":"<code>_make_transition(obs, actions, value, log_prob, reward, next_obs, terminated)</code>","text":"<p>Creates a transition object based on the input and output of an episode step. :param obs: The current state of the episode step in array format. :param actions: The action selected per actor. :param value: The value of the state per critic. :param log_prob: The actor log-probability of the selected action. :param reward: The collected reward after executing the action per actor. :param next_obs: The next state of the episode step in array format. :param terminated: Episode termination. :return: A transition object storing information about the state before and after executing the episode step,          the executed action, the collected reward, episode termination and optional additional information.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _make_transition(\n        self,\n        obs: ObsType,\n        actions: ActionType,\n        value: Float[Array, \"n_actors\"],\n        log_prob: Float[Array, \"n_actors\"],\n        reward: Float[Array, \"n_actors\"],\n        next_obs: ObsType,\n        terminated: Bool[Array, \"1\"],\n        ) -&gt; Transition:\n    \"\"\"\n    Creates a transition object based on the input and output of an episode step.\n    :param obs: The current state of the episode step in array format.\n    :param actions: The action selected per actor.\n    :param value: The value of the state per critic.\n    :param log_prob: The actor log-probability of the selected action.\n    :param reward: The collected reward after executing the action per actor.\n    :param next_obs: The next state of the episode step in array format.\n    :param terminated: Episode termination.\n    :return: A transition object storing information about the state before and after executing the episode step,\n             the executed action, the collected reward, episode termination and optional additional information.\n    \"\"\"\n\n    transition = Transition(obs.squeeze(), actions, value, log_prob, reward, next_obs, terminated)\n    transition = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), transition)\n\n    return transition\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._pp","title":"<code>_pp()</code>","text":"<p>Post-processes the training results, which includes:     - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored). :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def _pp(self) -&gt; None:\n    \"\"\"\n    Post-processes the training results, which includes:\n        - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored).\n    :return:\n    \"\"\"\n\n    self.actor_training = self.training_runner.actor_training\n    self.critic_training = self.training_runner.critic_training\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._process_trajectory","title":"<code>_process_trajectory(update_runner, traj_batch, last_obs)</code>","text":"<p>Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not guaranteed to end with termination, the value is estimated using the critic network. This assumption has been shown to have no influence by the end of training. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param traj_batch: The batch of trajectories, as collected by in rollout. :param last_state: The state at the end of every trajectory in the batch. :return: A batch of trajectories that includes an estimate of values and advantages.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _process_trajectory(self, update_runner: Runner, traj_batch: Transition, last_obs: ObsType) -&gt; Transition:\n    \"\"\"\n    Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not\n    guaranteed to end with termination, the value is estimated using the critic network. This assumption has been\n    shown to have no influence by the end of training.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param traj_batch: The batch of trajectories, as collected by in rollout.\n    :param last_state: The state at the end of every trajectory in the batch.\n    :return: A batch of trajectories that includes an estimate of values and advantages.\n    \"\"\"\n\n    traj_batch = jax.tree_util.tree_map(lambda x: x.squeeze(), traj_batch)\n    traj_batch = self._add_next_values(traj_batch, last_obs, update_runner.critic_training)\n\n    advantages = self._advantages(traj_batch, update_runner.hyperparams.gamma, update_runner.hyperparams.gae_lambda)\n    traj_batch = self._add_advantages(traj_batch, advantages)\n\n    return traj_batch\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._returns","title":"<code>_returns(traj_batch, last_next_state_value, gamma, gae_lambda)</code>","text":"<p>Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling step, trajectories do not start at the initial state. :param traj_batch: The batch of trajectories. :param last_next_state_value: The value of the last next state in each trajectory. :param gamma: Discount factor :param gae_lambda: The GAE \u03bb factor. :return: The returns over the episodes of the trajectory batch.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _returns(\n        self,\n        traj_batch: Transition,\n        last_next_state_value: Float[Array, \"batch_size\"],\n        gamma: float,\n        gae_lambda: float\n) -&gt; ReturnsType:\n    \"\"\"\n    Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the\n    trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n    episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n    step, trajectories do not start at the initial state.\n    :param traj_batch: The batch of trajectories.\n    :param last_next_state_value: The value of the last next state in each trajectory.\n    :param gamma: Discount factor\n    :param gae_lambda: The GAE \u03bb factor.\n    :return: The returns over the episodes of the trajectory batch.\n    \"\"\"\n\n    rewards_t = traj_batch.reward.squeeze()\n    terminated_t = 1.0 - traj_batch.terminated.astype(jnp.float32).squeeze()\n    discounts_t = (terminated_t * gamma).astype(jnp.float32)\n\n    \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n    next_state_values_t = jnp.concatenate(\n        [traj_batch.value.squeeze(), last_next_state_value[..., jnp.newaxis]],\n        axis=-1)[:, 1:]\n\n    rewards_t, discounts_t, next_state_values_t = jax.tree_util.tree_map(\n        lambda x: jnp.swapaxes(x, 0, 1), (rewards_t, discounts_t, next_state_values_t)\n    )\n\n    gae_lambda = jnp.ones_like(discounts_t) * gae_lambda\n\n    traj_runner = (rewards_t, discounts_t, next_state_values_t, gae_lambda)\n    end_value = jnp.take(next_state_values_t, -1, axis=0)  # Start from end of trajectory and work in reverse.\n    _, returns = lax.scan(self._trajectory_returns, end_value, traj_runner, reverse=True)\n\n    returns = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), returns)\n\n    return returns\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._rollout","title":"<code>_rollout(step_runner, i_step)</code>","text":"<p>Evaluation of trajectory rollout. In each step the agent: - evaluates policy and value - selects action - performs environment step - creates step transition :param step_runner: A tuple containing information on the environment state, the actor and critic training (parameters and networks) and a random key. :param i_step: Unused, required for lax.scan. :return: The updated step_runner tuple and the rollout step transition.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _rollout(self, step_runner: StepRunnerType, i_step: int) -&gt; Tuple[StepRunnerType, Transition]:\n    \"\"\"\n    Evaluation of trajectory rollout. In each step the agent:\n    - evaluates policy and value\n    - selects action\n    - performs environment step\n    - creates step transition\n    :param step_runner: A tuple containing information on the environment state, the actor and critic training\n    (parameters and networks) and a random key.\n    :param i_step: Unused, required for lax.scan.\n    :return: The updated step_runner tuple and the rollout step transition.\n    \"\"\"\n\n    envstate, obs, actor_training, critic_training, rng = step_runner\n\n    rng, rng_action = jax.random.split(rng)\n    actions = self._sample_actions(rng_action, actor_training, obs)\n\n    values = critic_training.apply_fn(lax.stop_gradient(critic_training.params), obs)\n\n    log_prob = self._log_prob(actor_training, lax.stop_gradient(actor_training.params), obs, actions)\n\n    rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, actions)\n\n    step_runner = (next_envstate, next_obs, actor_training, critic_training, rng)\n\n    terminated = info[\"terminated\"]\n\n    transition = self._make_transition(\n        obs=obs,\n        actions=actions,\n        value=values,\n        log_prob=log_prob,\n        reward=reward,\n        next_obs=next_obs,\n        terminated=terminated,\n    )\n\n    return step_runner, transition\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._sample_actions","title":"<code>_sample_actions(rng, training, obs)</code>  <code>abstractmethod</code>","text":"Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\ndef _sample_actions(\n        self,\n        rng: PRNGKeyArray,\n        training: TrainState,\n        obs: ObsType\n) -&gt; ActionType:\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._training_step","title":"<code>_training_step(update_runner, i_training_batch)</code>","text":"<p>Performs trainings steps to update the agent per training batch. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param i_training_batch: Training batch loop counter. :return: Tuple with updated runner and dictionary of metrics.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _training_step(\n        self,\n        update_runner: Runner,\n        i_training_batch: int\n) -&gt; Tuple[Runner, Dict[str, Float[Array, \"n_agents\"]]]:\n    \"\"\"\n    Performs trainings steps to update the agent per training batch.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param i_training_batch: Training batch loop counter.\n    :return: Tuple with updated runner and dictionary of metrics.\n    \"\"\"\n\n    n_training_steps = self.config.n_steps - self.config.n_steps // self.config.eval_frequency * i_training_batch\n    n_training_steps = jnp.clip(n_training_steps, 1, self.config.eval_frequency)\n\n    update_runner = lax.fori_loop(0, n_training_steps, self._update_step, update_runner)\n\n    if self.eval_during_training:\n        metrics = self._generate_metrics(runner=update_runner, update_step=i_training_batch)\n        i_training_step = self.config.eval_frequency * (i_training_batch + 1)\n        i_training_step = jnp.minimum(i_training_step, self.config.n_steps)\n        if self.checkpointing:\n            self._checkpoint(update_runner, metrics, i_training_step)\n    else:\n        metrics = {}\n\n    return update_runner, metrics\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._trajectory_advantages","title":"<code>_trajectory_advantages(value, traj)</code>  <code>abstractmethod</code>","text":"<p>Calculates the advantages per episode step over a batch of trajectories. :param value: The values of the steps in the trajectory according to the critic (including the one of the last  state). :param traj: The trajectory batch. :return: An array of returns.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\ndef _trajectory_advantages(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the advantages per episode step over a batch of trajectories.\n    :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n     state).\n    :param traj: The trajectory batch.\n    :return: An array of returns.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._trajectory_returns","title":"<code>_trajectory_returns(value, traj)</code>  <code>abstractmethod</code>","text":"<p>Calculates the returns per episode step over a batch of trajectories. :param value: The values of the steps in the trajectory according to the critic (including the one of the last  state). :param traj: The trajectory batch. :return: A tuple of returns.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\ndef _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the returns per episode step over a batch of trajectories.\n    :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n     state).\n    :param traj: The trajectory batch.\n    :return: A tuple of returns.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase._update_step","title":"<code>_update_step(i_update_step, update_runner)</code>","text":"<p>An update step of the actor and critic networks. This entails: - performing rollout for sampling a batch of trajectories. - assessing the value of the last state per trajectory using the critic. - evaluating the advantage per trajectory. - updating the actor and critic network parameters via the respective loss functions. - generating in-training performance metrics. In this approach, the update_runner already has a batch of environments initialized. The environments are not initialized in the beginning of every update step, which means that trajectories to not necessarily start from an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be truncated in trajectory sampling). :param i_update_step: Unused, required for progressbar. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :return: The updated runner</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _update_step(self, i_update_step: int, update_runner: Runner) -&gt; Runner:\n    \"\"\"\n    An update step of the actor and critic networks. This entails:\n    - performing rollout for sampling a batch of trajectories.\n    - assessing the value of the last state per trajectory using the critic.\n    - evaluating the advantage per trajectory.\n    - updating the actor and critic network parameters via the respective loss functions.\n    - generating in-training performance metrics.\n    In this approach, the update_runner already has a batch of environments initialized. The environments are not\n    initialized in the beginning of every update step, which means that trajectories to not necessarily start from\n    an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan\n    for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be\n    truncated in trajectory sampling).\n    :param i_update_step: Unused, required for progressbar.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :return: The updated runner\n    \"\"\"\n\n    rollout_runners = self._make_rollout_runners(update_runner)\n    scan_rollout_fn = lambda x: lax.scan(self._rollout, x, None, self.config.rollout_length)\n    rollout_runners, traj_batch = jax.vmap(scan_rollout_fn)(rollout_runners)\n    last_envstate, last_obs, _, _, rng = rollout_runners\n    traj_batch = self._process_trajectory(update_runner, traj_batch, last_obs)\n\n    actor_training, actor_loss = self._actor_update(update_runner, traj_batch)\n    critic_training, critic_loss = self._critic_update(update_runner, traj_batch)\n\n    \"\"\"Update runner as a dataclass.\"\"\"\n    update_runner = update_runner.replace(\n        envstate=last_envstate,\n        obs=last_obs,\n        actor_training=actor_training,\n        critic_training=critic_training,\n        rng=rng,\n        actor_loss=jnp.expand_dims(actor_loss, axis=-1),\n        critic_loss=jnp.expand_dims(critic_loss, axis=-1)\n    )\n\n    return update_runner\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.collect_training","title":"<code>collect_training(runner=None, metrics=None, previous_training_max_step=0)</code>","text":"<p>Collects training or restored checkpoint of output (the final state of the runner after training and the collected metrics). :param runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. This is at the state reached at the end of training. :param metrics: Dictionary of evaluation metrics (return per environment evaluation) :param previous_training_max_step: Maximum step reached during training. :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def collect_training(\n        self,\n        runner: Optional[Runner] = None,\n        metrics: Optional[Dict[str, Float[Array, \"1\"]]] = None,\n        previous_training_max_step: int = 0\n) -&gt; None:\n    \"\"\"\n    Collects training or restored checkpoint of output (the final state of the runner after training and the\n    collected metrics).\n    :param runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters. This is at the state reached at\n    the end of training.\n    :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n    :param previous_training_max_step: Maximum step reached during training.\n    :return:\n    \"\"\"\n\n    self.agent_trained = True\n    self.previous_training_max_step = previous_training_max_step\n    self.training_runner = runner\n    self.training_metrics = metrics\n    n_evals = list(metrics.values())[0].shape[0]\n    self.eval_steps_in_training = jnp.arange(n_evals) * self.config.eval_frequency\n    self._pp()\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.env_reset","title":"<code>env_reset(rng)</code>","text":"<p>Environment reset. :param rng: Random key for initialization. :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef env_reset(self, rng: PRNGKeyArray) -&gt; Tuple[PRNGKeyArray, ObsType, LogEnvState | EnvState | TruncationEnvState]:\n    \"\"\"\n    Environment reset.\n    :param rng: Random key for initialization.\n    :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.\n    \"\"\"\n\n    rng, reset_rng = jax.random.split(rng)\n    obs, envstate = self.env.reset(reset_rng, self.env_params)\n\n    return rng, obs, envstate\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.env_step","title":"<code>env_step(rng, envstate, actions)</code>","text":"<p>Environment step. :param rng: Random key for initialization. :param envstate: The environment state in LogEnvState format. :param actions: The actions selected per actor. :return: A tuple of: a random key after splitting the input, the next state in array and LogEnvState formats,          the collected reward after executing the action, episode termination and a dictionary of optional          additional information.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef env_step(\n        self,\n        rng: PRNGKeyArray,\n        envstate: LogEnvState | EnvState | TruncationEnvState,\n        actions: ActionType\n)-&gt; Tuple[\n    PRNGKeyArray,\n    ObsType,\n    LogEnvState | EnvState | TruncationEnvState,\n    Float[Array, \"1\"],\n    Bool[Array, \"1\"],\n    Dict[str, float | bool]\n]:\n    \"\"\"\n    Environment step.\n    :param rng: Random key for initialization.\n    :param envstate: The environment state in LogEnvState format.\n    :param actions: The actions selected per actor.\n    :return: A tuple of: a random key after splitting the input, the next state in array and LogEnvState formats,\n             the collected reward after executing the action, episode termination and a dictionary of optional\n             additional information.\n    \"\"\"\n\n    rng, step_rng = jax.random.split(rng)\n    next_obs, next_envstate, reward, done, info = \\\n        self.env.step(step_rng, envstate, actions.squeeze(), self.env_params)\n\n    return rng, next_obs, next_envstate, reward, done, info\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.eval","title":"<code>eval(rng, n_evals=32)</code>","text":"<p>Evaluates the trained agent's performance post-training using the trained agent's actor and critic. :param rng: Random key for evaluation. :param n_evals: Number of steps in agent evaluation. :return: Dictionary of evaluation metrics.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def eval(self, rng: PRNGKeyArray, n_evals: int = 32) -&gt; Float[Array, \"n_evals\"]:\n    \"\"\"\n    Evaluates the trained agent's performance post-training using the trained agent's actor and critic.\n    :param rng: Random key for evaluation.\n    :param n_evals: Number of steps in agent evaluation.\n    :return: Dictionary of evaluation metrics.\n    \"\"\"\n\n    eval_metrics = self._eval_agent(rng, self.actor_training, self.critic_training, n_evals)\n\n    return eval_metrics\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs training hyperparameters in a text file. To be used outside training. :param hyperparams: An instance of HyperParameters for training. :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def log_hyperparams(self, hyperparams: HyperParameters) -&gt; None:\n    \"\"\"\n    Logs training hyperparameters in a text file. To be used outside training.\n    :param hyperparams: An instance of HyperParameters for training.\n    :return:\n    \"\"\"\n\n    output_lst = [field + ': ' + str(getattr(hyperparams, field)) for field in hyperparams._fields]\n    output_lst = ['Hyperparameters:'] + output_lst\n    output_lst = '\\n'.join(output_lst)\n\n    if self.checkpointing:\n        with open(os.path.join(self.config.checkpoint_dir, 'hyperparameters.txt'), \"w\") as f:\n            f.write(output_lst)\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.policy","title":"<code>policy(training, obs)</code>  <code>abstractmethod</code>","text":"<p>Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state. :param obs: The current obs of the episode step in array format. :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@abstractmethod\ndef policy(self, training: TrainState, obs: ObsType) -&gt; ActionType:\n    \"\"\"\n    Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state.\n    :param obs: The current obs of the episode step in array format.\n    :return:\n    \"\"\"\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.restore","title":"<code>restore(mode='best', best_fn=None)</code>","text":"<p>Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then, post-processes the restored checkpoint. :param mode: Determines whether the best performing or latest checkpoint should be restored. :param best_fn: The function that should be used in determining the best performing checkpoint. :return:</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def restore(\n        self,\n        mode: str = \"best\",\n        best_fn: Optional[Callable[[Dict[str, Float[Array, \"1\"]]], [int]]] = None\n) -&gt; None:\n    \"\"\"\n    Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then,\n    post-processes the restored checkpoint.\n    :param mode: Determines whether the best performing or latest checkpoint should be restored.\n    :param best_fn: The function that should be used in determining the best performing checkpoint.\n    :return:\n    \"\"\"\n\n    steps = self.checkpoint_manager.all_steps()\n\n    # Log keys in checkpoints\n    ckpt = self.checkpoint_manager.restore(steps[0])\n    ckpt_keys = [key for key in list(ckpt.keys()) if key != \"runner\"]\n\n    # Collect history of metrics in training. Useful for continuing training.\n    metrics = {key: [None] * len(steps) for key in ckpt_keys}\n    for i, step in enumerate(steps):\n        ckpt = self.checkpoint_manager.restore(step)\n        for key in ckpt_keys:\n            metrics[key][i] = ckpt[key][jnp.newaxis, :]\n    metrics = {key: jnp.concatenate(val, axis=0) for (key, val) in metrics.items()}\n\n    if mode == \"best\":\n        if best_fn is not None:\n            step = steps[best_fn(metrics)]\n        else:\n            raise Exception(\"Function for determining best checkpoint not provided\")\n    elif mode == \"last\":\n        step = self.checkpoint_manager.latest_step()\n    else:\n        raise Exception(\"Unknown method for selecting a checkpoint.\")\n\n    \"\"\"\n    Create an empty target for restoring the checkpoint.\n    Some of the arguments come from restoring one of the ckpts.\n    \"\"\"\n\n    empty_actor_training = self._create_empty_trainstate(self.config.actor_network)\n    empty_critic_training = self._create_empty_trainstate(self.config.critic_network)\n\n    # Get some state and envstate for restoring the checkpoint.\n    _, obs, envstate = self.env_reset(jax.random.PRNGKey(1))\n\n    empty_runner = Runner(\n        actor_training=empty_actor_training,\n        critic_training=empty_critic_training,\n        envstate=envstate,\n        obs=obs,\n        rng=jax.random.split(jax.random.PRNGKey(1), self.config.batch_size),  # Just a dummy PRNGKey for initializing the networks parameters.\n        # Hyperparams can be loaded as a dict. If training continues, new hyperparams will be provided.\n        hyperparams=ckpt[\"runner\"][\"hyperparams\"]\n    )\n\n    target_ckpt = {\n        \"runner\": empty_runner,\n        \"terminated\": jnp.zeros(metrics[\"terminated\"].shape[1]),\n        \"truncated\": jnp.zeros(metrics[\"truncated\"].shape[1]),\n        \"final_rewards\": jnp.zeros(metrics[\"final_rewards\"].shape[1]),\n        \"returns\": jnp.zeros(metrics[\"returns\"].shape[1]),\n    }\n\n    ckpt = self.checkpoint_manager.restore(step, items=target_ckpt)\n\n    self.collect_training(ckpt[\"runner\"], metrics, previous_training_max_step=max(steps))\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.summarize","title":"<code>summarize(metrics)</code>","text":"<p>Summarizes collection of per-episode metrics. :param metrics: Metric per episode. :return: Summary of metric per episode.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>def summarize(\n        self,\n        metrics: Annotated[NDArray[np.float32], \"size_metrics\"] | Float[Array, \"size_metrics\"]\n) -&gt; MetricStats:\n    \"\"\"\n    Summarizes collection of per-episode metrics.\n    :param metrics: Metric per episode.\n    :return: Summary of metric per episode.\n    \"\"\"\n\n    return MetricStats(\n        episode_metric=metrics,\n        mean=metrics.mean(axis=-1),\n        var=metrics.var(axis=-1),\n        std=metrics.std(axis=-1),\n        min=metrics.min(axis=-1),\n        max=metrics.max(axis=-1),\n        median=jnp.median(metrics, axis=-1),\n        has_nans=jnp.any(jnp.isnan(metrics), axis=-1)\n    )\n</code></pre>"},{"location":"reference/ippobase/#jaxagents.ippo.IPPOBase.train","title":"<code>train(rng, hyperparams)</code>","text":"<p>Trains the agents. A jax_tqdm progressbar has been added in the lax.scan loop. :param rng: Random key for initialization. This is the original key for training. :param hyperparams: An instance of HyperParameters for training. :return: The final state of the step runner after training and the training metrics accumulated over all          training batches and steps.</p> Source code in <code>jaxagents\\ippo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef train(\n        self,\n        rng: PRNGKeyArray,\n        hyperparams: HyperParameters\n) -&gt; Tuple[Runner, Dict[str, Float[Array, \"n_agents\"]]]:\n    \"\"\"\n    Trains the agents. A jax_tqdm progressbar has been added in the lax.scan loop.\n    :param rng: Random key for initialization. This is the original key for training.\n    :param hyperparams: An instance of HyperParameters for training.\n    :return: The final state of the step runner after training and the training metrics accumulated over all\n             training batches and steps.\n    \"\"\"\n\n    rng, *_rng = jax.random.split(rng, 4)\n    actor_init_rng, critic_init_rng, runner_rng = _rng\n\n    actor_training = self._create_training(\n        actor_init_rng, self.config.actor_network, hyperparams.actor_optimizer_params\n    )\n    critic_training = self._create_training(\n        critic_init_rng, self.config.critic_network, hyperparams.critic_optimizer_params\n    )\n\n    update_runner = self._create_update_runner(runner_rng, actor_training, critic_training, hyperparams)\n\n    # Checkpoint initial state\n    if self.eval_during_training:\n        metrics_start = self._generate_metrics(runner=update_runner, update_step=0)\n        if self.checkpointing:\n            self._checkpoint(update_runner, metrics_start, self.previous_training_max_step)\n\n    # Initialize agent updating functions, which can be avoided to be done within the training loops.\n    actor_grad_fn = jax.grad(self._actor_loss, has_aux=True, allow_int=True)\n    self._actor_minibatch_fn = lambda x, y: self._actor_minibatch_update(x, y, actor_grad_fn)\n\n    critic_grad_fn = jax.grad(self._critic_loss, allow_int=True)\n    self._critic_minibatch_fn = lambda x, y: self._critic_minibatch_update(x, y, critic_grad_fn)\n\n    # Train, evaluate, checkpoint\n    n_training_batches = self.config.n_steps // self.config.eval_frequency\n    progressbar_desc = f'Training batch (training steps = batch x {self.config.eval_frequency})'\n\n    runner, metrics = lax.scan(\n        scan_tqdm(n_training_batches, desc=progressbar_desc)(self._training_step),\n        update_runner,\n        jnp.arange(n_training_batches),\n        n_training_batches\n    )\n\n    if self.eval_during_training:\n        metrics = {\n            key: jnp.concatenate((metrics_start[key][jnp.newaxis, :], metrics[key]), axis=0)\n            for key in metrics.keys()\n        }\n    else:\n        metrics= {}\n\n    return runner, metrics\n</code></pre>"},{"location":"reference/ppoagent/","title":"PPOAgent","text":"<p>               Bases: <code>PPOAgentBase</code></p> <p>PPO clip agent using the GAE (PPO2) for calculating the advantage. The actor loss function standardizes the advantage. See : https://spinningup.openai.com/en/latest/algorithms/ppo.html</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>class PPOAgent(PPOAgentBase):\n\n    \"\"\"\n    PPO clip agent using the GAE (PPO2) for calculating the advantage. The actor loss function standardizes the\n    advantage.\n    See : https://spinningup.openai.com/en/latest/algorithms/ppo.html\n    \"\"\"\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the returns per episode step over a batch of trajectories.\n        :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n        state). In the begining of the method, 'value' is the value of the state in the next step in the trajectory\n        (not the reverse iteration), and after calculation it is the value of the examined state in the examined step.\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n        rewards, discounts, next_state_values, gae_lambda = traj\n        value = rewards + discounts * ((1 - gae_lambda) * next_state_values + gae_lambda * value)\n        return value, value\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _trajectory_advantages(self, advantage: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the GAE per episode step over a batch of trajectories.\n        :param advantage: The GAE advantages of the steps in the trajectory according to the critic (including the one\n        of the last state). In the beginning of the method, 'advantage' is the advantage of the state in the next step\n        in the trajectory (not the reverse iteration), and after calculation it is the advantage of the examined state\n        in each step.\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n        rewards, values, next_state_values, terminated, gamma, gae_lambda = traj\n        d_t = rewards + (1 - terminated) * gamma * next_state_values - values  # Temporal difference residual at time t\n        advantage = d_t + gamma * gae_lambda * (1 - terminated) * advantage\n        return advantage, advantage\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_loss(\n            self,\n            training: TrainState,\n            obs: Annotated[ObsType, \"n_rollout batch_size\"],\n            action: Annotated[ActionType, \"batch_size\"],\n            log_prob_old: Float[Array, \"n_rollout batch_size\"],\n            advantage: ReturnsType,\n            hyperparams: HyperParameters\n    )-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n        \"\"\"\n        Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n        discounted returns and the value as estimated by the critic.\n        :param training: The actor TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param action: The actions in the trajectory batch.\n        :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n        :param advantage: The GAE over the trajectory batch.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n        \"\"\"\n\n        \"\"\" Standardize GAE, greatly improves behaviour\"\"\"\n        advantage = (advantage - advantage.mean(axis=0)) / (advantage.std(axis=0) + 1e-8)\n\n        log_prob_vmap = jax.vmap(jax.vmap(self._log_prob, in_axes=(None, None, 0, 0)), in_axes=(None, None, 0, 0))\n        log_prob = log_prob_vmap(training, training.params, obs, action)\n        log_policy_ratio = log_prob - log_prob_old\n        policy_ratio = jnp.exp(log_policy_ratio)\n        kl = jnp.sum(-log_policy_ratio)\n\n        \"\"\"\n        Adopt simplified formulation of clipped policy ratio * advantage as explained in the note of:\n        https://spinningup.openai.com/en/latest/algorithms/ppo.html#id2\n        \"\"\"\n        clip = jnp.where(jnp.greater(advantage, 0), 1 + hyperparams.eps_clip, 1 - hyperparams.eps_clip)\n        advantage_clip = advantage * clip\n\n        \"\"\"Actual clip calculation - not used but left here for comparison to simplified version\"\"\"\n        # advantage_clip = jnp.clip(policy_ratio, 1 - hyperparams.eps_clip, 1 + hyperparams.eps_clip) * advantage\n\n        loss_actor = jnp.minimum(policy_ratio * advantage, advantage_clip)\n\n        entropy_vmap = jax.vmap(jax.vmap(self._entropy, in_axes=(None, 0)), in_axes=(None, 0))\n        entropy = entropy_vmap(training, obs)\n\n        total_loss_actor = loss_actor.mean() + hyperparams.ent_coeff * entropy.mean()\n\n        \"\"\" Negative loss, because we want ascent but 'apply_gradients' applies descent \"\"\"\n        return -total_loss_actor, kl\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss(\n            self,\n            training: TrainState,\n            obs: Annotated[ObsType, \"n_rollout batch_size\"],\n            targets: ReturnsType,\n            hyperparams: HyperParameters\n    ) -&gt; Float[Array, \"1\"]:\n        \"\"\"\n        Calculates the critic loss.\n        :param training: The critic TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param targets: The targets over the trajectory batch for training the critic.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: The critic loss.\n        \"\"\"\n\n        value_vmap = jax.vmap(jax.vmap(training.apply_fn, in_axes=(None, 0)), in_axes=(None, 0))\n        value = value_vmap(training.params, obs)\n        residuals = value - targets\n        value_loss = jnp.mean(residuals ** 2)\n        critic_total_loss = hyperparams.vf_coeff * value_loss\n\n        return critic_total_loss\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; ActorLossInputType:\n        \"\"\"\n        Prepares the input required by the actor loss function. For the PPO agent, this entails the:\n        - the actions collected over the trajectory batch.\n        - the log-probability of the actions collected over the trajectory batch.\n        - the returns over the trajectory batch.\n        - the values over the trajectory batch as evaluated by the critic.\n        - the training hyperparameters.\n        The input is reshaped so that it is split into minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the actor loss function.\n        \"\"\"\n\n        # Shuffle the trajectory batch to collect minibatches.\n        # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n        # the batch are used per epoch.\n        minibatch_idx = jax.random.choice(\n            jax.random.PRNGKey(1),\n            jnp.arange(self.config.batch_size),\n            replace=False,\n            shape=(self.config.batch_size,)\n        )\n\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n        traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n        return (\n            traj_minibatch.obs,\n            traj_minibatch.action,\n            traj_minibatch.log_prob,\n            traj_minibatch.advantage,\n            update_runner.hyperparams\n        )\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n        \"\"\"\n        Prepares the input required by the critic loss function. For the PPO agent, this entails the:\n        - the states collected over the trajectory batch.\n        - the targets (returns = GAE + next_value) over the trajectory batch.\n        - the training hyperparameters.\n        The input is reshaped so that it is split into minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the critic loss function.\n        \"\"\"\n\n        # Shuffle the trajectory batch to collect minibatches.\n        # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n        # the batch are used per epoch.\n        minibatch_idx = jax.random.choice(\n            jax.random.PRNGKey(1),\n            jnp.arange(self.config.batch_size),\n            replace=False,\n            shape=(self.config.batch_size,)\n        )\n\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n        traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n        return (\n            traj_minibatch.obs,\n            traj_minibatch.advantage + traj_minibatch.value,\n            update_runner.hyperparams\n        )\n</code></pre>"},{"location":"reference/ppoagent/#jaxagents.ppo.PPOAgent._actor_loss","title":"<code>_actor_loss(training, obs, action, log_prob_old, advantage, hyperparams)</code>","text":"<p>Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the discounted returns and the value as estimated by the critic. :param training: The actor TrainState object. :param obs: The obs in the trajectory batch. :param action: The actions in the trajectory batch. :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch. :param advantage: The GAE over the trajectory batch. :param hyperparams: The HyperParameters object used for training. :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_loss(\n        self,\n        training: TrainState,\n        obs: Annotated[ObsType, \"n_rollout batch_size\"],\n        action: Annotated[ActionType, \"batch_size\"],\n        log_prob_old: Float[Array, \"n_rollout batch_size\"],\n        advantage: ReturnsType,\n        hyperparams: HyperParameters\n)-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n    \"\"\"\n    Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n    discounted returns and the value as estimated by the critic.\n    :param training: The actor TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param action: The actions in the trajectory batch.\n    :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n    :param advantage: The GAE over the trajectory batch.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n    \"\"\"\n\n    \"\"\" Standardize GAE, greatly improves behaviour\"\"\"\n    advantage = (advantage - advantage.mean(axis=0)) / (advantage.std(axis=0) + 1e-8)\n\n    log_prob_vmap = jax.vmap(jax.vmap(self._log_prob, in_axes=(None, None, 0, 0)), in_axes=(None, None, 0, 0))\n    log_prob = log_prob_vmap(training, training.params, obs, action)\n    log_policy_ratio = log_prob - log_prob_old\n    policy_ratio = jnp.exp(log_policy_ratio)\n    kl = jnp.sum(-log_policy_ratio)\n\n    \"\"\"\n    Adopt simplified formulation of clipped policy ratio * advantage as explained in the note of:\n    https://spinningup.openai.com/en/latest/algorithms/ppo.html#id2\n    \"\"\"\n    clip = jnp.where(jnp.greater(advantage, 0), 1 + hyperparams.eps_clip, 1 - hyperparams.eps_clip)\n    advantage_clip = advantage * clip\n\n    \"\"\"Actual clip calculation - not used but left here for comparison to simplified version\"\"\"\n    # advantage_clip = jnp.clip(policy_ratio, 1 - hyperparams.eps_clip, 1 + hyperparams.eps_clip) * advantage\n\n    loss_actor = jnp.minimum(policy_ratio * advantage, advantage_clip)\n\n    entropy_vmap = jax.vmap(jax.vmap(self._entropy, in_axes=(None, 0)), in_axes=(None, 0))\n    entropy = entropy_vmap(training, obs)\n\n    total_loss_actor = loss_actor.mean() + hyperparams.ent_coeff * entropy.mean()\n\n    \"\"\" Negative loss, because we want ascent but 'apply_gradients' applies descent \"\"\"\n    return -total_loss_actor, kl\n</code></pre>"},{"location":"reference/ppoagent/#jaxagents.ppo.PPOAgent._actor_loss_input","title":"<code>_actor_loss_input(update_runner, traj_batch)</code>","text":"<p>Prepares the input required by the actor loss function. For the PPO agent, this entails the: - the actions collected over the trajectory batch. - the log-probability of the actions collected over the trajectory batch. - the returns over the trajectory batch. - the values over the trajectory batch as evaluated by the critic. - the training hyperparameters. The input is reshaped so that it is split into minibatches. :param update_runner: The Runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the actor loss function.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; ActorLossInputType:\n    \"\"\"\n    Prepares the input required by the actor loss function. For the PPO agent, this entails the:\n    - the actions collected over the trajectory batch.\n    - the log-probability of the actions collected over the trajectory batch.\n    - the returns over the trajectory batch.\n    - the values over the trajectory batch as evaluated by the critic.\n    - the training hyperparameters.\n    The input is reshaped so that it is split into minibatches.\n    :param update_runner: The Runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the actor loss function.\n    \"\"\"\n\n    # Shuffle the trajectory batch to collect minibatches.\n    # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n    # the batch are used per epoch.\n    minibatch_idx = jax.random.choice(\n        jax.random.PRNGKey(1),\n        jnp.arange(self.config.batch_size),\n        replace=False,\n        shape=(self.config.batch_size,)\n    )\n\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n    traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n    return (\n        traj_minibatch.obs,\n        traj_minibatch.action,\n        traj_minibatch.log_prob,\n        traj_minibatch.advantage,\n        update_runner.hyperparams\n    )\n</code></pre>"},{"location":"reference/ppoagent/#jaxagents.ppo.PPOAgent._critic_loss","title":"<code>_critic_loss(training, obs, targets, hyperparams)</code>","text":"<p>Calculates the critic loss. :param training: The critic TrainState object. :param obs: The obs in the trajectory batch. :param targets: The targets over the trajectory batch for training the critic. :param hyperparams: The HyperParameters object used for training. :return: The critic loss.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_loss(\n        self,\n        training: TrainState,\n        obs: Annotated[ObsType, \"n_rollout batch_size\"],\n        targets: ReturnsType,\n        hyperparams: HyperParameters\n) -&gt; Float[Array, \"1\"]:\n    \"\"\"\n    Calculates the critic loss.\n    :param training: The critic TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param targets: The targets over the trajectory batch for training the critic.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: The critic loss.\n    \"\"\"\n\n    value_vmap = jax.vmap(jax.vmap(training.apply_fn, in_axes=(None, 0)), in_axes=(None, 0))\n    value = value_vmap(training.params, obs)\n    residuals = value - targets\n    value_loss = jnp.mean(residuals ** 2)\n    critic_total_loss = hyperparams.vf_coeff * value_loss\n\n    return critic_total_loss\n</code></pre>"},{"location":"reference/ppoagent/#jaxagents.ppo.PPOAgent._critic_loss_input","title":"<code>_critic_loss_input(update_runner, traj_batch)</code>","text":"<p>Prepares the input required by the critic loss function. For the PPO agent, this entails the: - the states collected over the trajectory batch. - the targets (returns = GAE + next_value) over the trajectory batch. - the training hyperparameters. The input is reshaped so that it is split into minibatches. :param update_runner: The Runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the critic loss function.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n    \"\"\"\n    Prepares the input required by the critic loss function. For the PPO agent, this entails the:\n    - the states collected over the trajectory batch.\n    - the targets (returns = GAE + next_value) over the trajectory batch.\n    - the training hyperparameters.\n    The input is reshaped so that it is split into minibatches.\n    :param update_runner: The Runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the critic loss function.\n    \"\"\"\n\n    # Shuffle the trajectory batch to collect minibatches.\n    # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n    # the batch are used per epoch.\n    minibatch_idx = jax.random.choice(\n        jax.random.PRNGKey(1),\n        jnp.arange(self.config.batch_size),\n        replace=False,\n        shape=(self.config.batch_size,)\n    )\n\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n    traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n    return (\n        traj_minibatch.obs,\n        traj_minibatch.advantage + traj_minibatch.value,\n        update_runner.hyperparams\n    )\n</code></pre>"},{"location":"reference/ppoagent/#jaxagents.ppo.PPOAgent._trajectory_advantages","title":"<code>_trajectory_advantages(advantage, traj)</code>","text":"<p>Calculates the GAE per episode step over a batch of trajectories. :param advantage: The GAE advantages of the steps in the trajectory according to the critic (including the one of the last state). In the beginning of the method, 'advantage' is the advantage of the state in the next step in the trajectory (not the reverse iteration), and after calculation it is the advantage of the examined state in each step. :param traj: The trajectory batch. :return: An array of returns.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _trajectory_advantages(self, advantage: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the GAE per episode step over a batch of trajectories.\n    :param advantage: The GAE advantages of the steps in the trajectory according to the critic (including the one\n    of the last state). In the beginning of the method, 'advantage' is the advantage of the state in the next step\n    in the trajectory (not the reverse iteration), and after calculation it is the advantage of the examined state\n    in each step.\n    :param traj: The trajectory batch.\n    :return: An array of returns.\n    \"\"\"\n    rewards, values, next_state_values, terminated, gamma, gae_lambda = traj\n    d_t = rewards + (1 - terminated) * gamma * next_state_values - values  # Temporal difference residual at time t\n    advantage = d_t + gamma * gae_lambda * (1 - terminated) * advantage\n    return advantage, advantage\n</code></pre>"},{"location":"reference/ppoagent/#jaxagents.ppo.PPOAgent._trajectory_returns","title":"<code>_trajectory_returns(value, traj)</code>","text":"<p>Calculates the returns per episode step over a batch of trajectories. :param value: The values of the steps in the trajectory according to the critic (including the one of the last state). In the begining of the method, 'value' is the value of the state in the next step in the trajectory (not the reverse iteration), and after calculation it is the value of the examined state in the examined step. :param traj: The trajectory batch. :return: An array of returns.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the returns per episode step over a batch of trajectories.\n    :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n    state). In the begining of the method, 'value' is the value of the state in the next step in the trajectory\n    (not the reverse iteration), and after calculation it is the value of the examined state in the examined step.\n    :param traj: The trajectory batch.\n    :return: An array of returns.\n    \"\"\"\n    rewards, discounts, next_state_values, gae_lambda = traj\n    value = rewards + discounts * ((1 - gae_lambda) * next_state_values + gae_lambda * value)\n    return value, value\n</code></pre>"},{"location":"reference/ppoagentbase/","title":"PPOAgentBase","text":"<p>               Bases: <code>ABC</code></p> <p>Base for PPO agents. Can be used for both discrete or continuous action environments, and its use depends on the provided actor network. Follows the instructions of: https://spinningup.openai.com/en/latest/algorithms/ppo.html Uses lax.scan for rollout, so trajectories may be truncated.</p> <p>Training relies on jitting several methods by treating the 'self' arg as static. According to suggested practice, this can prove dangerous (https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods - How to use jit with methods?); if attrs of 'self' change during training, the changes will not be registered in jit. In this case, neither agent training nor evaluation change any 'self' attrs, so using Strategy 2 of the suggested practice is valid. Otherwise, strategy 3 should have been used.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>class PPOAgentBase(ABC):\n    \"\"\"\n    Base for PPO agents.\n    Can be used for both discrete or continuous action environments, and its use depends on the provided actor network.\n    Follows the instructions of: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n    Uses lax.scan for rollout, so trajectories may be truncated.\n\n    Training relies on jitting several methods by treating the 'self' arg as static. According to suggested practice,\n    this can prove dangerous (https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods -\n    How to use jit with methods?); if attrs of 'self' change during training, the changes will not be registered in\n    jit. In this case, neither agent training nor evaluation change any 'self' attrs, so using Strategy 2 of the\n    suggested practice is valid. Otherwise, strategy 3 should have been used.\n    \"\"\"\n\n    # Function for performing a minibatch update of the actor network.\n    _actor_minibatch_fn: ClassVar[Callable[\n        [Tuple[TrainState, ActorLossInputType, float]],\n        Tuple[TrainState, ActorLossInputType, float]]\n    ]\n    # Function for performing a minibatch update of the critic network.\n    _critic_minibatch_fn: ClassVar[Callable[\n        [Tuple[TrainState, CriticLossInputType]],\n        Tuple[TrainState, CriticLossInputType]]\n    ]\n    agent_trained: ClassVar[bool] = False  # Whether the agent has been trained.\n    training_runner: ClassVar[Optional[Runner]] = None  # Runner object after training.\n    actor_training: ClassVar[Optional[TrainState]] = None  # Actor training object.\n    critic_training: ClassVar[Optional[TrainState]] = None  # Critic training object.\n    training_metrics: ClassVar[Optional[Dict[str, Float[Array, \"1\"]]]] = None  # Metrics collected during training.\n    eval_during_training: ClassVar[bool] = False  # Whether the agent's performance is evaluated during training\n    # The maximum step reached in precious training. Zero by default for starting a new training. Will be set by\n    # restoring or passing a trained agent (from serial training or restoring)\n    previous_training_max_step: ClassVar[int] = 0\n\n    def __init__(\n            self,\n            env: Environment,\n            env_params: EnvParams,\n            config: AgentConfig,\n            eval_during_training: bool = True\n    ) -&gt; None:\n        \"\"\"\n        :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n        :param env_params: A dataclass named \"EnvParams\" containing the parametrization of the environment.\n        :param config: The configuration of the agent as and AgentConfig object (from vpf_utils).\n        \"\"\"\n\n        self.config = config\n        self.eval_during_training = eval_during_training\n        self._init_checkpointer()\n        self._init_env(env, env_params)\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Returns a string containing only the non-default field values.\n        \"\"\"\n\n        output_lst = [field + ': ' + str(getattr(self.config, field)) for field in self.config._fields]\n        output_lst = ['Agent configuration:'] + output_lst\n\n        return '\\n'.join(output_lst)\n\n    \"\"\" GENERAL METHODS\"\"\"\n\n    def _init_env(self, env: Environment, env_params: EnvParams) -&gt; None:\n        \"\"\"\n        Environment initialization.\n        :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n        :param env_params: A dataclass containing the parametrization of the environment.\n        :return:\n        \"\"\"\n\n        env = TruncationWrapper(env, self.config.max_episode_steps)\n        # env = FlattenObservationWrapper(env)\n        # self.env = LogWrapper(env)\n        self.env = env\n        self.env_params = env_params\n\n    def _init_checkpointer(self) -&gt; None:\n        \"\"\"\n        Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If\n        so, sets the checkpoint manager using orbax.\n        :return:\n        \"\"\"\n\n        self.checkpointing = self.config.checkpoint_dir is not None\n\n        if self.checkpointing:\n\n            if not self.config.restore_agent:\n\n                dir_exists = os.path.exists(self.config.checkpoint_dir)\n                if not dir_exists:\n                    os.makedirs(self.config.checkpoint_dir)\n\n                dir_files = [\n                    file for file in os.listdir(self.config.checkpoint_dir)\n                    if os.path.isdir(os.path.join(self.config.checkpoint_dir, file))\n                ]\n                if len(dir_files) &gt; 0:\n                    for file in dir_files:\n                        file_path = os.path.join(self.config.checkpoint_dir, file)\n                        shutil.rmtree(file_path)\n\n                # Log training configuration\n                with open(os.path.join(self.config.checkpoint_dir, 'training_configuration.txt'), \"w\") as f:\n                    f.write(self.__str__())\n\n            orbax_checkpointer = orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler())\n\n            options = orbax.checkpoint.CheckpointManagerOptions(\n                create=True,\n                step_prefix='trainingstep',\n            )\n\n            self.checkpoint_manager = orbax.checkpoint.CheckpointManager(\n                self.config.checkpoint_dir,\n                orbax_checkpointer,\n                options\n            )\n\n        else:\n\n            self.checkpoint_manager = None\n\n    def _create_empty_trainstate(self, network) -&gt; TrainState:\n        \"\"\"\n        Creates an empty TrainState object for restoring checkpoints.\n        :param network: The actor or critic network.\n        :return:\n        \"\"\"\n\n        rng = jax.random.PRNGKey(1)  # Just a dummy PRNGKey for initializing the networks parameters.\n        network, params = self._init_network(rng, network)\n\n        optimizer_params = OptimizerParams()  # Use the default values of the OptimizerParams object.\n        tx = self._init_optimizer(optimizer_params)\n\n        empty_training = TrainState.create(apply_fn=network.apply, params=params, tx=tx)\n\n        return empty_training\n\n    def restore(\n            self,\n            mode: str = \"best\",\n            best_fn: Optional[Callable[[Dict[str, Float[Array, \"1\"]]], [int]]] = None\n    ) -&gt; None:\n        \"\"\"\n        Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then,\n        post-processes the restored checkpoint.\n        :param mode: Determines whether the best performing or latest checkpoint should be restored.\n        :param best_fn: The function that should be used in determining the best performing checkpoint.\n        :return:\n        \"\"\"\n\n        steps = self.checkpoint_manager.all_steps()\n\n        # Log keys in checkpoints\n        ckpt = self.checkpoint_manager.restore(steps[0])\n        ckpt_keys = [key for key in list(ckpt.keys()) if key != \"runner\"]\n\n        # Collect history of metrics in training. Useful for continuing training.\n        metrics = {key: [None] * len(steps) for key in ckpt_keys}\n        for i, step in enumerate(steps):\n            ckpt = self.checkpoint_manager.restore(step)\n            for key in ckpt_keys:\n                metrics[key][i] = ckpt[key][jnp.newaxis, :]\n        metrics = {key: jnp.concatenate(val, axis=0) for (key, val) in metrics.items()}\n\n        if mode == \"best\":\n            if best_fn is not None:\n                step = steps[best_fn(metrics)]\n            else:\n                raise Exception(\"Function for determining best checkpoint not provided\")\n        elif mode == \"last\":\n            step = self.checkpoint_manager.latest_step()\n        else:\n            raise Exception(\"Unknown method for selecting a checkpoint.\")\n\n        \"\"\"\n        Create an empty target for restoring the checkpoint.\n        Some of the arguments come from restoring one of the ckpts.\n        \"\"\"\n\n        empty_actor_training = self._create_empty_trainstate(self.config.actor_network)\n        empty_critic_training = self._create_empty_trainstate(self.config.critic_network)\n\n        # Get some obs and envstate for restoring the checkpoint.\n        _, obs, envstate = self.env_reset(jax.random.PRNGKey(1))\n\n        empty_runner = Runner(\n            actor_training=empty_actor_training,\n            critic_training=empty_critic_training,\n            envstate=envstate,\n            obs=obs,\n            rng=jax.random.split(jax.random.PRNGKey(1), self.config.batch_size),  # Just a dummy PRNGKey for initializing the networks parameters.\n            # Hyperparams can be loaded as a dict. If training continues, new hyperparams will be provided.\n            hyperparams=ckpt[\"runner\"][\"hyperparams\"]\n        )\n\n        target_ckpt = {\n            \"runner\": empty_runner,\n            \"terminated\": jnp.zeros(metrics[\"terminated\"].shape[1]),\n            \"truncated\": jnp.zeros(metrics[\"truncated\"].shape[1]),\n            \"final_rewards\": jnp.zeros(metrics[\"final_rewards\"].shape[1]),\n            \"returns\": jnp.zeros(metrics[\"returns\"].shape[1]),\n        }\n\n        ckpt = self.checkpoint_manager.restore(step, items=target_ckpt)\n\n        self.collect_training(ckpt[\"runner\"], metrics, previous_training_max_step=max(steps))\n\n    def _init_optimizer(self, optimizer_params: OptimizerParams) -&gt; optax.chain:\n        \"\"\"\n        Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to\n        initialize the appropriate optimizer. In this way, the optimizer can be initialized within the \"train\" method,\n        and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary.\n        :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n        :return: An optimizer in optax.chain.\n        \"\"\"\n\n        optimizer_params_dict = optimizer_params._asdict()  # Transform from NamedTuple to dict\n        optimizer_params_dict.pop('grad_clip', None)  # Remove 'grad_clip', since it is not part of the optimizer args.\n\n\n        \"\"\"\n        Get dictionary of optimizer parameters to pass in optimizer. The procedure preserves parameters that:\n            - are given in the OptimizerParams NamedTuple and are requested as args by the optimizer\n            - are requested as args by the optimizer and are given in the OptimizerParams NamedTuple\n        \"\"\"\n\n        optimizer_arg_names = self.config.optimizer.__code__.co_varnames  # List names of args of optimizer.\n\n        # Keep only the optimizer arg names that are also part of the OptimizerParams (dict from NamedTuple)\n        optimizer_arg_names = [\n            arg_name for arg_name in optimizer_arg_names if arg_name in list(optimizer_params_dict.keys())\n        ]\n        if len(optimizer_arg_names) == 0:\n            raise Exception(\n                \"The defined optimizer parameters do not include relevant arguments for this optimizer.\"\n                \"The optimizer has not been implemented yet. Define your own OptimizerParams object.\"\n            )\n\n        # Keep only the optimizer params that are arg names for the specific optimizer\n        optimizer_params_dict = {arg_name: optimizer_params_dict[arg_name] for arg_name in optimizer_arg_names}\n\n        # No need to scale by -1.0. 'TrainState.apply_gradients' is used for training, which subtracts the update.\n        tx = optax.chain(\n            optax.clip_by_global_norm(optimizer_params.grad_clip),\n            self.config.optimizer(**optimizer_params_dict)\n        )\n\n        return tx\n\n    def _init_network(\n            self,\n            rng: PRNGKeyArray,\n            network: flax.linen.Module\n    ) -&gt; Tuple[flax.linen.Module, FrozenDict]:\n        \"\"\"\n        Initialization of the actor or critic network.\n        :param rng: Random key for initialization.\n        :param network: The actor or critic network.\n        :return: A random key after splitting the input and the initial parameters of the policy network.\n        \"\"\"\n\n        # Initialize the agent networks. The number of actions is irrelevant for the Critic network, which should return\n        # a single value in the final layer. However, the network class should accept the number of actions as an\n        # argument, even if it isn't used.\n        network = network(self.config)\n\n        rng, *_rng = jax.random.split(rng, 3)\n        dummy_reset_rng, network_init_rng = _rng\n\n        dummy_obs, _ = self.env.reset(dummy_reset_rng, self.env_params)\n        init_x = jnp.zeros((1, dummy_obs.size))\n\n        params = network.init(network_init_rng, init_x)\n\n        return network, params\n\n    @partial(jax.jit, static_argnums=(0,))\n    def env_reset(self, rng: PRNGKeyArray) -&gt; Tuple[PRNGKeyArray, ObsType, LogEnvState | EnvState | TruncationEnvState]:\n        \"\"\"\n        Environment reset.\n        :param rng: Random key for initialization.\n        :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.\n        \"\"\"\n\n        rng, reset_rng = jax.random.split(rng)\n        obs, envstate = self.env.reset(reset_rng, self.env_params)\n        return rng, obs, envstate\n\n    @partial(jax.jit, static_argnums=(0,))\n    def env_step(\n            self,\n            rng: PRNGKeyArray,\n            envstate: LogEnvState | EnvState | TruncationEnvState,\n            action: ActionType\n    ) -&gt; Tuple[\n        PRNGKeyArray,\n        ObsType,\n        LogEnvState | EnvState | TruncationEnvState,\n        Float[Array, \"1\"],\n        Bool[Array, \"1\"],\n        Dict[str, float | bool]\n    ]:\n        \"\"\"\n        Environment step.\n        :param rng: Random key for initialization.\n        :param envstate: The environment state in LogEnvState format.\n        :param action: The action selected by the agent.\n        :return: A tuple of: a random key after splitting the input, the next obs in array and LogEnvState formats,\n                 the collected reward after executing the action, episode termination and a dictionary of optional\n                 additional information.\n        \"\"\"\n\n        rng, step_rng = jax.random.split(rng)\n        next_obs, next_envstate, reward, done, info = (\n            self.env.step(step_rng, envstate, action.squeeze(), self.env_params))\n\n        return rng, next_obs, next_envstate, reward, done, info\n\n\n    \"\"\" METHODS FOR TRAINING \"\"\"\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _make_transition(\n            self,\n            obs: ObsType,\n            action: ActionType,\n            value: Float[Array, \"1\"],\n            log_prob: Float[Array, \"1\"],\n            reward: Float[Array, \"1\"],\n            next_obs: ObsType,\n            terminated: Bool[Array, \"1\"],\n    ) -&gt; Transition:\n        \"\"\"\n        Creates a transition object based on the input and output of an episode step.\n        :param obs: The current obs of the episode step in array format.\n        :param action: The action selected by the agent.\n        :param value: The critic value of the obs.\n        :param log_prob: The actor log-probability of the selected action.\n        :param reward: The collected reward after executing the action.\n        :param next_obs: The next obs of the episode step in array format.\n        :param terminated: Episode termination.\n        :return: A transition object storing information about the state before and after executing the episode step,\n                 the executed action, the collected reward, episode termination and optional additional information.\n        \"\"\"\n\n        transition = Transition(obs.squeeze(), action, value, log_prob, reward, next_obs, terminated)\n        transition = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), transition)\n\n        return transition\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _generate_metrics(self, runner: Runner, update_step: int) -&gt; Dict[str, Float[Array, \"1\"]]:\n        \"\"\"\n        Generates metrics for on-policy learning. The agent performance during training is evaluated by running\n        n_evals episodes (until termination). The selected metric is the sum of rewards collected dring the episode.\n        If the user selects not to generate metrics (leading to faster training), an empty dictinary is returned.\n        :param runner: The update runner object, containing information about the current status of the actor's/critic's\n        training, the state of the environment and training hyperparameters.\n        :param update_step: The number of the update step.\n        :return: A dictionary of the sum of rewards collected over 'n_evals' episodes, or empty dictionary.\n        \"\"\"\n\n        metric = {}\n        if self.eval_during_training:\n            metric = self._eval_agent(\n                self.config.eval_rng,\n                runner.actor_training,\n                runner.critic_training,\n                self.config.n_evals\n            )\n\n        metric.update({\n            \"actor_loss\": runner.actor_loss,\n            \"critic_loss\": runner.critic_loss\n        })\n\n        return metric\n\n    def _create_training(\n            self,\n            rng: PRNGKeyArray,\n            network: type[flax.linen.Module],\n            optimizer_params: OptimizerParams\n    )-&gt; TrainState:\n        \"\"\"\n         Creates a TrainState object for the actor or the critic.\n        :param rng: Random key for initialization.\n        :param network: The actor or critic network.\n        :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n        :return: A TrainState object to be used in training the actor and cirtic networks.\n        \"\"\"\n\n        network, params = self._init_network(rng, network)\n        tx = self._init_optimizer(optimizer_params)\n        return TrainState.create(apply_fn=network.apply, tx=tx, params=params)\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _create_update_runner(\n            self,\n            rng: PRNGKeyArray,\n            actor_training: TrainState,\n            critic_training: TrainState,\n            hyperparams: HyperParameters\n    ) -&gt; Runner:\n        \"\"\"\n        Initializes the update runner as a Runner object. The runner contains n_evals initializations of the\n        environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and\n        one for the critic network, so that trajectory batches are used to train the same parameters.\n        :param rng: Random key for initialization.\n        :param actor_training: The actor TrainState object used in training.\n        :param critic_training: The critic TrainState object used in training.\n        :param hyperparams: An instance of HyperParameters for training.\n        :return: An update runner object to be used in trajectory sampling and training.\n        \"\"\"\n\n        rng, reset_rng, runner_rng = jax.random.split(rng, 3)\n        reset_rngs = jax.random.split(reset_rng, self.config.batch_size)\n        runner_rngs = jax.random.split(runner_rng, self.config.batch_size)\n\n        _, obs, envstate = jax.vmap(self.env_reset)(reset_rngs)\n\n        update_runner = Runner(\n            actor_training=actor_training,\n            critic_training=critic_training,\n            envstate=envstate,\n            obs=obs,\n            rng=runner_rngs,\n            hyperparams=hyperparams,\n            actor_loss=jnp.zeros(1),\n            critic_loss=jnp.zeros(1),\n        )\n\n        return update_runner\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _add_next_values(\n            self,\n            traj_batch: Transition,\n            last_obs: ObsType,\n            critic_training: TrainState\n    ) -&gt; Transition:\n        \"\"\"\n        Concatenates all state values but the first one with the value estimate of the final state, to represent the\n        values of the next state (1-step lag).\n        :param traj_batch: The batch of trajectories.\n        :param last_obs: The obs at the end of every trajectory in the batch.\n        :param critic_training: The critic TrainState object (either mid- or post-training).\n        :return: The batch of trajectories with the updated next-state values.\n        \"\"\"\n        last_state_value_vmap = jax.vmap(critic_training.apply_fn, in_axes=(None, 0))\n        last_state_value = last_state_value_vmap(lax.stop_gradient(critic_training.params), last_obs)\n\n        \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n        next_values_t = jnp.concatenate(\n            [traj_batch.value.squeeze(), last_state_value[..., jnp.newaxis]],\n            axis=-1)[:, 1:]\n\n        traj_batch = traj_batch._replace(next_value=next_values_t)\n\n        return traj_batch\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _add_advantages(self, traj_batch: Transition, advantage: ReturnsType) -&gt; Transition:\n        \"\"\"\n        Simply inputs the advantages in the batch of trajectories.\n        :param traj_batch: The batch of trajectories.\n        :param advantage: The advantage over the trajectory batch.\n        :return: The batch of trajectories with the updated advantage.\n        \"\"\"\n        traj_batch = traj_batch._replace(advantage=advantage)\n\n        return traj_batch\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _returns(\n            self,\n            traj_batch: Transition,\n            last_next_state_value: Float[Array, \"batch_size\"],\n            gamma: float,\n            gae_lambda: float\n    ) -&gt; ReturnsType:\n        \"\"\"\n        Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the\n        trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n        episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n        step, trajectories do not start at the initial state.\n        :param traj_batch: The batch of trajectories.\n        :param last_next_state_value: The value of the last next state in each trajectory.\n        :param gamma: Discount factor\n        :param gae_lambda: The GAE \u03bb factor.\n        :return: The returns over the episodes of the trajectory batch.\n        \"\"\"\n\n        rewards_t = traj_batch.reward.squeeze()\n        terminated_t = 1.0 - traj_batch.terminated.astype(jnp.float32).squeeze()\n        discounts_t = (terminated_t * gamma).astype(jnp.float32)\n\n        \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n        next_state_values_t = jnp.concatenate(\n            [traj_batch.value.squeeze(), last_next_state_value[..., jnp.newaxis]],\n            axis=-1)[:, 1:]\n\n        rewards_t, discounts_t, next_state_values_t = jax.tree_util.tree_map(\n            lambda x: jnp.swapaxes(x, 0, 1), (rewards_t, discounts_t, next_state_values_t)\n        )\n\n        gae_lambda = jnp.ones_like(discounts_t) * gae_lambda\n\n        traj_runner = (rewards_t, discounts_t, next_state_values_t, gae_lambda)\n        end_value = jnp.take(next_state_values_t, -1, axis=0)  # Start from end of trajectory and work in reverse.\n        _, returns = lax.scan(self._trajectory_returns, end_value, traj_runner, reverse=True)\n\n        returns = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), returns)\n\n        return returns\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _advantages(\n            self,\n            traj_batch: Transition,\n            gamma: float,\n            gae_lambda: float\n    ) -&gt; ReturnsType:\n        \"\"\"\n        Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the\n        trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n        episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n        step, trajectories do not start at the initial state.\n        :param traj_batch: The batch of trajectories.\n        :param last_next_state_value: The value of the last next state in each trajectory.\n        :param gamma: Discount factor\n        :param gae_lambda: The GAE \u03bb factor.\n        :return: The returns over the episodes of the trajectory batch.\n        \"\"\"\n\n        rewards_t = traj_batch.reward.squeeze()\n        values_t = traj_batch.value.squeeze()\n        terminated_t = traj_batch.terminated.squeeze()\n        next_state_values_t = traj_batch.next_value.squeeze()\n        gamma_t = jnp.ones_like(terminated_t) * gamma\n        gae_lambda_t = jnp.ones_like(terminated_t) * gae_lambda\n\n        rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t = jax.tree_util.tree_map(\n            lambda x: jnp.swapaxes(x, 0, 1),\n            (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n        )\n\n        traj_runner = (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n        \"\"\"\n        TODO:\n        Advantage of last step is taken from the critic, in contrast to traditional approaches, where the rollout \n        ends with episode termination and the advantage is zero. Training is still successful and the influence of this\n        implementation choice is negligible.\n        \"\"\"\n        end_advantage = jnp.zeros(self.config.batch_size)\n        _, advantages = jax.lax.scan(self._trajectory_advantages, end_advantage, traj_runner, reverse=True)\n\n        advantages = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), advantages)\n\n        return advantages\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _make_rollout_runners(self, update_runner: Runner) -&gt; Tuple[StepRunnerType, ...]:\n        \"\"\"\n        Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner\n        object and broadcasting the TrainState object for the critic and the network in the update_runner object to the\n        same dimension.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :return: tuple with step runners to be used in rollout.\n        \"\"\"\n\n        rollout_runner = (\n            update_runner.envstate,\n            update_runner.obs,\n            update_runner.actor_training,\n            update_runner.critic_training,\n            update_runner.rng,\n        )\n        rollout_runners = jax.vmap(\n            lambda v, w, x, y, z: (v, w, x, y, z), in_axes=(0, 0, None, None, 0)\n        )(*rollout_runner)\n        return rollout_runners\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _rollout(self, step_runner: StepRunnerType, i_step: int) -&gt; Tuple[StepRunnerType, Transition]:\n        \"\"\"\n        Evaluation of trajectory rollout. In each step the agent:\n        - evaluates policy and value\n        - selects action\n        - performs environment step\n        - creates step transition\n        :param step_runner: A tuple containing information on the environment state, the actor and critic training\n        (parameters and networks) and a random key.\n        :param i_step: Unused, required for lax.scan.\n        :return: The updated step_runner tuple and the rollout step transition.\n        \"\"\"\n\n        envstate, obs, actor_training, critic_training, rng = step_runner\n\n        rng, rng_action = jax.random.split(rng)\n        action = self._sample_action(rng_action, actor_training, obs)\n\n        value = critic_training.apply_fn(lax.stop_gradient(critic_training.params), obs)\n\n        log_prob = self._log_prob(actor_training, lax.stop_gradient(actor_training.params), obs, action)\n\n        rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, action)\n\n        step_runner = (next_envstate, next_obs, actor_training, critic_training, rng)\n\n        terminated = info[\"terminated\"]\n\n        transition = self._make_transition(\n            obs=obs,\n            action=action,\n            value=value,\n            log_prob=log_prob,\n            reward=reward,\n            next_obs=next_obs,\n            terminated=terminated,\n        )\n\n        return step_runner, transition\n\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _process_trajectory(self, update_runner: Runner, traj_batch: Transition, last_obs: ObsType) -&gt; Transition:\n        \"\"\"\n        Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not\n        guaranteed to end with termination, the value is estimated using the critic network. This assumption has been\n        shown to have no influence by the end of training.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param traj_batch: The batch of trajectories, as collected by in rollout.\n        :param last_obs: The obs at the end of every trajectory in the batch.\n        :return: A batch of trajectories that includes an estimate of values and advantages.\n        \"\"\"\n\n        traj_batch = jax.tree_util.tree_map(lambda x: x.squeeze(), traj_batch)\n        traj_batch = self._add_next_values(traj_batch, last_obs, update_runner.critic_training)\n\n        advantages = self._advantages(traj_batch, update_runner.hyperparams.gamma, update_runner.hyperparams.gae_lambda)\n        traj_batch = self._add_advantages(traj_batch, advantages)\n\n        return traj_batch\n\n    @staticmethod\n    def _actor_minibatch_update(\n            i_minibatch: int,\n            minibatch_runner: Tuple[TrainState, ActorLossInputType, float],\n            grad_fn: Callable[[Any], ActorLossInputType]\n    ) -&gt; Annotated[Tuple[TrainState, ActorLossInputType, float], \"n_minibatch\"]:\n        \"\"\"\n        Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be\n        passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n        :param i_minibatch: Number of minibatch update.\n        :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence.\n        :param grad_fn: The gradient function of the training loss.\n        :return: Minibatch runner with an updated TrainState.\n        \"\"\"\n\n        actor_training, actor_loss_input, kl = minibatch_runner\n        *traj_batch, hyperparams = actor_loss_input\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n        grad_input_minibatch = (actor_training, *traj_minibatch, hyperparams)\n        grads, kl = grad_fn(*grad_input_minibatch)\n        actor_training = actor_training.apply_gradients(grads=grads.params)\n        return actor_training, actor_loss_input, kl\n\n    @staticmethod\n    def _critic_minibatch_update(\n            i_minibatch: int,\n            minibatch_runner: Tuple[TrainState, CriticLossInputType],\n            grad_fn: Callable[[Any], CriticLossInputType]\n    ) -&gt; Tuple[TrainState, CriticLossInputType]:\n        \"\"\"\n        Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be\n        passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n        :param i_minibatch: Number of minibatch update.\n        :param minibatch_runner: A tuple containing the TranState object and the loss input arguments.\n        :param grad_fn: The gradient function of the training loss.\n        :return: Minibatch runner with an updated TrainState.\n        \"\"\"\n\n        critic_training, critic_loss_input = minibatch_runner\n        *traj_batch, hyperparams = critic_loss_input\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n        grad_input_minibatch = (critic_training, *traj_minibatch, hyperparams)\n        grads = grad_fn(*grad_input_minibatch)\n        critic_training = critic_training.apply_gradients(grads=grads.params)\n        return critic_training, critic_loss_input\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_epoch(\n            self,\n            epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]\n    ) -&gt; Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]:\n        \"\"\"\n        Performs a Gradient Ascent update of the actor.\n        :param epoch_runner: A tuple containing the following information about the update:\n        - actor_training: TrainState object for actor training\n        - actor_loss_input: tuple with the inputs required by the actor loss function.\n        - kl: The KL divergence collected during the update (used in checking for early stopping).\n        - epoch: The number of the current training epoch.\n        - kl_threshold: The KL divergence threshold for early stopping.\n        :return: The updated epoch runner.\n        \"\"\"\n\n        actor_training, actor_loss_input, kl, epoch, kl_threshold = epoch_runner\n        minibatch_runner = (actor_training, actor_loss_input, 0)\n        n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n        minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._actor_minibatch_fn, minibatch_runner)\n        actor_training, _, kl = minibatch_runner\n\n        return actor_training, actor_loss_input, kl, epoch+1, kl_threshold\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_training_cond(\n            self,\n            epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]\n    ) -&gt; Bool[Array, \"1\"]:\n        \"\"\"\n        Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been\n        met or due to KL divergence early stopping).\n        :param epoch_runner: A tuple containing the following information about the update:\n        - actor_training: TrainState object for actor training\n        - actor_loss_input: tuple with the inputs required by the actor loss function.\n        - kl: The KL divergence collected during the update (used in checking for early stopping).\n        - epoch: The number of the current training epoch.\n        - kl_threshold: The KL-divergence threshold for early stopping.\n        :return: Whether the lax.while_loop over training epochs finishes.\n        \"\"\"\n\n        _, _, kl, epoch, kl_threshold = epoch_runner\n        return jnp.logical_and(\n            jnp.less(epoch, self.config.actor_epochs),\n            jnp.less_equal(kl, kl_threshold)\n        )\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_update(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[TrainState, Float[Array, \"1\"]]:\n        \"\"\"\n        Prepares the input and performs Gradient Ascent for the actor network.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param traj_batch: The batch of trajectories.\n        :return: The actor training object updated after actor_epochs steps of Gradient Ascent.\n        \"\"\"\n\n        actor_loss_input = self._actor_loss_input(update_runner, traj_batch)\n\n        start_kl, start_epoch = -jnp.inf, 1\n        actor_epoch_runner = (\n            update_runner.actor_training,\n            actor_loss_input,\n            start_kl,\n            start_epoch,\n            update_runner.hyperparams.kl_threshold\n        )\n        actor_epoch_runner = lax.while_loop(self._actor_training_cond, self._actor_epoch, actor_epoch_runner)\n        actor_training, _, _, _, _ = actor_epoch_runner\n\n        actor_loss, _ = self._actor_loss(\n            actor_training,\n            traj_batch.obs,\n            traj_batch.action,\n            traj_batch.log_prob,\n            traj_batch.advantage,\n            update_runner.hyperparams\n        )\n\n        return actor_training, actor_loss\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_epoch(\n            self,\n            i_epoch: int,\n            epoch_runner: Tuple[TrainState, CriticLossInputType]\n    ) -&gt; Tuple[TrainState, CriticLossInputType]:\n        \"\"\"\n        Performs a Gradient Descent update of the critic.\n        :param: i_epoch: The current training epoch (unused but required by lax.fori_loop).\n        :param epoch_runner: A tuple containing the following information about the update:\n        - critic_training: TrainState object for critic training\n        - critic_loss_input: tuple with the inputs required by the critic loss function.\n        :return: The updated epoch runner.\n        \"\"\"\n\n        critic_training, critic_loss_input = epoch_runner\n        minibatch_runner = (critic_training, critic_loss_input)\n        n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n        minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._critic_minibatch_fn, minibatch_runner)\n        critic_training, _ = minibatch_runner\n\n        return critic_training, critic_loss_input\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_update(self, update_runner: Runner, traj_batch: Transition) -&gt;  Tuple[TrainState, Float[Array, \"1\"]]:\n        \"\"\"\n        Prepares the input and performs Gradient Descent for the critic network.\n        :param update_runner: The Runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param traj_batch: The batch of trajectories.\n        :return: The critic training object updated after actor_epochs steps of Gradient Ascent.\n        \"\"\"\n\n        critic_loss_input = self._critic_loss_input(update_runner, traj_batch)\n        critic_epoch_runner = (update_runner.critic_training, critic_loss_input)\n        critic_epoch_runner = lax.fori_loop(0, self.config.critic_epochs, self._critic_epoch, critic_epoch_runner)\n        critic_training, _ = critic_epoch_runner\n\n        critic_targets = critic_loss_input[1].reshape(-1, self.config.rollout_length)\n        critic_loss = self._critic_loss(critic_training, traj_batch.obs, critic_targets, update_runner.hyperparams)\n\n        return critic_training, critic_loss\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _update_step(self, i_update_step: int, update_runner: Runner) -&gt; Runner:\n        \"\"\"\n        An update step of the actor and critic networks. This entails:\n        - performing rollout for sampling a batch of trajectories.\n        - assessing the value of the last state per trajectory using the critic.\n        - evaluating the advantage per trajectory.\n        - updating the actor and critic network parameters via the respective loss functions.\n        - generating in-training performance metrics.\n        In this approach, the update_runner already has a batch of environments initialized. The environments are not\n        initialized in the beginning of every update step, which means that trajectories to not necessarily start from\n        an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan\n        for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be\n        truncated in trajectory sampling).\n        :param i_update_step: Unused, required for progressbar.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :return: The updated runner\n        \"\"\"\n\n        rollout_runners = self._make_rollout_runners(update_runner)\n        scan_rollout_fn = lambda x: lax.scan(self._rollout, x, None, self.config.rollout_length)\n        rollout_runners, traj_batch = jax.vmap(scan_rollout_fn)(rollout_runners)\n        last_envstate, last_obs, _, _, rng = rollout_runners\n        traj_batch = self._process_trajectory(update_runner, traj_batch, last_obs)\n\n        actor_training, actor_loss = self._actor_update(update_runner, traj_batch)\n        critic_training, critic_loss = self._critic_update(update_runner, traj_batch)\n\n        \"\"\"Update runner as a dataclass.\"\"\"\n        update_runner = update_runner.replace(\n            envstate=last_envstate,\n            obs=last_obs,\n            actor_training=actor_training,\n            critic_training=critic_training,\n            rng=rng,\n            actor_loss=jnp.expand_dims(actor_loss, axis=-1),\n            critic_loss=jnp.expand_dims(critic_loss, axis=-1)\n        )\n\n        return update_runner\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _checkpoint(self, update_runner: Runner, metrics: Dict[str, Float[Array, \"1\"]], i_training_step: int) -&gt; None:\n        \"\"\"\n        Wraps the base checkpointing method in a Python callback.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n        :param i_training_step: Training step\n        :return:\n        \"\"\"\n\n        jax.experimental.io_callback(self._checkpoint_base, None, update_runner, metrics, i_training_step)\n\n    def _checkpoint_base(\n            self,\n            update_runner: Runner,\n            metrics: Dict[str, Float[Array, \"1\"]],\n            i_training_step: int\n    ) -&gt; None:\n        \"\"\"\n        Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following:\n        - The training runner object.\n        - Returns of the evaluation episodes\n        The average return over the evaluated episodes is used as the checkpoint metric.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param metrics: Dictionary of evaluation metrics (return per episode evaluation)\n        :param i_training_step: Training step\n        :return:\n        \"\"\"\n\n        if self.checkpointing:\n\n            ckpt = {\n                \"runner\": update_runner,\n                \"terminated\": metrics[\"terminated\"],\n                \"truncated\": metrics[\"truncated\"],\n                \"final_rewards\": metrics[\"final_rewards\"],\n                \"returns\": metrics[\"returns\"]\n            }\n\n            save_args = orbax_utils.save_args_from_target(ckpt)\n\n            self.checkpoint_manager.save(\n                # Use maximum number of steps reached in previous training. Set to zero by default during agent\n                # initialization if a new training is executed. In case of continuing training, the checkpoint of step\n                # zero replaces the last checkpoint of the previous training. The two checkpoints are the same.\n                i_training_step+self.previous_training_max_step,\n                ckpt,\n                save_kwargs={'save_args': save_args},\n            )\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _training_step(\n            self,\n            update_runner: Runner,\n            i_training_batch: int\n    ) -&gt; Tuple[Runner, Dict[str, Float[Array, \"1\"]]]:\n        \"\"\"\n        Performs trainings steps to update the agent per training batch.\n        :param update_runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters.\n        :param i_training_batch: Training batch loop counter.\n        :return: tuple with updated runner and dictionary of metrics.\n        \"\"\"\n\n        n_training_steps = self.config.n_steps - self.config.n_steps // self.config.eval_frequency * i_training_batch\n        n_training_steps = jnp.clip(n_training_steps, 1, self.config.eval_frequency)\n\n        update_runner = lax.fori_loop(0, n_training_steps, self._update_step, update_runner)\n\n        if self.eval_during_training:\n            metrics = self._generate_metrics(runner=update_runner, update_step=i_training_batch)\n            i_training_step = self.config.eval_frequency * (i_training_batch + 1)\n            i_training_step = jnp.minimum(i_training_step, self.config.n_steps)\n            if self.checkpointing:\n                self._checkpoint(update_runner, metrics, i_training_step)\n        else:\n            metrics = {}\n\n        return update_runner, metrics\n\n    @partial(jax.jit, static_argnums=(0,))\n    def train(self, rng: PRNGKeyArray, hyperparams: HyperParameters) -&gt; Tuple[Runner, Dict[str, Float[Array, \"1\"]]]:\n        \"\"\"\n        Trains the agent. A jax_tqdm progressbar has been added in the lax.scan loop.\n        :param rng: Random key for initialization. This is the original key for training.\n        :param hyperparams: An instance of HyperParameters for training.\n        :return: The final state of the step runner after training and the training metrics accumulated over all\n                 training batches and steps.\n        \"\"\"\n\n        rng, *_rng = jax.random.split(rng, 4)\n        actor_init_rng, critic_init_rng, runner_rng = _rng\n\n        actor_training = self._create_training(\n            actor_init_rng, self.config.actor_network, hyperparams.actor_optimizer_params\n        )\n        critic_training = self._create_training(\n            critic_init_rng, self.config.critic_network, hyperparams.critic_optimizer_params\n        )\n\n        update_runner = self._create_update_runner(runner_rng, actor_training, critic_training, hyperparams)\n\n        # Checkpoint initial state\n        if self.eval_during_training:\n            metrics_start = self._generate_metrics(runner=update_runner, update_step=0)\n            if self.checkpointing:\n                self._checkpoint(update_runner, metrics_start, self.previous_training_max_step)\n\n        # Initialize agent updating functions, which can be avoided to be done within the training loops.\n        actor_grad_fn = jax.grad(self._actor_loss, has_aux=True, allow_int=True)\n        self._actor_minibatch_fn = lambda x, y: self._actor_minibatch_update(x, y, actor_grad_fn)\n\n        critic_grad_fn = jax.grad(self._critic_loss, allow_int=True)\n        self._critic_minibatch_fn = lambda x, y: self._critic_minibatch_update(x, y, critic_grad_fn)\n\n        # Train, evaluate, checkpoint\n\n        n_training_batches = self.config.n_steps // self.config.eval_frequency\n        progressbar_desc = f'Training batch (training steps = batch x {self.config.eval_frequency})'\n\n        runner, metrics = lax.scan(\n            scan_tqdm(n_training_batches, desc=progressbar_desc)(self._training_step),\n            update_runner,\n            jnp.arange(n_training_batches),\n            n_training_batches\n        )\n\n        if self.eval_during_training:\n            metrics = {\n                key: jnp.concatenate((metrics_start[key][jnp.newaxis, :], metrics[key]), axis=0)\n                for key in metrics.keys()\n            }\n        else:\n            metrics= {}\n\n        return runner, metrics\n\n    @abstractmethod\n    def _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the returns per episode step over a batch of trajectories.\n        :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n         state).\n        :param traj: The trajectory batch.\n        :return: A tuple of returns.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _trajectory_advantages(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the advantages per episode step over a batch of trajectories.\n        :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n         state).\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _actor_loss(\n            self,\n            training: TrainState,\n            obs: Float[Array, \"n_rollout batch_size obs_size\"],\n            action: Float[Array, \"n_rollout batch_size\"],\n            log_prob_old: Float[Array, \"n_rollout batch_size\"],\n            advantage: ReturnsType,\n            hyperparams: HyperParameters\n    )-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n        \"\"\"\n        Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n        discounted returns and the value as estimated by the critic.\n        :param training: The actor TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param action: The actions in the trajectory batch.\n        :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n        :param advantage: The advantage over the trajectory batch.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _critic_loss(\n            self,\n            training: TrainState,\n            obs: Float[Array, \"n_rollout batch_size obs_size\"],\n            targets: Float[Array, \"batch_size n_rollout\"],\n            hyperparams: HyperParameters\n    ) -&gt; float:\n        \"\"\"\n        Calculates the critic loss.\n        :param training: The critic TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param targets: The returns over the trajectory batch, which act as the targets for training the critic.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: The critic loss.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[ActorLossInputType]:\n        \"\"\"\n        Prepares the input required by the actor loss function. The input is reshaped so that it is split into\n        minibatches.\n        :param update_runner: The runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the actor loss function.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n        \"\"\"\n        Prepares the input required by the critic loss function. The input is reshaped so that it is split into\n        minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the critic loss function.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def _entropy(self, training: TrainState, obs: ObsType)-&gt; Float[Array, \"1\"]:\n        raise NotImplemented\n\n    @abstractmethod\n    def _log_prob(\n            self,\n            training: TrainState,\n            params: FrozenDict,\n            obs: ObsType,\n            action: ActionType\n    ) -&gt; Float[Array, \"n_actors\"]:\n        raise NotImplemented\n\n    @abstractmethod\n    def _sample_action(\n            self,\n            rng: PRNGKeyArray,\n            training: TrainState,\n            obs: ObsType\n    ) -&gt; ActionType:\n        raise NotImplemented\n\n    \"\"\" METHODS FOR APPLYING AGENT\"\"\"\n\n    @abstractmethod\n    def policy(self, training: TrainState, obs: ObsType) -&gt; ActionType:\n        \"\"\"\n        Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state.\n        :param obs: The current obs of the episode step in array format.\n        :return:\n        \"\"\"\n        raise NotImplemented\n\n    \"\"\" METHODS FOR PERFORMANCE EVALUATION \"\"\"\n\n    def _eval_agent(\n            self,\n            rng: PRNGKeyArray,\n            actor_training: TrainState,\n            critic_training: TrainState,\n            n_episodes: int = 1\n    ) -&gt; Dict[str, Float[Array, \"1\"] | Bool[Array, \"1\"]]:\n        \"\"\"\n        Evaluates the agents for n_episodes complete episodes using 'lax.while_loop'.\n        :param rng: A random key used for evaluating the agent.\n        :param actor_training: The actor TrainState object (either mid- or post-training).\n        :param critic_training: The critic TrainState object (either mid- or post-training).\n        :param n_episodes: The update_runner object used during training.\n        :return: The sum of rewards collected over n_episodes episodes.\n        \"\"\"\n\n        rng_eval = jax.random.split(rng, n_episodes)\n        rng, obs, envstate = jax.vmap(self.env_reset)(rng_eval)\n\n        eval_runner = (\n            envstate,\n            obs,\n            actor_training,\n            jnp.zeros(1, dtype=jnp.bool).squeeze(),\n            jnp.zeros(1, dtype=jnp.bool).squeeze(),\n            jnp.zeros(1).squeeze(),\n            jnp.zeros(1).squeeze(),\n            rng,\n        )\n        eval_runners = jax.vmap(\n            lambda s, t, u, v, w, x, y, z: (s, t, u, v, w, x, y, z),\n            in_axes=(0, 0, None, None, None, None, None, 0)\n        )(*eval_runner)\n\n        eval_runner = jax.vmap(lambda x: lax.while_loop(self._eval_cond, self._eval_body, x))(eval_runners)\n        _, _, _, terminated, truncated, final_rewards, returns, _ = eval_runner\n\n        return self._eval_metrics(terminated, truncated, final_rewards, returns)\n\n    def _eval_metrics(\n            self,\n            terminated: Bool[Array, \"1\"],\n            truncated: Bool[Array, \"1\"],\n            final_rewards: Float[Array, \"1\"],\n            returns: Float[Array, \"1\"]\n    ) -&gt; Dict[str, Float[Array, \"1\"] | Bool[Array, \"1\"]]:\n        \"\"\"\n        Evaluate the metrics.\n        :param terminated: Whether the episode finished by termination.\n        :param truncated: Whether the episode finished by truncation.\n        :param final_rewards: The rewards collected in the final step of the episode.\n        :param returns: The sum of rewards collected during the episode.\n        :return: Dictionary combining the input arguments and the case-specific special metrics.\n        \"\"\"\n        metrics = {\n            \"terminated\": terminated,\n            \"truncated\": truncated,\n            \"final_rewards\": final_rewards,\n            \"returns\": returns\n        }\n\n        return metrics\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _eval_body(self, eval_runner: EvalRunnerType) -&gt; EvalRunnerType:\n        \"\"\"\n        A step in the episode to be used with 'lax.while_loop' for evaluation of the agent in a complete episode.\n        :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n        states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n        over the episode and a random key.\n        :return: The updated eval_runner tuple.\n        \"\"\"\n\n        envstate, obs, actor_training, terminated, truncated, reward, returns, rng = eval_runner\n\n        action = self.policy(actor_training, obs)\n\n        rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, action)\n\n        terminated = info[\"terminated\"]\n        truncated = info[\"truncated\"]\n\n        returns += reward\n\n        eval_runner = (next_envstate, next_obs, actor_training, terminated, truncated, reward, returns, rng)\n\n        return eval_runner\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _eval_cond(self, eval_runner: EvalRunnerType) -&gt; Bool[Array, \"1\"]:\n        \"\"\"\n        Checks whether the episode is terminated, meaning that the 'lax.while_loop' can stop.\n        :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n        states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n        over the episode and a random key.\n        :return: Whether the episode is terminated, which means that the while loop must stop.\n        \"\"\"\n\n        _, _, _, terminated, truncated, _, _, _ = eval_runner\n        return jnp.logical_and(jnp.logical_not(terminated), jnp.logical_not(truncated))\n\n    def eval(self, rng: PRNGKeyArray, n_evals: int = 32) -&gt; Float[Array, \"n_evals\"]:\n        \"\"\"\n        Evaluates the trained agent's performance post-training using the trained agent's actor and critic.\n        :param rng: Random key for evaluation.\n        :param n_evals: Number of steps in agent evaluation.\n        :return: Dictionary of evaluation metrics.\n        \"\"\"\n\n        eval_metrics = self._eval_agent(rng, self.actor_training, self.critic_training, n_evals)\n\n        return eval_metrics\n\n    \"\"\" METHODS FOR POST-PROCESSING \"\"\"\n\n    def log_hyperparams(self, hyperparams: HyperParameters) -&gt; None:\n        \"\"\"\n        Logs training hyperparameters in a text file. To be used outside training.\n        :param hyperparams: An instance of HyperParameters for training.\n        :return:\n        \"\"\"\n\n        output_lst = [field + ': ' + str(getattr(hyperparams, field)) for field in hyperparams._fields]\n        output_lst = ['Hyperparameters:'] + output_lst\n        output_lst = '\\n'.join(output_lst)\n\n        if self.checkpointing:\n            with open(os.path.join(self.config.checkpoint_dir, 'hyperparameters.txt'), \"w\") as f:\n                f.write(output_lst)\n\n    def collect_training(\n            self,\n            runner: Optional[Runner] = None,\n            metrics: Optional[Dict[str, Float[Array, \"1\"]]] = None,\n            previous_training_max_step: int = 0\n    ) -&gt; None:\n        \"\"\"\n        Collects training or restored checkpoint of output (the final state of the runner after training and the\n        collected metrics).\n        :param runner: The runner object, containing information about the current status of the actor's/\n        critic's training, the state of the environment and training hyperparameters. This is at the state reached at\n        the end of training.\n        :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n        :param previous_training_max_step: Maximum step reached during training.\n        :return:\n        \"\"\"\n\n        self.agent_trained = True\n        self.previous_training_max_step = previous_training_max_step\n        self.training_runner = runner\n        self.training_metrics = metrics\n        n_evals = list(metrics.values())[0].shape[0]\n        self.eval_steps_in_training = jnp.arange(n_evals) * self.config.eval_frequency\n        self._pp()\n\n    def _pp(self) -&gt; None:\n        \"\"\"\n        Post-processes the training results, which includes:\n            - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored).\n        :return:\n        \"\"\"\n\n        self.actor_training = self.training_runner.actor_training\n        self.critic_training = self.training_runner.critic_training\n\n    def summarize(\n            self,\n            metrics: Annotated[NDArray[np.float32], \"size_metrics\"] | Float[Array, \"size_metrics\"]\n    ) -&gt; MetricStats:\n        \"\"\"\n        Summarizes collection of per-episode metrics.\n        :param metrics: Metric per episode.\n        :return: Summary of metric per episode.\n        \"\"\"\n\n        return MetricStats(\n            episode_metric=metrics,\n            mean=metrics.mean(axis=-1),\n            var=metrics.var(axis=-1),\n            std=metrics.std(axis=-1),\n            min=metrics.min(axis=-1),\n            max=metrics.max(axis=-1),\n            median=jnp.median(metrics, axis=-1),\n            has_nans=jnp.any(jnp.isnan(metrics), axis=-1)\n        )\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._actor_minibatch_fn","title":"<code>_actor_minibatch_fn</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._critic_minibatch_fn","title":"<code>_critic_minibatch_fn</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.actor_training","title":"<code>actor_training = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.agent_trained","title":"<code>agent_trained = False</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.critic_training","title":"<code>critic_training = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.eval_during_training","title":"<code>eval_during_training = eval_during_training</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.previous_training_max_step","title":"<code>previous_training_max_step = 0</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.training_metrics","title":"<code>training_metrics = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.training_runner","title":"<code>training_runner = None</code>  <code>class-attribute</code>","text":""},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.__init__","title":"<code>__init__(env, env_params, config, eval_during_training=True)</code>","text":"<p>:param env: A gymnax or custom environment that inherits from the basic gymnax class. :param env_params: A dataclass named \"EnvParams\" containing the parametrization of the environment. :param config: The configuration of the agent as and AgentConfig object (from vpf_utils).</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def __init__(\n        self,\n        env: Environment,\n        env_params: EnvParams,\n        config: AgentConfig,\n        eval_during_training: bool = True\n) -&gt; None:\n    \"\"\"\n    :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n    :param env_params: A dataclass named \"EnvParams\" containing the parametrization of the environment.\n    :param config: The configuration of the agent as and AgentConfig object (from vpf_utils).\n    \"\"\"\n\n    self.config = config\n    self.eval_during_training = eval_during_training\n    self._init_checkpointer()\n    self._init_env(env, env_params)\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.__str__","title":"<code>__str__()</code>","text":"<p>Returns a string containing only the non-default field values.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string containing only the non-default field values.\n    \"\"\"\n\n    output_lst = [field + ': ' + str(getattr(self.config, field)) for field in self.config._fields]\n    output_lst = ['Agent configuration:'] + output_lst\n\n    return '\\n'.join(output_lst)\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._actor_epoch","title":"<code>_actor_epoch(epoch_runner)</code>","text":"<p>Performs a Gradient Ascent update of the actor. :param epoch_runner: A tuple containing the following information about the update: - actor_training: TrainState object for actor training - actor_loss_input: tuple with the inputs required by the actor loss function. - kl: The KL divergence collected during the update (used in checking for early stopping). - epoch: The number of the current training epoch. - kl_threshold: The KL divergence threshold for early stopping. :return: The updated epoch runner.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_epoch(\n        self,\n        epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]\n) -&gt; Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]:\n    \"\"\"\n    Performs a Gradient Ascent update of the actor.\n    :param epoch_runner: A tuple containing the following information about the update:\n    - actor_training: TrainState object for actor training\n    - actor_loss_input: tuple with the inputs required by the actor loss function.\n    - kl: The KL divergence collected during the update (used in checking for early stopping).\n    - epoch: The number of the current training epoch.\n    - kl_threshold: The KL divergence threshold for early stopping.\n    :return: The updated epoch runner.\n    \"\"\"\n\n    actor_training, actor_loss_input, kl, epoch, kl_threshold = epoch_runner\n    minibatch_runner = (actor_training, actor_loss_input, 0)\n    n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n    minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._actor_minibatch_fn, minibatch_runner)\n    actor_training, _, kl = minibatch_runner\n\n    return actor_training, actor_loss_input, kl, epoch+1, kl_threshold\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._actor_loss","title":"<code>_actor_loss(training, obs, action, log_prob_old, advantage, hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the discounted returns and the value as estimated by the critic. :param training: The actor TrainState object. :param obs: The obs in the trajectory batch. :param action: The actions in the trajectory batch. :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch. :param advantage: The advantage over the trajectory batch. :param hyperparams: The HyperParameters object used for training. :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _actor_loss(\n        self,\n        training: TrainState,\n        obs: Float[Array, \"n_rollout batch_size obs_size\"],\n        action: Float[Array, \"n_rollout batch_size\"],\n        log_prob_old: Float[Array, \"n_rollout batch_size\"],\n        advantage: ReturnsType,\n        hyperparams: HyperParameters\n)-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n    \"\"\"\n    Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n    discounted returns and the value as estimated by the critic.\n    :param training: The actor TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param action: The actions in the trajectory batch.\n    :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n    :param advantage: The advantage over the trajectory batch.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._actor_loss_input","title":"<code>_actor_loss_input(update_runner, traj_batch)</code>  <code>abstractmethod</code>","text":"<p>Prepares the input required by the actor loss function. The input is reshaped so that it is split into minibatches. :param update_runner: The runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the actor loss function.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[ActorLossInputType]:\n    \"\"\"\n    Prepares the input required by the actor loss function. The input is reshaped so that it is split into\n    minibatches.\n    :param update_runner: The runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the actor loss function.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._actor_minibatch_update","title":"<code>_actor_minibatch_update(i_minibatch, minibatch_runner, grad_fn)</code>  <code>staticmethod</code>","text":"<p>Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn. :param i_minibatch: Number of minibatch update. :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence. :param grad_fn: The gradient function of the training loss. :return: Minibatch runner with an updated TrainState.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@staticmethod\ndef _actor_minibatch_update(\n        i_minibatch: int,\n        minibatch_runner: Tuple[TrainState, ActorLossInputType, float],\n        grad_fn: Callable[[Any], ActorLossInputType]\n) -&gt; Annotated[Tuple[TrainState, ActorLossInputType, float], \"n_minibatch\"]:\n    \"\"\"\n    Performs a minibatch update of the actor network. Not jitted, so that the grad_fn argument can be\n    passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n    :param i_minibatch: Number of minibatch update.\n    :param minibatch_runner: A tuple containing the TranState object, the loss input arguments and the KL divergence.\n    :param grad_fn: The gradient function of the training loss.\n    :return: Minibatch runner with an updated TrainState.\n    \"\"\"\n\n    actor_training, actor_loss_input, kl = minibatch_runner\n    *traj_batch, hyperparams = actor_loss_input\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n    grad_input_minibatch = (actor_training, *traj_minibatch, hyperparams)\n    grads, kl = grad_fn(*grad_input_minibatch)\n    actor_training = actor_training.apply_gradients(grads=grads.params)\n    return actor_training, actor_loss_input, kl\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._actor_training_cond","title":"<code>_actor_training_cond(epoch_runner)</code>","text":"<p>Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been met or due to KL divergence early stopping). :param epoch_runner: A tuple containing the following information about the update: - actor_training: TrainState object for actor training - actor_loss_input: tuple with the inputs required by the actor loss function. - kl: The KL divergence collected during the update (used in checking for early stopping). - epoch: The number of the current training epoch. - kl_threshold: The KL-divergence threshold for early stopping. :return: Whether the lax.while_loop over training epochs finishes.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_training_cond(\n        self,\n        epoch_runner: Tuple[TrainState, ActorLossInputType, Float[Array, \"1\"], int, float]\n) -&gt; Bool[Array, \"1\"]:\n    \"\"\"\n    Checks whether the lax.while_loop over epochs should be terminated (either because the number of epochs has been\n    met or due to KL divergence early stopping).\n    :param epoch_runner: A tuple containing the following information about the update:\n    - actor_training: TrainState object for actor training\n    - actor_loss_input: tuple with the inputs required by the actor loss function.\n    - kl: The KL divergence collected during the update (used in checking for early stopping).\n    - epoch: The number of the current training epoch.\n    - kl_threshold: The KL-divergence threshold for early stopping.\n    :return: Whether the lax.while_loop over training epochs finishes.\n    \"\"\"\n\n    _, _, kl, epoch, kl_threshold = epoch_runner\n    return jnp.logical_and(\n        jnp.less(epoch, self.config.actor_epochs),\n        jnp.less_equal(kl, kl_threshold)\n    )\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._actor_update","title":"<code>_actor_update(update_runner, traj_batch)</code>","text":"<p>Prepares the input and performs Gradient Ascent for the actor network. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param traj_batch: The batch of trajectories. :return: The actor training object updated after actor_epochs steps of Gradient Ascent.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _actor_update(self, update_runner: Runner, traj_batch: Transition) -&gt; Tuple[TrainState, Float[Array, \"1\"]]:\n    \"\"\"\n    Prepares the input and performs Gradient Ascent for the actor network.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param traj_batch: The batch of trajectories.\n    :return: The actor training object updated after actor_epochs steps of Gradient Ascent.\n    \"\"\"\n\n    actor_loss_input = self._actor_loss_input(update_runner, traj_batch)\n\n    start_kl, start_epoch = -jnp.inf, 1\n    actor_epoch_runner = (\n        update_runner.actor_training,\n        actor_loss_input,\n        start_kl,\n        start_epoch,\n        update_runner.hyperparams.kl_threshold\n    )\n    actor_epoch_runner = lax.while_loop(self._actor_training_cond, self._actor_epoch, actor_epoch_runner)\n    actor_training, _, _, _, _ = actor_epoch_runner\n\n    actor_loss, _ = self._actor_loss(\n        actor_training,\n        traj_batch.obs,\n        traj_batch.action,\n        traj_batch.log_prob,\n        traj_batch.advantage,\n        update_runner.hyperparams\n    )\n\n    return actor_training, actor_loss\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._add_advantages","title":"<code>_add_advantages(traj_batch, advantage)</code>","text":"<p>Simply inputs the advantages in the batch of trajectories. :param traj_batch: The batch of trajectories. :param advantage: The advantage over the trajectory batch. :return: The batch of trajectories with the updated advantage.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _add_advantages(self, traj_batch: Transition, advantage: ReturnsType) -&gt; Transition:\n    \"\"\"\n    Simply inputs the advantages in the batch of trajectories.\n    :param traj_batch: The batch of trajectories.\n    :param advantage: The advantage over the trajectory batch.\n    :return: The batch of trajectories with the updated advantage.\n    \"\"\"\n    traj_batch = traj_batch._replace(advantage=advantage)\n\n    return traj_batch\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._add_next_values","title":"<code>_add_next_values(traj_batch, last_obs, critic_training)</code>","text":"<p>Concatenates all state values but the first one with the value estimate of the final state, to represent the values of the next state (1-step lag). :param traj_batch: The batch of trajectories. :param last_obs: The obs at the end of every trajectory in the batch. :param critic_training: The critic TrainState object (either mid- or post-training). :return: The batch of trajectories with the updated next-state values.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _add_next_values(\n        self,\n        traj_batch: Transition,\n        last_obs: ObsType,\n        critic_training: TrainState\n) -&gt; Transition:\n    \"\"\"\n    Concatenates all state values but the first one with the value estimate of the final state, to represent the\n    values of the next state (1-step lag).\n    :param traj_batch: The batch of trajectories.\n    :param last_obs: The obs at the end of every trajectory in the batch.\n    :param critic_training: The critic TrainState object (either mid- or post-training).\n    :return: The batch of trajectories with the updated next-state values.\n    \"\"\"\n    last_state_value_vmap = jax.vmap(critic_training.apply_fn, in_axes=(None, 0))\n    last_state_value = last_state_value_vmap(lax.stop_gradient(critic_training.params), last_obs)\n\n    \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n    next_values_t = jnp.concatenate(\n        [traj_batch.value.squeeze(), last_state_value[..., jnp.newaxis]],\n        axis=-1)[:, 1:]\n\n    traj_batch = traj_batch._replace(next_value=next_values_t)\n\n    return traj_batch\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._advantages","title":"<code>_advantages(traj_batch, gamma, gae_lambda)</code>","text":"<p>Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling step, trajectories do not start at the initial state. :param traj_batch: The batch of trajectories. :param last_next_state_value: The value of the last next state in each trajectory. :param gamma: Discount factor :param gae_lambda: The GAE \u03bb factor. :return: The returns over the episodes of the trajectory batch.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _advantages(\n        self,\n        traj_batch: Transition,\n        gamma: float,\n        gae_lambda: float\n) -&gt; ReturnsType:\n    \"\"\"\n    Calculates the advantage of every step in the trajectory batch. To do so, it identifies episodes in the\n    trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n    episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n    step, trajectories do not start at the initial state.\n    :param traj_batch: The batch of trajectories.\n    :param last_next_state_value: The value of the last next state in each trajectory.\n    :param gamma: Discount factor\n    :param gae_lambda: The GAE \u03bb factor.\n    :return: The returns over the episodes of the trajectory batch.\n    \"\"\"\n\n    rewards_t = traj_batch.reward.squeeze()\n    values_t = traj_batch.value.squeeze()\n    terminated_t = traj_batch.terminated.squeeze()\n    next_state_values_t = traj_batch.next_value.squeeze()\n    gamma_t = jnp.ones_like(terminated_t) * gamma\n    gae_lambda_t = jnp.ones_like(terminated_t) * gae_lambda\n\n    rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t = jax.tree_util.tree_map(\n        lambda x: jnp.swapaxes(x, 0, 1),\n        (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n    )\n\n    traj_runner = (rewards_t, values_t, next_state_values_t, terminated_t, gamma_t, gae_lambda_t)\n    \"\"\"\n    TODO:\n    Advantage of last step is taken from the critic, in contrast to traditional approaches, where the rollout \n    ends with episode termination and the advantage is zero. Training is still successful and the influence of this\n    implementation choice is negligible.\n    \"\"\"\n    end_advantage = jnp.zeros(self.config.batch_size)\n    _, advantages = jax.lax.scan(self._trajectory_advantages, end_advantage, traj_runner, reverse=True)\n\n    advantages = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), advantages)\n\n    return advantages\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._checkpoint","title":"<code>_checkpoint(update_runner, metrics, i_training_step)</code>","text":"<p>Wraps the base checkpointing method in a Python callback. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param metrics: Dictionary of evaluation metrics (return per environment evaluation) :param i_training_step: Training step :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _checkpoint(self, update_runner: Runner, metrics: Dict[str, Float[Array, \"1\"]], i_training_step: int) -&gt; None:\n    \"\"\"\n    Wraps the base checkpointing method in a Python callback.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n    :param i_training_step: Training step\n    :return:\n    \"\"\"\n\n    jax.experimental.io_callback(self._checkpoint_base, None, update_runner, metrics, i_training_step)\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._checkpoint_base","title":"<code>_checkpoint_base(update_runner, metrics, i_training_step)</code>","text":"<p>Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following: - The training runner object. - Returns of the evaluation episodes The average return over the evaluated episodes is used as the checkpoint metric. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param metrics: Dictionary of evaluation metrics (return per episode evaluation) :param i_training_step: Training step :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _checkpoint_base(\n        self,\n        update_runner: Runner,\n        metrics: Dict[str, Float[Array, \"1\"]],\n        i_training_step: int\n) -&gt; None:\n    \"\"\"\n    Implements checkpointing, to be wrapped in a Python callback. Checkpoints the following:\n    - The training runner object.\n    - Returns of the evaluation episodes\n    The average return over the evaluated episodes is used as the checkpoint metric.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param metrics: Dictionary of evaluation metrics (return per episode evaluation)\n    :param i_training_step: Training step\n    :return:\n    \"\"\"\n\n    if self.checkpointing:\n\n        ckpt = {\n            \"runner\": update_runner,\n            \"terminated\": metrics[\"terminated\"],\n            \"truncated\": metrics[\"truncated\"],\n            \"final_rewards\": metrics[\"final_rewards\"],\n            \"returns\": metrics[\"returns\"]\n        }\n\n        save_args = orbax_utils.save_args_from_target(ckpt)\n\n        self.checkpoint_manager.save(\n            # Use maximum number of steps reached in previous training. Set to zero by default during agent\n            # initialization if a new training is executed. In case of continuing training, the checkpoint of step\n            # zero replaces the last checkpoint of the previous training. The two checkpoints are the same.\n            i_training_step+self.previous_training_max_step,\n            ckpt,\n            save_kwargs={'save_args': save_args},\n        )\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._create_empty_trainstate","title":"<code>_create_empty_trainstate(network)</code>","text":"<p>Creates an empty TrainState object for restoring checkpoints. :param network: The actor or critic network. :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _create_empty_trainstate(self, network) -&gt; TrainState:\n    \"\"\"\n    Creates an empty TrainState object for restoring checkpoints.\n    :param network: The actor or critic network.\n    :return:\n    \"\"\"\n\n    rng = jax.random.PRNGKey(1)  # Just a dummy PRNGKey for initializing the networks parameters.\n    network, params = self._init_network(rng, network)\n\n    optimizer_params = OptimizerParams()  # Use the default values of the OptimizerParams object.\n    tx = self._init_optimizer(optimizer_params)\n\n    empty_training = TrainState.create(apply_fn=network.apply, params=params, tx=tx)\n\n    return empty_training\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._create_training","title":"<code>_create_training(rng, network, optimizer_params)</code>","text":"<p>Creates a TrainState object for the actor or the critic. :param rng: Random key for initialization. :param network: The actor or critic network. :param optimizer_params: A NamedTuple containing the parametrization of the optimizer. :return: A TrainState object to be used in training the actor and cirtic networks.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _create_training(\n        self,\n        rng: PRNGKeyArray,\n        network: type[flax.linen.Module],\n        optimizer_params: OptimizerParams\n)-&gt; TrainState:\n    \"\"\"\n     Creates a TrainState object for the actor or the critic.\n    :param rng: Random key for initialization.\n    :param network: The actor or critic network.\n    :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n    :return: A TrainState object to be used in training the actor and cirtic networks.\n    \"\"\"\n\n    network, params = self._init_network(rng, network)\n    tx = self._init_optimizer(optimizer_params)\n    return TrainState.create(apply_fn=network.apply, tx=tx, params=params)\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._create_update_runner","title":"<code>_create_update_runner(rng, actor_training, critic_training, hyperparams)</code>","text":"<p>Initializes the update runner as a Runner object. The runner contains n_evals initializations of the environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and one for the critic network, so that trajectory batches are used to train the same parameters. :param rng: Random key for initialization. :param actor_training: The actor TrainState object used in training. :param critic_training: The critic TrainState object used in training. :param hyperparams: An instance of HyperParameters for training. :return: An update runner object to be used in trajectory sampling and training.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _create_update_runner(\n        self,\n        rng: PRNGKeyArray,\n        actor_training: TrainState,\n        critic_training: TrainState,\n        hyperparams: HyperParameters\n) -&gt; Runner:\n    \"\"\"\n    Initializes the update runner as a Runner object. The runner contains n_evals initializations of the\n    environment, which are used for sampling trajectories. The update runner has one TrainState for the actor and\n    one for the critic network, so that trajectory batches are used to train the same parameters.\n    :param rng: Random key for initialization.\n    :param actor_training: The actor TrainState object used in training.\n    :param critic_training: The critic TrainState object used in training.\n    :param hyperparams: An instance of HyperParameters for training.\n    :return: An update runner object to be used in trajectory sampling and training.\n    \"\"\"\n\n    rng, reset_rng, runner_rng = jax.random.split(rng, 3)\n    reset_rngs = jax.random.split(reset_rng, self.config.batch_size)\n    runner_rngs = jax.random.split(runner_rng, self.config.batch_size)\n\n    _, obs, envstate = jax.vmap(self.env_reset)(reset_rngs)\n\n    update_runner = Runner(\n        actor_training=actor_training,\n        critic_training=critic_training,\n        envstate=envstate,\n        obs=obs,\n        rng=runner_rngs,\n        hyperparams=hyperparams,\n        actor_loss=jnp.zeros(1),\n        critic_loss=jnp.zeros(1),\n    )\n\n    return update_runner\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._critic_epoch","title":"<code>_critic_epoch(i_epoch, epoch_runner)</code>","text":"<p>Performs a Gradient Descent update of the critic. :param: i_epoch: The current training epoch (unused but required by lax.fori_loop). :param epoch_runner: A tuple containing the following information about the update: - critic_training: TrainState object for critic training - critic_loss_input: tuple with the inputs required by the critic loss function. :return: The updated epoch runner.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_epoch(\n        self,\n        i_epoch: int,\n        epoch_runner: Tuple[TrainState, CriticLossInputType]\n) -&gt; Tuple[TrainState, CriticLossInputType]:\n    \"\"\"\n    Performs a Gradient Descent update of the critic.\n    :param: i_epoch: The current training epoch (unused but required by lax.fori_loop).\n    :param epoch_runner: A tuple containing the following information about the update:\n    - critic_training: TrainState object for critic training\n    - critic_loss_input: tuple with the inputs required by the critic loss function.\n    :return: The updated epoch runner.\n    \"\"\"\n\n    critic_training, critic_loss_input = epoch_runner\n    minibatch_runner = (critic_training, critic_loss_input)\n    n_minibatch_updates = self.config.batch_size // self.config.minibatch_size\n    minibatch_runner = lax.fori_loop(0, n_minibatch_updates, self._critic_minibatch_fn, minibatch_runner)\n    critic_training, _ = minibatch_runner\n\n    return critic_training, critic_loss_input\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._critic_loss","title":"<code>_critic_loss(training, obs, targets, hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Calculates the critic loss. :param training: The critic TrainState object. :param obs: The obs in the trajectory batch. :param targets: The returns over the trajectory batch, which act as the targets for training the critic. :param hyperparams: The HyperParameters object used for training. :return: The critic loss.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _critic_loss(\n        self,\n        training: TrainState,\n        obs: Float[Array, \"n_rollout batch_size obs_size\"],\n        targets: Float[Array, \"batch_size n_rollout\"],\n        hyperparams: HyperParameters\n) -&gt; float:\n    \"\"\"\n    Calculates the critic loss.\n    :param training: The critic TrainState object.\n    :param obs: The obs in the trajectory batch.\n    :param targets: The returns over the trajectory batch, which act as the targets for training the critic.\n    :param hyperparams: The HyperParameters object used for training.\n    :return: The critic loss.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._critic_loss_input","title":"<code>_critic_loss_input(update_runner, traj_batch)</code>  <code>abstractmethod</code>","text":"<p>Prepares the input required by the critic loss function. The input is reshaped so that it is split into minibatches. :param update_runner: The Runner object used in training. :param traj_batch: The batch of trajectories. :return: A tuple of input to the critic loss function.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n    \"\"\"\n    Prepares the input required by the critic loss function. The input is reshaped so that it is split into\n    minibatches.\n    :param update_runner: The Runner object used in training.\n    :param traj_batch: The batch of trajectories.\n    :return: A tuple of input to the critic loss function.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._critic_minibatch_update","title":"<code>_critic_minibatch_update(i_minibatch, minibatch_runner, grad_fn)</code>  <code>staticmethod</code>","text":"<p>Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn. :param i_minibatch: Number of minibatch update. :param minibatch_runner: A tuple containing the TranState object and the loss input arguments. :param grad_fn: The gradient function of the training loss. :return: Minibatch runner with an updated TrainState.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@staticmethod\ndef _critic_minibatch_update(\n        i_minibatch: int,\n        minibatch_runner: Tuple[TrainState, CriticLossInputType],\n        grad_fn: Callable[[Any], CriticLossInputType]\n) -&gt; Tuple[TrainState, CriticLossInputType]:\n    \"\"\"\n    Performs a minibatch update of the critic network. Not jitted, so that the grad_fn argument can be\n    passed. This choice doesn't hurt performance. To be called using a lambda function for defining grad_fn.\n    :param i_minibatch: Number of minibatch update.\n    :param minibatch_runner: A tuple containing the TranState object and the loss input arguments.\n    :param grad_fn: The gradient function of the training loss.\n    :return: Minibatch runner with an updated TrainState.\n    \"\"\"\n\n    critic_training, critic_loss_input = minibatch_runner\n    *traj_batch, hyperparams = critic_loss_input\n    traj_minibatch = jax.tree_map(lambda x: jnp.take(x, i_minibatch, axis=0), traj_batch)\n    grad_input_minibatch = (critic_training, *traj_minibatch, hyperparams)\n    grads = grad_fn(*grad_input_minibatch)\n    critic_training = critic_training.apply_gradients(grads=grads.params)\n    return critic_training, critic_loss_input\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._critic_update","title":"<code>_critic_update(update_runner, traj_batch)</code>","text":"<p>Prepares the input and performs Gradient Descent for the critic network. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param traj_batch: The batch of trajectories. :return: The critic training object updated after actor_epochs steps of Gradient Ascent.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _critic_update(self, update_runner: Runner, traj_batch: Transition) -&gt;  Tuple[TrainState, Float[Array, \"1\"]]:\n    \"\"\"\n    Prepares the input and performs Gradient Descent for the critic network.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param traj_batch: The batch of trajectories.\n    :return: The critic training object updated after actor_epochs steps of Gradient Ascent.\n    \"\"\"\n\n    critic_loss_input = self._critic_loss_input(update_runner, traj_batch)\n    critic_epoch_runner = (update_runner.critic_training, critic_loss_input)\n    critic_epoch_runner = lax.fori_loop(0, self.config.critic_epochs, self._critic_epoch, critic_epoch_runner)\n    critic_training, _ = critic_epoch_runner\n\n    critic_targets = critic_loss_input[1].reshape(-1, self.config.rollout_length)\n    critic_loss = self._critic_loss(critic_training, traj_batch.obs, critic_targets, update_runner.hyperparams)\n\n    return critic_training, critic_loss\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._entropy","title":"<code>_entropy(training, obs)</code>  <code>abstractmethod</code>","text":"Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _entropy(self, training: TrainState, obs: ObsType)-&gt; Float[Array, \"1\"]:\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._eval_agent","title":"<code>_eval_agent(rng, actor_training, critic_training, n_episodes=1)</code>","text":"<p>Evaluates the agents for n_episodes complete episodes using 'lax.while_loop'. :param rng: A random key used for evaluating the agent. :param actor_training: The actor TrainState object (either mid- or post-training). :param critic_training: The critic TrainState object (either mid- or post-training). :param n_episodes: The update_runner object used during training. :return: The sum of rewards collected over n_episodes episodes.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _eval_agent(\n        self,\n        rng: PRNGKeyArray,\n        actor_training: TrainState,\n        critic_training: TrainState,\n        n_episodes: int = 1\n) -&gt; Dict[str, Float[Array, \"1\"] | Bool[Array, \"1\"]]:\n    \"\"\"\n    Evaluates the agents for n_episodes complete episodes using 'lax.while_loop'.\n    :param rng: A random key used for evaluating the agent.\n    :param actor_training: The actor TrainState object (either mid- or post-training).\n    :param critic_training: The critic TrainState object (either mid- or post-training).\n    :param n_episodes: The update_runner object used during training.\n    :return: The sum of rewards collected over n_episodes episodes.\n    \"\"\"\n\n    rng_eval = jax.random.split(rng, n_episodes)\n    rng, obs, envstate = jax.vmap(self.env_reset)(rng_eval)\n\n    eval_runner = (\n        envstate,\n        obs,\n        actor_training,\n        jnp.zeros(1, dtype=jnp.bool).squeeze(),\n        jnp.zeros(1, dtype=jnp.bool).squeeze(),\n        jnp.zeros(1).squeeze(),\n        jnp.zeros(1).squeeze(),\n        rng,\n    )\n    eval_runners = jax.vmap(\n        lambda s, t, u, v, w, x, y, z: (s, t, u, v, w, x, y, z),\n        in_axes=(0, 0, None, None, None, None, None, 0)\n    )(*eval_runner)\n\n    eval_runner = jax.vmap(lambda x: lax.while_loop(self._eval_cond, self._eval_body, x))(eval_runners)\n    _, _, _, terminated, truncated, final_rewards, returns, _ = eval_runner\n\n    return self._eval_metrics(terminated, truncated, final_rewards, returns)\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._eval_body","title":"<code>_eval_body(eval_runner)</code>","text":"<p>A step in the episode to be used with 'lax.while_loop' for evaluation of the agent in a complete episode. :param eval_runner: A tuple containing information about the environment state, the actor and critic training states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards over the episode and a random key. :return: The updated eval_runner tuple.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _eval_body(self, eval_runner: EvalRunnerType) -&gt; EvalRunnerType:\n    \"\"\"\n    A step in the episode to be used with 'lax.while_loop' for evaluation of the agent in a complete episode.\n    :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n    states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n    over the episode and a random key.\n    :return: The updated eval_runner tuple.\n    \"\"\"\n\n    envstate, obs, actor_training, terminated, truncated, reward, returns, rng = eval_runner\n\n    action = self.policy(actor_training, obs)\n\n    rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, action)\n\n    terminated = info[\"terminated\"]\n    truncated = info[\"truncated\"]\n\n    returns += reward\n\n    eval_runner = (next_envstate, next_obs, actor_training, terminated, truncated, reward, returns, rng)\n\n    return eval_runner\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._eval_cond","title":"<code>_eval_cond(eval_runner)</code>","text":"<p>Checks whether the episode is terminated, meaning that the 'lax.while_loop' can stop. :param eval_runner: A tuple containing information about the environment state, the actor and critic training states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards over the episode and a random key. :return: Whether the episode is terminated, which means that the while loop must stop.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _eval_cond(self, eval_runner: EvalRunnerType) -&gt; Bool[Array, \"1\"]:\n    \"\"\"\n    Checks whether the episode is terminated, meaning that the 'lax.while_loop' can stop.\n    :param eval_runner: A tuple containing information about the environment state, the actor and critic training\n    states, whether the episode is terminated (for checking the condition in 'lax.while_loop'), the sum of rewards\n    over the episode and a random key.\n    :return: Whether the episode is terminated, which means that the while loop must stop.\n    \"\"\"\n\n    _, _, _, terminated, truncated, _, _, _ = eval_runner\n    return jnp.logical_and(jnp.logical_not(terminated), jnp.logical_not(truncated))\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._eval_metrics","title":"<code>_eval_metrics(terminated, truncated, final_rewards, returns)</code>","text":"<p>Evaluate the metrics. :param terminated: Whether the episode finished by termination. :param truncated: Whether the episode finished by truncation. :param final_rewards: The rewards collected in the final step of the episode. :param returns: The sum of rewards collected during the episode. :return: Dictionary combining the input arguments and the case-specific special metrics.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _eval_metrics(\n        self,\n        terminated: Bool[Array, \"1\"],\n        truncated: Bool[Array, \"1\"],\n        final_rewards: Float[Array, \"1\"],\n        returns: Float[Array, \"1\"]\n) -&gt; Dict[str, Float[Array, \"1\"] | Bool[Array, \"1\"]]:\n    \"\"\"\n    Evaluate the metrics.\n    :param terminated: Whether the episode finished by termination.\n    :param truncated: Whether the episode finished by truncation.\n    :param final_rewards: The rewards collected in the final step of the episode.\n    :param returns: The sum of rewards collected during the episode.\n    :return: Dictionary combining the input arguments and the case-specific special metrics.\n    \"\"\"\n    metrics = {\n        \"terminated\": terminated,\n        \"truncated\": truncated,\n        \"final_rewards\": final_rewards,\n        \"returns\": returns\n    }\n\n    return metrics\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._generate_metrics","title":"<code>_generate_metrics(runner, update_step)</code>","text":"<p>Generates metrics for on-policy learning. The agent performance during training is evaluated by running n_evals episodes (until termination). The selected metric is the sum of rewards collected dring the episode. If the user selects not to generate metrics (leading to faster training), an empty dictinary is returned. :param runner: The update runner object, containing information about the current status of the actor's/critic's training, the state of the environment and training hyperparameters. :param update_step: The number of the update step. :return: A dictionary of the sum of rewards collected over 'n_evals' episodes, or empty dictionary.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _generate_metrics(self, runner: Runner, update_step: int) -&gt; Dict[str, Float[Array, \"1\"]]:\n    \"\"\"\n    Generates metrics for on-policy learning. The agent performance during training is evaluated by running\n    n_evals episodes (until termination). The selected metric is the sum of rewards collected dring the episode.\n    If the user selects not to generate metrics (leading to faster training), an empty dictinary is returned.\n    :param runner: The update runner object, containing information about the current status of the actor's/critic's\n    training, the state of the environment and training hyperparameters.\n    :param update_step: The number of the update step.\n    :return: A dictionary of the sum of rewards collected over 'n_evals' episodes, or empty dictionary.\n    \"\"\"\n\n    metric = {}\n    if self.eval_during_training:\n        metric = self._eval_agent(\n            self.config.eval_rng,\n            runner.actor_training,\n            runner.critic_training,\n            self.config.n_evals\n        )\n\n    metric.update({\n        \"actor_loss\": runner.actor_loss,\n        \"critic_loss\": runner.critic_loss\n    })\n\n    return metric\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._init_checkpointer","title":"<code>_init_checkpointer()</code>","text":"<p>Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If so, sets the checkpoint manager using orbax. :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _init_checkpointer(self) -&gt; None:\n    \"\"\"\n    Sets whether checkpointing should be performed, decided by whether a checkpoint directory has been provided. If\n    so, sets the checkpoint manager using orbax.\n    :return:\n    \"\"\"\n\n    self.checkpointing = self.config.checkpoint_dir is not None\n\n    if self.checkpointing:\n\n        if not self.config.restore_agent:\n\n            dir_exists = os.path.exists(self.config.checkpoint_dir)\n            if not dir_exists:\n                os.makedirs(self.config.checkpoint_dir)\n\n            dir_files = [\n                file for file in os.listdir(self.config.checkpoint_dir)\n                if os.path.isdir(os.path.join(self.config.checkpoint_dir, file))\n            ]\n            if len(dir_files) &gt; 0:\n                for file in dir_files:\n                    file_path = os.path.join(self.config.checkpoint_dir, file)\n                    shutil.rmtree(file_path)\n\n            # Log training configuration\n            with open(os.path.join(self.config.checkpoint_dir, 'training_configuration.txt'), \"w\") as f:\n                f.write(self.__str__())\n\n        orbax_checkpointer = orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler())\n\n        options = orbax.checkpoint.CheckpointManagerOptions(\n            create=True,\n            step_prefix='trainingstep',\n        )\n\n        self.checkpoint_manager = orbax.checkpoint.CheckpointManager(\n            self.config.checkpoint_dir,\n            orbax_checkpointer,\n            options\n        )\n\n    else:\n\n        self.checkpoint_manager = None\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._init_env","title":"<code>_init_env(env, env_params)</code>","text":"<p>Environment initialization. :param env: A gymnax or custom environment that inherits from the basic gymnax class. :param env_params: A dataclass containing the parametrization of the environment. :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _init_env(self, env: Environment, env_params: EnvParams) -&gt; None:\n    \"\"\"\n    Environment initialization.\n    :param env: A gymnax or custom environment that inherits from the basic gymnax class.\n    :param env_params: A dataclass containing the parametrization of the environment.\n    :return:\n    \"\"\"\n\n    env = TruncationWrapper(env, self.config.max_episode_steps)\n    # env = FlattenObservationWrapper(env)\n    # self.env = LogWrapper(env)\n    self.env = env\n    self.env_params = env_params\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._init_network","title":"<code>_init_network(rng, network)</code>","text":"<p>Initialization of the actor or critic network. :param rng: Random key for initialization. :param network: The actor or critic network. :return: A random key after splitting the input and the initial parameters of the policy network.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _init_network(\n        self,\n        rng: PRNGKeyArray,\n        network: flax.linen.Module\n) -&gt; Tuple[flax.linen.Module, FrozenDict]:\n    \"\"\"\n    Initialization of the actor or critic network.\n    :param rng: Random key for initialization.\n    :param network: The actor or critic network.\n    :return: A random key after splitting the input and the initial parameters of the policy network.\n    \"\"\"\n\n    # Initialize the agent networks. The number of actions is irrelevant for the Critic network, which should return\n    # a single value in the final layer. However, the network class should accept the number of actions as an\n    # argument, even if it isn't used.\n    network = network(self.config)\n\n    rng, *_rng = jax.random.split(rng, 3)\n    dummy_reset_rng, network_init_rng = _rng\n\n    dummy_obs, _ = self.env.reset(dummy_reset_rng, self.env_params)\n    init_x = jnp.zeros((1, dummy_obs.size))\n\n    params = network.init(network_init_rng, init_x)\n\n    return network, params\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._init_optimizer","title":"<code>_init_optimizer(optimizer_params)</code>","text":"<p>Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to initialize the appropriate optimizer. In this way, the optimizer can be initialized within the \"train\" method, and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary. :param optimizer_params: A NamedTuple containing the parametrization of the optimizer. :return: An optimizer in optax.chain.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _init_optimizer(self, optimizer_params: OptimizerParams) -&gt; optax.chain:\n    \"\"\"\n    Optimizer initialization. This method uses the optax optimizer function given in the agent configuration to\n    initialize the appropriate optimizer. In this way, the optimizer can be initialized within the \"train\" method,\n    and thus several combinations of its parameters can be ran with jax.vmap. Jit is neither possible nor necessary.\n    :param optimizer_params: A NamedTuple containing the parametrization of the optimizer.\n    :return: An optimizer in optax.chain.\n    \"\"\"\n\n    optimizer_params_dict = optimizer_params._asdict()  # Transform from NamedTuple to dict\n    optimizer_params_dict.pop('grad_clip', None)  # Remove 'grad_clip', since it is not part of the optimizer args.\n\n\n    \"\"\"\n    Get dictionary of optimizer parameters to pass in optimizer. The procedure preserves parameters that:\n        - are given in the OptimizerParams NamedTuple and are requested as args by the optimizer\n        - are requested as args by the optimizer and are given in the OptimizerParams NamedTuple\n    \"\"\"\n\n    optimizer_arg_names = self.config.optimizer.__code__.co_varnames  # List names of args of optimizer.\n\n    # Keep only the optimizer arg names that are also part of the OptimizerParams (dict from NamedTuple)\n    optimizer_arg_names = [\n        arg_name for arg_name in optimizer_arg_names if arg_name in list(optimizer_params_dict.keys())\n    ]\n    if len(optimizer_arg_names) == 0:\n        raise Exception(\n            \"The defined optimizer parameters do not include relevant arguments for this optimizer.\"\n            \"The optimizer has not been implemented yet. Define your own OptimizerParams object.\"\n        )\n\n    # Keep only the optimizer params that are arg names for the specific optimizer\n    optimizer_params_dict = {arg_name: optimizer_params_dict[arg_name] for arg_name in optimizer_arg_names}\n\n    # No need to scale by -1.0. 'TrainState.apply_gradients' is used for training, which subtracts the update.\n    tx = optax.chain(\n        optax.clip_by_global_norm(optimizer_params.grad_clip),\n        self.config.optimizer(**optimizer_params_dict)\n    )\n\n    return tx\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._log_prob","title":"<code>_log_prob(training, params, obs, action)</code>  <code>abstractmethod</code>","text":"Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _log_prob(\n        self,\n        training: TrainState,\n        params: FrozenDict,\n        obs: ObsType,\n        action: ActionType\n) -&gt; Float[Array, \"n_actors\"]:\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._make_rollout_runners","title":"<code>_make_rollout_runners(update_runner)</code>","text":"<p>Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner object and broadcasting the TrainState object for the critic and the network in the update_runner object to the same dimension. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :return: tuple with step runners to be used in rollout.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _make_rollout_runners(self, update_runner: Runner) -&gt; Tuple[StepRunnerType, ...]:\n    \"\"\"\n    Creates a rollout_runners tuple to be used in rollout by combining the batched environments in the update_runner\n    object and broadcasting the TrainState object for the critic and the network in the update_runner object to the\n    same dimension.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :return: tuple with step runners to be used in rollout.\n    \"\"\"\n\n    rollout_runner = (\n        update_runner.envstate,\n        update_runner.obs,\n        update_runner.actor_training,\n        update_runner.critic_training,\n        update_runner.rng,\n    )\n    rollout_runners = jax.vmap(\n        lambda v, w, x, y, z: (v, w, x, y, z), in_axes=(0, 0, None, None, 0)\n    )(*rollout_runner)\n    return rollout_runners\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._make_transition","title":"<code>_make_transition(obs, action, value, log_prob, reward, next_obs, terminated)</code>","text":"<p>Creates a transition object based on the input and output of an episode step. :param obs: The current obs of the episode step in array format. :param action: The action selected by the agent. :param value: The critic value of the obs. :param log_prob: The actor log-probability of the selected action. :param reward: The collected reward after executing the action. :param next_obs: The next obs of the episode step in array format. :param terminated: Episode termination. :return: A transition object storing information about the state before and after executing the episode step,          the executed action, the collected reward, episode termination and optional additional information.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _make_transition(\n        self,\n        obs: ObsType,\n        action: ActionType,\n        value: Float[Array, \"1\"],\n        log_prob: Float[Array, \"1\"],\n        reward: Float[Array, \"1\"],\n        next_obs: ObsType,\n        terminated: Bool[Array, \"1\"],\n) -&gt; Transition:\n    \"\"\"\n    Creates a transition object based on the input and output of an episode step.\n    :param obs: The current obs of the episode step in array format.\n    :param action: The action selected by the agent.\n    :param value: The critic value of the obs.\n    :param log_prob: The actor log-probability of the selected action.\n    :param reward: The collected reward after executing the action.\n    :param next_obs: The next obs of the episode step in array format.\n    :param terminated: Episode termination.\n    :return: A transition object storing information about the state before and after executing the episode step,\n             the executed action, the collected reward, episode termination and optional additional information.\n    \"\"\"\n\n    transition = Transition(obs.squeeze(), action, value, log_prob, reward, next_obs, terminated)\n    transition = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), transition)\n\n    return transition\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._pp","title":"<code>_pp()</code>","text":"<p>Post-processes the training results, which includes:     - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored). :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def _pp(self) -&gt; None:\n    \"\"\"\n    Post-processes the training results, which includes:\n        - Setting the policy actor and critic TrainStates of a Runner object (e.g. last in training of restored).\n    :return:\n    \"\"\"\n\n    self.actor_training = self.training_runner.actor_training\n    self.critic_training = self.training_runner.critic_training\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._process_trajectory","title":"<code>_process_trajectory(update_runner, traj_batch, last_obs)</code>","text":"<p>Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not guaranteed to end with termination, the value is estimated using the critic network. This assumption has been shown to have no influence by the end of training. :param update_runner: The Runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param traj_batch: The batch of trajectories, as collected by in rollout. :param last_obs: The obs at the end of every trajectory in the batch. :return: A batch of trajectories that includes an estimate of values and advantages.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _process_trajectory(self, update_runner: Runner, traj_batch: Transition, last_obs: ObsType) -&gt; Transition:\n    \"\"\"\n    Estimates the value and advantages for a batch of trajectories. For the last state of trajectory, which is not\n    guaranteed to end with termination, the value is estimated using the critic network. This assumption has been\n    shown to have no influence by the end of training.\n    :param update_runner: The Runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param traj_batch: The batch of trajectories, as collected by in rollout.\n    :param last_obs: The obs at the end of every trajectory in the batch.\n    :return: A batch of trajectories that includes an estimate of values and advantages.\n    \"\"\"\n\n    traj_batch = jax.tree_util.tree_map(lambda x: x.squeeze(), traj_batch)\n    traj_batch = self._add_next_values(traj_batch, last_obs, update_runner.critic_training)\n\n    advantages = self._advantages(traj_batch, update_runner.hyperparams.gamma, update_runner.hyperparams.gae_lambda)\n    traj_batch = self._add_advantages(traj_batch, advantages)\n\n    return traj_batch\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._returns","title":"<code>_returns(traj_batch, last_next_state_value, gamma, gae_lambda)</code>","text":"<p>Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling step, trajectories do not start at the initial state. :param traj_batch: The batch of trajectories. :param last_next_state_value: The value of the last next state in each trajectory. :param gamma: Discount factor :param gae_lambda: The GAE \u03bb factor. :return: The returns over the episodes of the trajectory batch.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _returns(\n        self,\n        traj_batch: Transition,\n        last_next_state_value: Float[Array, \"batch_size\"],\n        gamma: float,\n        gae_lambda: float\n) -&gt; ReturnsType:\n    \"\"\"\n    Calculates the returns of every step in the trajectory batch. To do so, it identifies episodes in the\n    trajectories. Note that because lax.scan is used in sampling trajectories, they do not necessarily finish with\n    episode termination (episodes may be truncated). Also, since the environment is not re-initialized per sampling\n    step, trajectories do not start at the initial state.\n    :param traj_batch: The batch of trajectories.\n    :param last_next_state_value: The value of the last next state in each trajectory.\n    :param gamma: Discount factor\n    :param gae_lambda: The GAE \u03bb factor.\n    :return: The returns over the episodes of the trajectory batch.\n    \"\"\"\n\n    rewards_t = traj_batch.reward.squeeze()\n    terminated_t = 1.0 - traj_batch.terminated.astype(jnp.float32).squeeze()\n    discounts_t = (terminated_t * gamma).astype(jnp.float32)\n\n    \"\"\"Remove first entry so that the next state values per step are in sync with the state rewards.\"\"\"\n    next_state_values_t = jnp.concatenate(\n        [traj_batch.value.squeeze(), last_next_state_value[..., jnp.newaxis]],\n        axis=-1)[:, 1:]\n\n    rewards_t, discounts_t, next_state_values_t = jax.tree_util.tree_map(\n        lambda x: jnp.swapaxes(x, 0, 1), (rewards_t, discounts_t, next_state_values_t)\n    )\n\n    gae_lambda = jnp.ones_like(discounts_t) * gae_lambda\n\n    traj_runner = (rewards_t, discounts_t, next_state_values_t, gae_lambda)\n    end_value = jnp.take(next_state_values_t, -1, axis=0)  # Start from end of trajectory and work in reverse.\n    _, returns = lax.scan(self._trajectory_returns, end_value, traj_runner, reverse=True)\n\n    returns = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), returns)\n\n    return returns\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._rollout","title":"<code>_rollout(step_runner, i_step)</code>","text":"<p>Evaluation of trajectory rollout. In each step the agent: - evaluates policy and value - selects action - performs environment step - creates step transition :param step_runner: A tuple containing information on the environment state, the actor and critic training (parameters and networks) and a random key. :param i_step: Unused, required for lax.scan. :return: The updated step_runner tuple and the rollout step transition.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _rollout(self, step_runner: StepRunnerType, i_step: int) -&gt; Tuple[StepRunnerType, Transition]:\n    \"\"\"\n    Evaluation of trajectory rollout. In each step the agent:\n    - evaluates policy and value\n    - selects action\n    - performs environment step\n    - creates step transition\n    :param step_runner: A tuple containing information on the environment state, the actor and critic training\n    (parameters and networks) and a random key.\n    :param i_step: Unused, required for lax.scan.\n    :return: The updated step_runner tuple and the rollout step transition.\n    \"\"\"\n\n    envstate, obs, actor_training, critic_training, rng = step_runner\n\n    rng, rng_action = jax.random.split(rng)\n    action = self._sample_action(rng_action, actor_training, obs)\n\n    value = critic_training.apply_fn(lax.stop_gradient(critic_training.params), obs)\n\n    log_prob = self._log_prob(actor_training, lax.stop_gradient(actor_training.params), obs, action)\n\n    rng, next_obs, next_envstate, reward, done, info = self.env_step(rng, envstate, action)\n\n    step_runner = (next_envstate, next_obs, actor_training, critic_training, rng)\n\n    terminated = info[\"terminated\"]\n\n    transition = self._make_transition(\n        obs=obs,\n        action=action,\n        value=value,\n        log_prob=log_prob,\n        reward=reward,\n        next_obs=next_obs,\n        terminated=terminated,\n    )\n\n    return step_runner, transition\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._sample_action","title":"<code>_sample_action(rng, training, obs)</code>  <code>abstractmethod</code>","text":"Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _sample_action(\n        self,\n        rng: PRNGKeyArray,\n        training: TrainState,\n        obs: ObsType\n) -&gt; ActionType:\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._training_step","title":"<code>_training_step(update_runner, i_training_batch)</code>","text":"<p>Performs trainings steps to update the agent per training batch. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :param i_training_batch: Training batch loop counter. :return: tuple with updated runner and dictionary of metrics.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _training_step(\n        self,\n        update_runner: Runner,\n        i_training_batch: int\n) -&gt; Tuple[Runner, Dict[str, Float[Array, \"1\"]]]:\n    \"\"\"\n    Performs trainings steps to update the agent per training batch.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :param i_training_batch: Training batch loop counter.\n    :return: tuple with updated runner and dictionary of metrics.\n    \"\"\"\n\n    n_training_steps = self.config.n_steps - self.config.n_steps // self.config.eval_frequency * i_training_batch\n    n_training_steps = jnp.clip(n_training_steps, 1, self.config.eval_frequency)\n\n    update_runner = lax.fori_loop(0, n_training_steps, self._update_step, update_runner)\n\n    if self.eval_during_training:\n        metrics = self._generate_metrics(runner=update_runner, update_step=i_training_batch)\n        i_training_step = self.config.eval_frequency * (i_training_batch + 1)\n        i_training_step = jnp.minimum(i_training_step, self.config.n_steps)\n        if self.checkpointing:\n            self._checkpoint(update_runner, metrics, i_training_step)\n    else:\n        metrics = {}\n\n    return update_runner, metrics\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._trajectory_advantages","title":"<code>_trajectory_advantages(value, traj)</code>  <code>abstractmethod</code>","text":"<p>Calculates the advantages per episode step over a batch of trajectories. :param value: The values of the steps in the trajectory according to the critic (including the one of the last  state). :param traj: The trajectory batch. :return: An array of returns.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _trajectory_advantages(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the advantages per episode step over a batch of trajectories.\n    :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n     state).\n    :param traj: The trajectory batch.\n    :return: An array of returns.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._trajectory_returns","title":"<code>_trajectory_returns(value, traj)</code>  <code>abstractmethod</code>","text":"<p>Calculates the returns per episode step over a batch of trajectories. :param value: The values of the steps in the trajectory according to the critic (including the one of the last  state). :param traj: The trajectory batch. :return: A tuple of returns.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the returns per episode step over a batch of trajectories.\n    :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n     state).\n    :param traj: The trajectory batch.\n    :return: A tuple of returns.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase._update_step","title":"<code>_update_step(i_update_step, update_runner)</code>","text":"<p>An update step of the actor and critic networks. This entails: - performing rollout for sampling a batch of trajectories. - assessing the value of the last state per trajectory using the critic. - evaluating the advantage per trajectory. - updating the actor and critic network parameters via the respective loss functions. - generating in-training performance metrics. In this approach, the update_runner already has a batch of environments initialized. The environments are not initialized in the beginning of every update step, which means that trajectories to not necessarily start from an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be truncated in trajectory sampling). :param i_update_step: Unused, required for progressbar. :param update_runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. :return: The updated runner</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef _update_step(self, i_update_step: int, update_runner: Runner) -&gt; Runner:\n    \"\"\"\n    An update step of the actor and critic networks. This entails:\n    - performing rollout for sampling a batch of trajectories.\n    - assessing the value of the last state per trajectory using the critic.\n    - evaluating the advantage per trajectory.\n    - updating the actor and critic network parameters via the respective loss functions.\n    - generating in-training performance metrics.\n    In this approach, the update_runner already has a batch of environments initialized. The environments are not\n    initialized in the beginning of every update step, which means that trajectories to not necessarily start from\n    an initial state (which lead to better results when benchmarking with Cartpole-v1). Moreover, the use of lax.scan\n    for rollout means that the trajectories do not necessarily stop with episode termination (episodes can be\n    truncated in trajectory sampling).\n    :param i_update_step: Unused, required for progressbar.\n    :param update_runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters.\n    :return: The updated runner\n    \"\"\"\n\n    rollout_runners = self._make_rollout_runners(update_runner)\n    scan_rollout_fn = lambda x: lax.scan(self._rollout, x, None, self.config.rollout_length)\n    rollout_runners, traj_batch = jax.vmap(scan_rollout_fn)(rollout_runners)\n    last_envstate, last_obs, _, _, rng = rollout_runners\n    traj_batch = self._process_trajectory(update_runner, traj_batch, last_obs)\n\n    actor_training, actor_loss = self._actor_update(update_runner, traj_batch)\n    critic_training, critic_loss = self._critic_update(update_runner, traj_batch)\n\n    \"\"\"Update runner as a dataclass.\"\"\"\n    update_runner = update_runner.replace(\n        envstate=last_envstate,\n        obs=last_obs,\n        actor_training=actor_training,\n        critic_training=critic_training,\n        rng=rng,\n        actor_loss=jnp.expand_dims(actor_loss, axis=-1),\n        critic_loss=jnp.expand_dims(critic_loss, axis=-1)\n    )\n\n    return update_runner\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.collect_training","title":"<code>collect_training(runner=None, metrics=None, previous_training_max_step=0)</code>","text":"<p>Collects training or restored checkpoint of output (the final state of the runner after training and the collected metrics). :param runner: The runner object, containing information about the current status of the actor's/ critic's training, the state of the environment and training hyperparameters. This is at the state reached at the end of training. :param metrics: Dictionary of evaluation metrics (return per environment evaluation) :param previous_training_max_step: Maximum step reached during training. :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def collect_training(\n        self,\n        runner: Optional[Runner] = None,\n        metrics: Optional[Dict[str, Float[Array, \"1\"]]] = None,\n        previous_training_max_step: int = 0\n) -&gt; None:\n    \"\"\"\n    Collects training or restored checkpoint of output (the final state of the runner after training and the\n    collected metrics).\n    :param runner: The runner object, containing information about the current status of the actor's/\n    critic's training, the state of the environment and training hyperparameters. This is at the state reached at\n    the end of training.\n    :param metrics: Dictionary of evaluation metrics (return per environment evaluation)\n    :param previous_training_max_step: Maximum step reached during training.\n    :return:\n    \"\"\"\n\n    self.agent_trained = True\n    self.previous_training_max_step = previous_training_max_step\n    self.training_runner = runner\n    self.training_metrics = metrics\n    n_evals = list(metrics.values())[0].shape[0]\n    self.eval_steps_in_training = jnp.arange(n_evals) * self.config.eval_frequency\n    self._pp()\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.env_reset","title":"<code>env_reset(rng)</code>","text":"<p>Environment reset. :param rng: Random key for initialization. :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef env_reset(self, rng: PRNGKeyArray) -&gt; Tuple[PRNGKeyArray, ObsType, LogEnvState | EnvState | TruncationEnvState]:\n    \"\"\"\n    Environment reset.\n    :param rng: Random key for initialization.\n    :return: A random key after splitting the input, the reset environment in array and LogEnvState formats.\n    \"\"\"\n\n    rng, reset_rng = jax.random.split(rng)\n    obs, envstate = self.env.reset(reset_rng, self.env_params)\n    return rng, obs, envstate\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.env_step","title":"<code>env_step(rng, envstate, action)</code>","text":"<p>Environment step. :param rng: Random key for initialization. :param envstate: The environment state in LogEnvState format. :param action: The action selected by the agent. :return: A tuple of: a random key after splitting the input, the next obs in array and LogEnvState formats,          the collected reward after executing the action, episode termination and a dictionary of optional          additional information.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef env_step(\n        self,\n        rng: PRNGKeyArray,\n        envstate: LogEnvState | EnvState | TruncationEnvState,\n        action: ActionType\n) -&gt; Tuple[\n    PRNGKeyArray,\n    ObsType,\n    LogEnvState | EnvState | TruncationEnvState,\n    Float[Array, \"1\"],\n    Bool[Array, \"1\"],\n    Dict[str, float | bool]\n]:\n    \"\"\"\n    Environment step.\n    :param rng: Random key for initialization.\n    :param envstate: The environment state in LogEnvState format.\n    :param action: The action selected by the agent.\n    :return: A tuple of: a random key after splitting the input, the next obs in array and LogEnvState formats,\n             the collected reward after executing the action, episode termination and a dictionary of optional\n             additional information.\n    \"\"\"\n\n    rng, step_rng = jax.random.split(rng)\n    next_obs, next_envstate, reward, done, info = (\n        self.env.step(step_rng, envstate, action.squeeze(), self.env_params))\n\n    return rng, next_obs, next_envstate, reward, done, info\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.eval","title":"<code>eval(rng, n_evals=32)</code>","text":"<p>Evaluates the trained agent's performance post-training using the trained agent's actor and critic. :param rng: Random key for evaluation. :param n_evals: Number of steps in agent evaluation. :return: Dictionary of evaluation metrics.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def eval(self, rng: PRNGKeyArray, n_evals: int = 32) -&gt; Float[Array, \"n_evals\"]:\n    \"\"\"\n    Evaluates the trained agent's performance post-training using the trained agent's actor and critic.\n    :param rng: Random key for evaluation.\n    :param n_evals: Number of steps in agent evaluation.\n    :return: Dictionary of evaluation metrics.\n    \"\"\"\n\n    eval_metrics = self._eval_agent(rng, self.actor_training, self.critic_training, n_evals)\n\n    return eval_metrics\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs training hyperparameters in a text file. To be used outside training. :param hyperparams: An instance of HyperParameters for training. :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def log_hyperparams(self, hyperparams: HyperParameters) -&gt; None:\n    \"\"\"\n    Logs training hyperparameters in a text file. To be used outside training.\n    :param hyperparams: An instance of HyperParameters for training.\n    :return:\n    \"\"\"\n\n    output_lst = [field + ': ' + str(getattr(hyperparams, field)) for field in hyperparams._fields]\n    output_lst = ['Hyperparameters:'] + output_lst\n    output_lst = '\\n'.join(output_lst)\n\n    if self.checkpointing:\n        with open(os.path.join(self.config.checkpoint_dir, 'hyperparameters.txt'), \"w\") as f:\n            f.write(output_lst)\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.policy","title":"<code>policy(training, obs)</code>  <code>abstractmethod</code>","text":"<p>Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state. :param obs: The current obs of the episode step in array format. :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@abstractmethod\ndef policy(self, training: TrainState, obs: ObsType) -&gt; ActionType:\n    \"\"\"\n    Evaluates the action of the optimal policy (argmax) according to the trained agent for the given state.\n    :param obs: The current obs of the episode step in array format.\n    :return:\n    \"\"\"\n    raise NotImplemented\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.restore","title":"<code>restore(mode='best', best_fn=None)</code>","text":"<p>Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then, post-processes the restored checkpoint. :param mode: Determines whether the best performing or latest checkpoint should be restored. :param best_fn: The function that should be used in determining the best performing checkpoint. :return:</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def restore(\n        self,\n        mode: str = \"best\",\n        best_fn: Optional[Callable[[Dict[str, Float[Array, \"1\"]]], [int]]] = None\n) -&gt; None:\n    \"\"\"\n    Restores a checkpoint (best or latest) and collects the history of metrics as assessed during training. Then,\n    post-processes the restored checkpoint.\n    :param mode: Determines whether the best performing or latest checkpoint should be restored.\n    :param best_fn: The function that should be used in determining the best performing checkpoint.\n    :return:\n    \"\"\"\n\n    steps = self.checkpoint_manager.all_steps()\n\n    # Log keys in checkpoints\n    ckpt = self.checkpoint_manager.restore(steps[0])\n    ckpt_keys = [key for key in list(ckpt.keys()) if key != \"runner\"]\n\n    # Collect history of metrics in training. Useful for continuing training.\n    metrics = {key: [None] * len(steps) for key in ckpt_keys}\n    for i, step in enumerate(steps):\n        ckpt = self.checkpoint_manager.restore(step)\n        for key in ckpt_keys:\n            metrics[key][i] = ckpt[key][jnp.newaxis, :]\n    metrics = {key: jnp.concatenate(val, axis=0) for (key, val) in metrics.items()}\n\n    if mode == \"best\":\n        if best_fn is not None:\n            step = steps[best_fn(metrics)]\n        else:\n            raise Exception(\"Function for determining best checkpoint not provided\")\n    elif mode == \"last\":\n        step = self.checkpoint_manager.latest_step()\n    else:\n        raise Exception(\"Unknown method for selecting a checkpoint.\")\n\n    \"\"\"\n    Create an empty target for restoring the checkpoint.\n    Some of the arguments come from restoring one of the ckpts.\n    \"\"\"\n\n    empty_actor_training = self._create_empty_trainstate(self.config.actor_network)\n    empty_critic_training = self._create_empty_trainstate(self.config.critic_network)\n\n    # Get some obs and envstate for restoring the checkpoint.\n    _, obs, envstate = self.env_reset(jax.random.PRNGKey(1))\n\n    empty_runner = Runner(\n        actor_training=empty_actor_training,\n        critic_training=empty_critic_training,\n        envstate=envstate,\n        obs=obs,\n        rng=jax.random.split(jax.random.PRNGKey(1), self.config.batch_size),  # Just a dummy PRNGKey for initializing the networks parameters.\n        # Hyperparams can be loaded as a dict. If training continues, new hyperparams will be provided.\n        hyperparams=ckpt[\"runner\"][\"hyperparams\"]\n    )\n\n    target_ckpt = {\n        \"runner\": empty_runner,\n        \"terminated\": jnp.zeros(metrics[\"terminated\"].shape[1]),\n        \"truncated\": jnp.zeros(metrics[\"truncated\"].shape[1]),\n        \"final_rewards\": jnp.zeros(metrics[\"final_rewards\"].shape[1]),\n        \"returns\": jnp.zeros(metrics[\"returns\"].shape[1]),\n    }\n\n    ckpt = self.checkpoint_manager.restore(step, items=target_ckpt)\n\n    self.collect_training(ckpt[\"runner\"], metrics, previous_training_max_step=max(steps))\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.summarize","title":"<code>summarize(metrics)</code>","text":"<p>Summarizes collection of per-episode metrics. :param metrics: Metric per episode. :return: Summary of metric per episode.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>def summarize(\n        self,\n        metrics: Annotated[NDArray[np.float32], \"size_metrics\"] | Float[Array, \"size_metrics\"]\n) -&gt; MetricStats:\n    \"\"\"\n    Summarizes collection of per-episode metrics.\n    :param metrics: Metric per episode.\n    :return: Summary of metric per episode.\n    \"\"\"\n\n    return MetricStats(\n        episode_metric=metrics,\n        mean=metrics.mean(axis=-1),\n        var=metrics.var(axis=-1),\n        std=metrics.std(axis=-1),\n        min=metrics.min(axis=-1),\n        max=metrics.max(axis=-1),\n        median=jnp.median(metrics, axis=-1),\n        has_nans=jnp.any(jnp.isnan(metrics), axis=-1)\n    )\n</code></pre>"},{"location":"reference/ppoagentbase/#jaxagents.ppo.PPOAgentBase.train","title":"<code>train(rng, hyperparams)</code>","text":"<p>Trains the agent. A jax_tqdm progressbar has been added in the lax.scan loop. :param rng: Random key for initialization. This is the original key for training. :param hyperparams: An instance of HyperParameters for training. :return: The final state of the step runner after training and the training metrics accumulated over all          training batches and steps.</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>@partial(jax.jit, static_argnums=(0,))\ndef train(self, rng: PRNGKeyArray, hyperparams: HyperParameters) -&gt; Tuple[Runner, Dict[str, Float[Array, \"1\"]]]:\n    \"\"\"\n    Trains the agent. A jax_tqdm progressbar has been added in the lax.scan loop.\n    :param rng: Random key for initialization. This is the original key for training.\n    :param hyperparams: An instance of HyperParameters for training.\n    :return: The final state of the step runner after training and the training metrics accumulated over all\n             training batches and steps.\n    \"\"\"\n\n    rng, *_rng = jax.random.split(rng, 4)\n    actor_init_rng, critic_init_rng, runner_rng = _rng\n\n    actor_training = self._create_training(\n        actor_init_rng, self.config.actor_network, hyperparams.actor_optimizer_params\n    )\n    critic_training = self._create_training(\n        critic_init_rng, self.config.critic_network, hyperparams.critic_optimizer_params\n    )\n\n    update_runner = self._create_update_runner(runner_rng, actor_training, critic_training, hyperparams)\n\n    # Checkpoint initial state\n    if self.eval_during_training:\n        metrics_start = self._generate_metrics(runner=update_runner, update_step=0)\n        if self.checkpointing:\n            self._checkpoint(update_runner, metrics_start, self.previous_training_max_step)\n\n    # Initialize agent updating functions, which can be avoided to be done within the training loops.\n    actor_grad_fn = jax.grad(self._actor_loss, has_aux=True, allow_int=True)\n    self._actor_minibatch_fn = lambda x, y: self._actor_minibatch_update(x, y, actor_grad_fn)\n\n    critic_grad_fn = jax.grad(self._critic_loss, allow_int=True)\n    self._critic_minibatch_fn = lambda x, y: self._critic_minibatch_update(x, y, critic_grad_fn)\n\n    # Train, evaluate, checkpoint\n\n    n_training_batches = self.config.n_steps // self.config.eval_frequency\n    progressbar_desc = f'Training batch (training steps = batch x {self.config.eval_frequency})'\n\n    runner, metrics = lax.scan(\n        scan_tqdm(n_training_batches, desc=progressbar_desc)(self._training_step),\n        update_runner,\n        jnp.arange(n_training_batches),\n        n_training_batches\n    )\n\n    if self.eval_during_training:\n        metrics = {\n            key: jnp.concatenate((metrics_start[key][jnp.newaxis, :], metrics[key]), axis=0)\n            for key in metrics.keys()\n        }\n    else:\n        metrics= {}\n\n    return runner, metrics\n</code></pre>"}]}