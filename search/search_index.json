{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JAXAgents","text":"<p>JAXAgents is a high-performance (Multi-Agent) Reinforcement Learning library built on JAX, designed for rapid experimentation, scalable training of RL agents and fast hyperparameter tuning. It supports a variety of algorithms and environments, making it suitable for both research and practical applications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>RL: Implementations of popular RL algorithms, including:</li> <li>Q-learning:<ul> <li>Deep Q Networks (DQN)</li> <li>Double Deep Q Networks (DDQN)</li> <li>Categorical DQN (C51)</li> <li>Quantile Regression DQN (QRDQN)</li> </ul> </li> <li>Policy Gradient:<ul> <li>REINFORCE</li> <li>Proximal Policy Optimization (PPO) with Generalized Advantage Estimation (GAE)</li> </ul> </li> <li> <p>Multi-Agent RL:</p> <ul> <li>Independent PPO (IPPO)</li> </ul> </li> <li> <p>High Performance: Leveraging JAX's capabilities for just-in-time compilation and automatic differentiation, enabling efficient computation on CPUs and GPUs.</p> </li> <li> <p>Modular Design: Structured for easy extension and customization, facilitating experimentation with new algorithms and environments.</p> </li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Ensure you have Python 3.10 or higher installed. Then, install JAX Agents via pip:</p> <pre><code>pip install jaxagents\n</code></pre> <p>Note: Also available on PyPI</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Here's a simple example to train a PPO agent:</p> <pre><code>import jaxagents\n\n# Initialize environment and agent\nenv = jaxagents.environments.make('CartPole-v1')\nagent = jaxagents.agents.PPO(env)\n\n# Train the agent\nagent.train(num_episodes=1000)\n</code></pre> <p>For more detailed examples and usage, refer to the documentation.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Comprehensive documentation is available at jax-agents.readthedocs.io, covering:</p> <ul> <li>Installation and setup</li> <li>Detailed API references</li> <li>Tutorials and examples</li> <li>Advanced topics and customization</li> </ul>"},{"location":"#development","title":"\ud83d\udee0\ufe0f Development","text":"<p>To contribute or modify the library:</p> <pre><code>git clone https://github.com/amavrits/jax-agents.git\ncd jax-agents\npip install -e .\n</code></pre>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under a proprietary license. For more information, please refer to the LICENSE file.</p> <p>For any questions or contributions, feel free to open an issue or submit a pull request on the GitHub repository.</p>"},{"location":"reference/","title":"Reference","text":"<p>               Bases: <code>PPOAgentBase</code></p> <p>PPO clip agent using the GAE (PPO2) for calculating the advantage. The actor loss function standardizes the advantage. See : https://spinningup.openai.com/en/latest/algorithms/ppo.html</p> Source code in <code>jaxagents\\ppo.py</code> <pre><code>class PPOAgent(PPOAgentBase):\n\n    \"\"\"\n    PPO clip agent using the GAE (PPO2) for calculating the advantage. The actor loss function standardizes the\n    advantage.\n    See : https://spinningup.openai.com/en/latest/algorithms/ppo.html\n    \"\"\"\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _trajectory_returns(self, value: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the returns per episode step over a batch of trajectories.\n        :param value: The values of the steps in the trajectory according to the critic (including the one of the last\n        state). In the begining of the method, 'value' is the value of the state in the next step in the trajectory\n        (not the reverse iteration), and after calculation it is the value of the examined state in the examined step.\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n        rewards, discounts, next_state_values, gae_lambda = traj\n        value = rewards + discounts * ((1 - gae_lambda) * next_state_values + gae_lambda * value)\n        return value, value\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _trajectory_advantages(self, advantage: Float[Array, \"batch_size\"], traj: Transition) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the GAE per episode step over a batch of trajectories.\n        :param advantage: The GAE advantages of the steps in the trajectory according to the critic (including the one\n        of the last state). In the beginning of the method, 'advantage' is the advantage of the state in the next step\n        in the trajectory (not the reverse iteration), and after calculation it is the advantage of the examined state\n        in each step.\n        :param traj: The trajectory batch.\n        :return: An array of returns.\n        \"\"\"\n        rewards, values, next_state_values, terminated, gamma, gae_lambda = traj\n        d_t = rewards + (1 - terminated) * gamma * next_state_values - values  # Temporal difference residual at time t\n        advantage = d_t + gamma * gae_lambda * (1 - terminated) * advantage\n        return advantage, advantage\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_loss(\n            self,\n            training: TrainState,\n            obs: Annotated[ObsType, \"n_rollout batch_size\"],\n            action: Annotated[ActionType, \"batch_size\"],\n            log_prob_old: Float[Array, \"n_rollout batch_size\"],\n            advantage: ReturnsType,\n            hyperparams: HyperParameters\n    )-&gt; Tuple[Float[Array, \"1\"], Float[Array, \"1\"]]:\n        \"\"\"\n        Calculates the actor loss. For the REINFORCE agent, the advantage function is the difference between the\n        discounted returns and the value as estimated by the critic.\n        :param training: The actor TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param action: The actions in the trajectory batch.\n        :param log_prob_old: Log-probabilities of the old policy collected over the trajectory batch.\n        :param advantage: The GAE over the trajectory batch.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: A tuple containing the actor loss and the KL divergence (for early checking stopping criterion).\n        \"\"\"\n\n        \"\"\" Standardize GAE, greatly improves behaviour\"\"\"\n        advantage = (advantage - advantage.mean(axis=0)) / (advantage.std(axis=0) + 1e-8)\n\n        log_prob_vmap = jax.vmap(jax.vmap(self._log_prob, in_axes=(None, None, 0, 0)), in_axes=(None, None, 0, 0))\n        log_prob = log_prob_vmap(training, training.params, obs, action)\n        log_policy_ratio = log_prob - log_prob_old\n        policy_ratio = jnp.exp(log_policy_ratio)\n        kl = jnp.sum(-log_policy_ratio)\n\n        \"\"\"\n        Adopt simplified formulation of clipped policy ratio * advantage as explained in the note of:\n        https://spinningup.openai.com/en/latest/algorithms/ppo.html#id2\n        \"\"\"\n        clip = jnp.where(jnp.greater(advantage, 0), 1 + hyperparams.eps_clip, 1 - hyperparams.eps_clip)\n        advantage_clip = advantage * clip\n\n        \"\"\"Actual clip calculation - not used but left here for comparison to simplified version\"\"\"\n        # advantage_clip = jnp.clip(policy_ratio, 1 - hyperparams.eps_clip, 1 + hyperparams.eps_clip) * advantage\n\n        loss_actor = jnp.minimum(policy_ratio * advantage, advantage_clip)\n\n        entropy_vmap = jax.vmap(jax.vmap(self._entropy, in_axes=(None, 0)), in_axes=(None, 0))\n        entropy = entropy_vmap(training, obs)\n\n        total_loss_actor = loss_actor.mean() + hyperparams.ent_coeff * entropy.mean()\n\n        \"\"\" Negative loss, because we want ascent but 'apply_gradients' applies descent \"\"\"\n        return -total_loss_actor, kl\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss(\n            self,\n            training: TrainState,\n            obs: Annotated[ObsType, \"n_rollout batch_size\"],\n            targets: ReturnsType,\n            hyperparams: HyperParameters\n    ) -&gt; Float[Array, \"1\"]:\n        \"\"\"\n        Calculates the critic loss.\n        :param training: The critic TrainState object.\n        :param obs: The obs in the trajectory batch.\n        :param targets: The targets over the trajectory batch for training the critic.\n        :param hyperparams: The HyperParameters object used for training.\n        :return: The critic loss.\n        \"\"\"\n\n        value_vmap = jax.vmap(jax.vmap(training.apply_fn, in_axes=(None, 0)), in_axes=(None, 0))\n        value = value_vmap(training.params, obs)\n        residuals = value - targets\n        value_loss = jnp.mean(residuals ** 2)\n        critic_total_loss = hyperparams.vf_coeff * value_loss\n\n        return critic_total_loss\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _actor_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; ActorLossInputType:\n        \"\"\"\n        Prepares the input required by the actor loss function. For the PPO agent, this entails the:\n        - the actions collected over the trajectory batch.\n        - the log-probability of the actions collected over the trajectory batch.\n        - the returns over the trajectory batch.\n        - the values over the trajectory batch as evaluated by the critic.\n        - the training hyperparameters.\n        The input is reshaped so that it is split into minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the actor loss function.\n        \"\"\"\n\n        # Shuffle the trajectory batch to collect minibatches.\n        # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n        # the batch are used per epoch.\n        minibatch_idx = jax.random.choice(\n            jax.random.PRNGKey(1),\n            jnp.arange(self.config.batch_size),\n            replace=False,\n            shape=(self.config.batch_size,)\n        )\n\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n        traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n        return (\n            traj_minibatch.obs,\n            traj_minibatch.action,\n            traj_minibatch.log_prob,\n            traj_minibatch.advantage,\n            update_runner.hyperparams\n        )\n\n    @partial(jax.jit, static_argnums=(0,))\n    def _critic_loss_input(self, update_runner: Runner, traj_batch: Transition) -&gt; CriticLossInputType:\n        \"\"\"\n        Prepares the input required by the critic loss function. For the PPO agent, this entails the:\n        - the states collected over the trajectory batch.\n        - the targets (returns = GAE + next_value) over the trajectory batch.\n        - the training hyperparameters.\n        The input is reshaped so that it is split into minibatches.\n        :param update_runner: The Runner object used in training.\n        :param traj_batch: The batch of trajectories.\n        :return: A tuple of input to the critic loss function.\n        \"\"\"\n\n        # Shuffle the trajectory batch to collect minibatches.\n        # Poor practice in using the random key, which however doesn't influence the training, since all trajectories in\n        # the batch are used per epoch.\n        minibatch_idx = jax.random.choice(\n            jax.random.PRNGKey(1),\n            jnp.arange(self.config.batch_size),\n            replace=False,\n            shape=(self.config.batch_size,)\n        )\n\n        traj_minibatch = jax.tree_map(lambda x: jnp.take(x, minibatch_idx, axis=0), traj_batch)\n        traj_minibatch = jax.tree_map(lambda x: x.reshape(-1, self.config.minibatch_size, *x.shape[1:]), traj_minibatch)\n\n        return (\n            traj_minibatch.obs,\n            traj_minibatch.advantage + traj_minibatch.value,\n            update_runner.hyperparams\n        )\n</code></pre>"}]}